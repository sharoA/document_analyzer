#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ContentAnalyzerServiceå†…å®¹åˆ†ææœåŠ¡
"""

import sys
import os
import asyncio
import json
from unittest.mock import Mock, AsyncMock

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.analysis_services.content_analyzer import ContentAnalyzerService

class MockLLMClient:
    """æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯"""
    
    def chat(self, messages, max_tokens=4000):
        """æ¨¡æ‹ŸèŠå¤©å“åº”"""
        user_content = messages[-1]['content']
        
        # æ ¹æ®æç¤ºå†…å®¹è¿”å›ä¸åŒçš„æ¨¡æ‹Ÿå“åº”
        if "JSON æ ¼å¼è¾“å‡ºåˆ†æç»“æœ" in user_content:
            return '''
{
    "current_change": [
        {
            "changeType": "ä¿®æ”¹",
            "changeReason": "ç³»ç»Ÿè‡ªåŠ¨å®¡æ‰¹åŠŸèƒ½ä»äººå·¥å®¡æ‰¹æ”¹ä¸ºè‡ªåŠ¨è¯„åˆ†",
            "changeItems": ["è¯„åˆ†æ¨¡å‹è‡ªåŠ¨å®¡æ‰¹", "å–æ¶ˆäººå·¥å®¡æ‰¹æµç¨‹"],
            "version": ["å†å²ç‰ˆæœ¬V1.6.docx"]
        }
    ]
}
'''
        elif "åˆ é™¤é¡¹" in user_content:
            return '''
{
    "changeType": "åˆ é™¤",
    "deletedItem": "æ‰‹åŠ¨å®¡æ‰¹åŠŸèƒ½",
    "section": "3.1 é¢åº¦ç”³è¯·",
    "analysisResult": "åˆ é™¤æ‰‹åŠ¨å®¡æ‰¹åŠŸèƒ½ï¼Œæ”¹ä¸ºç³»ç»Ÿè‡ªåŠ¨å®¡æ‰¹ï¼Œæé«˜æ•ˆç‡"
}
'''
        else:
            return "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„LLMå“åº”"

def create_test_markdown_content():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„markdownå†…å®¹"""
    return """
# éœ€æ±‚æ–‡æ¡£ - é“¶è¡Œä¿¡è´·ç³»ç»ŸV2.0

## 1. ç³»ç»Ÿæ¦‚è¿°
æœ¬ç³»ç»Ÿæ˜¯é“¶è¡Œä¿¡è´·ä¸šåŠ¡çš„æ ¸å¿ƒç³»ç»Ÿï¼Œè´Ÿè´£å¤„ç†å®¢æˆ·çš„ä¿¡è´·ç”³è¯·ã€å®¡æ‰¹å’Œæ”¾æ¬¾æµç¨‹ã€‚

## 2. åŠŸèƒ½æ¨¡å—

### 2.1 å®¢æˆ·ç®¡ç†
- å®¢æˆ·ä¿¡æ¯å½•å…¥å’Œç»´æŠ¤
- å®¢æˆ·å¾ä¿¡æŸ¥è¯¢
- å®¢æˆ·é£é™©è¯„ä¼°

### 2.2 ä¿¡è´·ç”³è¯·
- åœ¨çº¿ç”³è¯·æäº¤
- ç”³è¯·ææ–™ä¸Šä¼ 
- ç”³è¯·çŠ¶æ€è·Ÿè¸ª

## 3. ä¸šåŠ¡æµç¨‹

### 3.1 é¢åº¦ç”³è¯·
ç”¨æˆ·é€šè¿‡Appå‘èµ·é¢åº¦ç”³è¯·ï¼Œç³»ç»Ÿè‡ªåŠ¨è°ƒç”¨å®¡æ‰¹æœåŠ¡ï¼Œé‡‡ç”¨è¯„åˆ†æ¨¡å‹è‡ªåŠ¨å®¡æ‰¹ã€‚
åˆ†å€¼ä½äº80åˆ†çš„ç”³è¯·å°†è¢«è‡ªåŠ¨æ‹’ç»ã€‚

~~åˆ é™¤äº†æ‰‹åŠ¨å®¡æ‰¹åŠŸèƒ½~~

### 3.2 æ”¾æ¬¾æµç¨‹
1. å®¡æ‰¹é€šè¿‡åè‡ªåŠ¨ç”Ÿæˆæ”¾æ¬¾æŒ‡ä»¤
2. ç³»ç»Ÿè°ƒç”¨é“¶è¡Œæ ¸å¿ƒç³»ç»Ÿå®Œæˆæ”¾æ¬¾
3. å‘é€æ”¾æ¬¾æˆåŠŸé€šçŸ¥ç»™å®¢æˆ·

![æµç¨‹å›¾](images/loan_process.png)

## 4. æŠ€æœ¯æ¶æ„

### 4.1 ç³»ç»Ÿæ¶æ„
é‡‡ç”¨å¾®æœåŠ¡æ¶æ„ï¼ŒåŒ…å«ä»¥ä¸‹æœåŠ¡ï¼š
- ç”¨æˆ·æœåŠ¡
- å®¡æ‰¹æœåŠ¡
- æ”¾æ¬¾æœåŠ¡
- é€šçŸ¥æœåŠ¡

### 4.2 æ•°æ®åº“è®¾è®¡
ä½¿ç”¨MySQLä½œä¸ºä¸»æ•°æ®åº“ï¼ŒRedisä½œä¸ºç¼“å­˜æ•°æ®åº“ã€‚

## 5. æ¥å£è®¾è®¡

### 5.1 ç”³è¯·æ¥å£
- POST /api/v1/applications
- å‚æ•°ï¼šå®¢æˆ·ä¿¡æ¯ã€ç”³è¯·é‡‘é¢ã€ç”³è¯·æœŸé™

### 5.2 æŸ¥è¯¢æ¥å£  
- GET /api/v1/applications/{id}
- è¿”å›ï¼šç”³è¯·è¯¦æƒ…å’ŒçŠ¶æ€

å–æ¶ˆäº† /api/v1/manual-approval æ¥å£ï¼Œä¸å†æ”¯æŒæ‰‹åŠ¨å®¡æ‰¹ã€‚
"""

async def test_content_analyzer():
    """æµ‹è¯•å†…å®¹åˆ†æå™¨"""
    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•ContentAnalyzerService")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„LLMå®¢æˆ·ç«¯
    mock_llm_client = MockLLMClient()
    
    try:
        # åˆå§‹åŒ–ContentAnalyzerService
        print("ğŸ”§ åˆå§‹åŒ–ContentAnalyzerService...")
        analyzer = ContentAnalyzerService(llm_client=mock_llm_client)
        print("âœ… ContentAnalyzerServiceåˆå§‹åŒ–æˆåŠŸ")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = {
            "document_content": create_test_markdown_content()
        }
        
        print(f"\nğŸ“„ æµ‹è¯•æ–‡æ¡£é•¿åº¦: {len(test_data['document_content'])} å­—ç¬¦")
        
        # æ‰§è¡Œåˆ†æ
        print("\nğŸ” å¼€å§‹æ‰§è¡Œå†…å®¹åˆ†æ...")
        result = await analyzer.analyze("test_task_001", test_data)
        
        # æ£€æŸ¥ç»“æœ
        if result["success"]:
            print("âœ… å†…å®¹åˆ†ææˆåŠŸ!")
            
            # æ‰“å°åˆ†æç»“æœ
            data = result["data"]
            
            print(f"\nğŸ“Š åˆ†æç»“æœç»Ÿè®¡:")
            print(f"   - ç»“æ„åŒ–å†…å®¹å—æ•°é‡: {len(data['structured_chunks'])}")
            print(f"   - åˆ†æè€—æ—¶: {data['metadata']['analysis_time']:.2f}ç§’")
            print(f"   - å†…å®¹é•¿åº¦: {data['metadata']['content_length']} å­—ç¬¦")
            
            # æ˜¾ç¤ºç»“æ„åŒ–å†…å®¹å—
            print(f"\nğŸ“‹ ç»“æ„åŒ–å†…å®¹å—:")
            for i, chunk in enumerate(data['structured_chunks'], 1):
                print(f"   {i}. [{chunk['level']}çº§] {chunk['section']}")
                print(f"      å†…å®¹: {chunk['content'][:50]}...")
                if chunk['image_refs']:
                    print(f"      å›¾ç‰‡: {chunk['image_refs']}")
            
            # æ˜¾ç¤ºå†å²å¯¹æ¯”ç»“æœ
            history_comp = data['history_comparison']
            print(f"\nğŸ” å†å²å¯¹æ¯”ç»“æœ:")
            print(f"   - æ€»å†…å®¹å—: {history_comp['summary']['total_chunks']}")
            print(f"   - æ–°å¢é¡¹: {history_comp['summary']['new_items']}")
            print(f"   - ä¿®æ”¹é¡¹: {history_comp['summary']['modified_items']}")
            print(f"   - åˆ é™¤é¡¹: {history_comp['summary']['deleted_items']}")
            
            # æ˜¾ç¤ºè¯†åˆ«åˆ°çš„åˆ é™¤é¡¹
            if history_comp['deleted_items']:
                print(f"\nğŸ—‘ï¸ è¯†åˆ«åˆ°çš„åˆ é™¤é¡¹:")
                for item in history_comp['deleted_items']:
                    print(f"   - ç« èŠ‚: {item['section']}")
                    print(f"     åˆ é™¤å†…å®¹: {item['deleted_item']}")
                    print(f"     æ£€æµ‹æ–¹æ³•: {item['detection_method']}")
            
            # æ˜¾ç¤ºå˜æ›´åˆ†æ
            change_analysis = data['change_analysis']
            print(f"\nğŸ“ˆ å˜æ›´åˆ†æ:")
            print(f"   - å˜æ›´åˆ†ææ•°é‡: {change_analysis['summary']['total_changes']}")
            print(f"   - åˆ é™¤åˆ†ææ•°é‡: {change_analysis['summary']['total_deletions']}")
            
            # æ˜¾ç¤ºè¯¦ç»†çš„å˜æ›´åˆ†æï¼ˆå¦‚æœæœ‰ï¼‰
            if change_analysis['change_analyses']:
                print(f"\nğŸ“ è¯¦ç»†å˜æ›´åˆ†æ:")
                for analysis in change_analysis['change_analyses']:
                    if 'current_change' in analysis:
                        for change in analysis['current_change']:
                            print(f"   - å˜æ›´ç±»å‹: {change['changeType']}")
                            print(f"   - å˜æ›´åŸå› : {change['changeReason']}")
                            print(f"   - å˜æ›´é¡¹: {change['changeItems']}")
            
            # æ˜¾ç¤ºåˆ é™¤åˆ†æï¼ˆå¦‚æœæœ‰ï¼‰
            if change_analysis['deletion_analyses']:
                print(f"\nğŸ—‘ï¸ åˆ é™¤åˆ†æ:")
                for analysis in change_analysis['deletion_analyses']:
                    print(f"   - åˆ é™¤é¡¹: {analysis['deletedItem']}")
                    print(f"   - ç« èŠ‚: {analysis['section']}")
                    print(f"   - åˆ†æç»“æœ: {analysis['analysisResult']}")
            
        else:
            print(f"âŒ å†…å®¹åˆ†æå¤±è´¥: {result['error']}")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

async def test_document_preprocessing():
    """æµ‹è¯•æ–‡æ¡£é¢„å¤„ç†åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ“„ æµ‹è¯•æ–‡æ¡£é¢„å¤„ç†åŠŸèƒ½")
    print("=" * 60)
    
    try:
        mock_llm_client = MockLLMClient()
        analyzer = ContentAnalyzerService(llm_client=mock_llm_client)
        
        # æµ‹è¯•markdownå†…å®¹
        markdown_content = create_test_markdown_content()
        
        print("ğŸ”§ æ‰§è¡Œæ–‡æ¡£é¢„å¤„ç†...")
        chunks = await analyzer._preprocess_document(markdown_content)
        
        print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(chunks)} ä¸ªå†…å®¹å—")
        
        # æ˜¾ç¤ºæ¯ä¸ªå†…å®¹å—çš„è¯¦ç»†ä¿¡æ¯
        for i, chunk in enumerate(chunks, 1):
            print(f"\nğŸ“‹ å†…å®¹å— {i}:")
            print(f"   æ ‡é¢˜: {chunk['section']}")
            print(f"   çº§åˆ«: {chunk['level']}")
            print(f"   å†…å®¹é•¿åº¦: {len(chunk['content'])} å­—ç¬¦")
            print(f"   å†…å®¹é¢„è§ˆ: {chunk['content'][:100]}...")
            print(f"   å‘é‡ç»´åº¦: {len(chunk['embedding'])}")
            if chunk['image_refs']:
                print(f"   å›¾ç‰‡å¼•ç”¨: {chunk['image_refs']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡æ¡£é¢„å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_deletion_detection():
    """æµ‹è¯•åˆ é™¤é¡¹è¯†åˆ«åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("ğŸ—‘ï¸ æµ‹è¯•åˆ é™¤é¡¹è¯†åˆ«åŠŸèƒ½")
    print("=" * 60)
    
    try:
        mock_llm_client = MockLLMClient()
        analyzer = ContentAnalyzerService(llm_client=mock_llm_client)
        
        # åˆ›å»ºåŒ…å«åˆ é™¤æè¿°çš„æµ‹è¯•å†…å®¹
        test_chunks = [
            {
                "section": "3.1 é¢åº¦ç”³è¯·",
                "content": "åˆ é™¤äº†æ‰‹åŠ¨å®¡æ‰¹åŠŸèƒ½ï¼Œæ”¹ä¸ºç³»ç»Ÿè‡ªåŠ¨å®¡æ‰¹ã€‚å»é™¤äº†äººå·¥å¹²é¢„ç¯èŠ‚ã€‚",
                "level": 3
            },
            {
                "section": "5.1 æ¥å£è®¾è®¡", 
                "content": "å–æ¶ˆäº† /api/v1/manual-approval æ¥å£ï¼Œä¸å†æ”¯æŒæ‰‹åŠ¨å®¡æ‰¹ã€‚",
                "level": 3
            },
            {
                "section": "4.2 åŠŸèƒ½æ¨¡å—",
                "content": "~~ç§»é™¤äº†çŸ­ä¿¡é€šçŸ¥åŠŸèƒ½~~ï¼Œæ”¹ä¸ºé‚®ä»¶é€šçŸ¥ã€‚",
                "level": 3
            }
        ]
        
        print("ğŸ” æ‰§è¡Œåˆ é™¤é¡¹è¯†åˆ«...")
        deleted_items = await analyzer._identify_deleted_items(test_chunks)
        
        print(f"âœ… è¯†åˆ«å®Œæˆï¼Œæ‰¾åˆ° {len(deleted_items)} ä¸ªåˆ é™¤é¡¹")
        
        for i, item in enumerate(deleted_items, 1):
            print(f"\nğŸ—‘ï¸ åˆ é™¤é¡¹ {i}:")
            print(f"   ç« èŠ‚: {item['section']}")
            print(f"   åˆ é™¤å†…å®¹: {item['deleted_item']}")
            print(f"   æ£€æµ‹æ–¹æ³•: {item['detection_method']}")
            print(f"   ä¸Šä¸‹æ–‡: {item['context'][:100]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ é™¤é¡¹è¯†åˆ«æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ ContentAnalyzerService æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    tests = [
        ("æ–‡æ¡£é¢„å¤„ç†", test_document_preprocessing),
        ("åˆ é™¤é¡¹è¯†åˆ«", test_deletion_detection),
        ("å®Œæ•´å†…å®¹åˆ†æ", test_content_analyzer),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"\nâ–¶ï¸ å¼€å§‹æµ‹è¯•: {test_name}")
        try:
            results[test_name] = await test_func()
        except Exception as e:
            print(f"âŒ æµ‹è¯• {test_name} å¼‚å¸¸: {e}")
            results[test_name] = False
    
    # æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 60)
    
    passed = 0
    for test_name, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"   {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ¯ æ€»è®¡: {passed}/{len(tests)} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if passed == len(tests):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")

if __name__ == "__main__":
    asyncio.run(main()) 