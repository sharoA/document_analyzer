#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆéœ€æ±‚æ–‡æ¡£å‘é‡åŒ–åˆå§‹åŒ–è„šæœ¬
è§£å†³åˆ†æ®µç­–ç•¥é—®é¢˜ï¼Œé‡‡ç”¨æ··åˆåˆ†æ®µç­–ç•¥ï¼š
1. æ™ºèƒ½è¯­ä¹‰åˆ†æ®µ + é•¿åº¦æ§åˆ¶
2. å†…å®¹è´¨é‡è¿‡æ»¤ 
3. é€‚å½“é‡å å¤„ç†
"""

import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

# æ–‡ä»¶å¤„ç†ç›¸å…³
import docx
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Weaviate å’Œ LangChain
import weaviate
import weaviate.classes as wvc
from langchain.schema import Document

# é¡¹ç›®é…ç½®
try:
    from src.utils.weaviate_helper import get_weaviate_client
except ImportError:
    import sys
    project_root = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, project_root)
    from src.utils.weaviate_helper import get_weaviate_client

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OptimizedRequirementsDocsInitializer:
    """ä¼˜åŒ–ç‰ˆéœ€æ±‚æ–‡æ¡£åˆå§‹åŒ–å™¨ - æ··åˆåˆ†æ®µç­–ç•¥"""
    
    def __init__(self, requirements_docs_path: str = "/Users/renyu/Documents/knowledge_base/é“¾æ•°_LS/éœ€æ±‚æ–‡æ¡£"):
        """
        åˆå§‹åŒ–éœ€æ±‚æ–‡æ¡£å¤„ç†å™¨
        
        Args:
            requirements_docs_path: éœ€æ±‚æ–‡æ¡£ç›®å½•è·¯å¾„
        """
        self.requirements_docs_path = Path(requirements_docs_path)
        self.weaviate_client = None
        self.embeddings_model = None
        self.text_splitter = None
        
        # åªå¤„ç†DOCXæ–‡ä»¶
        self.supported_extensions = {'.docx'}
        
        # åˆ†æ®µç­–ç•¥é…ç½®
        self.config = {
            'min_chunk_length': 50,     # æœ€å°æ®µè½é•¿åº¦
            'max_chunk_length': 800,    # æœ€å¤§æ®µè½é•¿åº¦ï¼Œè¶…è¿‡åˆ™äºŒæ¬¡åˆ†å‰²
            'overlap_length': 100,      # é‡å é•¿åº¦
            'merge_threshold': 150,     # çŸ­æ®µè½åˆå¹¶é˜ˆå€¼
        }
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
    
    def _initialize_components(self):
        """åˆå§‹åŒ–å„ç§ç»„ä»¶"""
        try:
            # åˆå§‹åŒ– Weaviate å®¢æˆ·ç«¯
            self.weaviate_client = get_weaviate_client()
            logger.info("âœ… Weaviate å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - ä½¿ç”¨bge-large-zhï¼ˆ1024ç»´ï¼Œä¸­æ–‡ä¼˜åŒ–ï¼‰
            self.embeddings_model = SentenceTransformer('BAAI/bge-large-zh')
            logger.info("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ (bge-large-zh, 1024ç»´ï¼Œä¸­æ–‡ä¼˜åŒ–)")
            
            # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆç”¨äºé•¿æ®µè½çš„äºŒæ¬¡åˆ†å‰²ï¼‰
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.config['max_chunk_length'],
                chunk_overlap=self.config['overlap_length'],
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼Œ", " ", ""]
            )
            logger.info("âœ… æ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            logger.info("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _clear_weaviate_database(self):
        """æ¸…ç©ºWeaviateæ•°æ®åº“ä¸­çš„éœ€æ±‚æ–‡æ¡£æ•°æ®"""
        try:
            logger.info("ğŸ—‘ï¸ å¼€å§‹æ¸…ç©ºéœ€æ±‚æ–‡æ¡£ç›¸å…³æ•°æ®...")
            
            collection_name = "Document"
            if self.weaviate_client.collections.exists(collection_name):
                collection = self.weaviate_client.collections.get(collection_name)
                
                # æŸ¥è¯¢æ‰€æœ‰LSé¡¹ç›®çš„æ–‡æ¡£
                results = collection.query.fetch_objects(
                    filters=wvc.query.Filter.by_property("project").equal("LS"),
                    limit=10000
                )
                
                # åˆ é™¤è¿™äº›æ–‡æ¡£
                deleted_count = 0
                for obj in results.objects:
                    try:
                        collection.data.delete_by_id(obj.uuid)
                        deleted_count += 1
                    except Exception as e:
                        logger.warning(f"âš ï¸ åˆ é™¤æ–‡æ¡£å¤±è´¥ {obj.uuid}: {e}")
                
                logger.info(f"âœ… åˆ é™¤äº† {deleted_count} ä¸ªéœ€æ±‚æ–‡æ¡£ç›¸å…³è®°å½•")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºéœ€æ±‚æ–‡æ¡£æ•°æ®å¤±è´¥: {e}")
    
    def _is_meaningful_content(self, text: str) -> bool:
        """åˆ¤æ–­å†…å®¹æ˜¯å¦æœ‰æ„ä¹‰ï¼ˆè¿‡æ»¤æ ¼å¼åŒ–å†…å®¹ï¼‰"""
        text = text.strip()
        
        # è¿‡æ»¤æ¡ä»¶
        meaningless_patterns = [
            r'^æ–‡æ¡£æ§åˆ¶$',
            r'^ä¸­ä¼äº‘é“¾ï¼ˆåŒ—äº¬ï¼‰é‡‘èä¿¡æ¯æœåŠ¡æœ‰é™å…¬å¸$',
            r'^äº§å“éœ€æ±‚è¯´æ˜ä¹¦$',
            r'^éœ€æ±‚æ–‡æ¡£$',
            r'^\d+$',  # çº¯æ•°å­—
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+$',  # çº¯ä¸­æ–‡æ•°å­—
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« $',  # ç« èŠ‚æ ‡è®°
            r'^é™„å½•\s*[A-Z]?$',
            r'^ç›®å½•$',
            r'^ç´¢å¼•$',
        ]
        
        for pattern in meaningless_patterns:
            if re.match(pattern, text):
                return False
        
        # å†…å®¹å¤ªçŸ­ä¸”æ²¡æœ‰å®è´¨å†…å®¹
        if len(text) < 20 and not any(keyword in text for keyword in [
            'åŠŸèƒ½', 'æ¥å£', 'éœ€æ±‚', 'è®¾è®¡', 'å®ç°', 'è°ƒæ•´', 'æ–°å¢', 'ä¿®æ”¹', 'åˆ é™¤'
        ]):
            return False
        
        return True
    
    def _is_heading(self, paragraph, text: str) -> bool:
        """æ”¹è¿›çš„æ ‡é¢˜è¯†åˆ«"""
        # æ£€æŸ¥æ®µè½æ ·å¼
        if paragraph.style.name.startswith('Heading'):
            return True
        
        # æ£€æŸ¥æ•°å­—æ ¼å¼æ ‡é¢˜ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        if re.match(r'^\d+\.?\d*\s+.{2,}', text):  # è‡³å°‘2ä¸ªå­—ç¬¦çš„æ ‡é¢˜å†…å®¹
            return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ„ä¹‰çš„æ ‡é¢˜
        if len(text) < 100 and self._is_meaningful_content(text) and any(keyword in text for keyword in [
            'ç³»ç»Ÿ', 'æ¨¡å—', 'åŠŸèƒ½', 'æ¥å£', 'è®¾è®¡', 'æ¶æ„', 'éœ€æ±‚', 'èƒŒæ™¯', 'ç›®æ ‡', 'èŒƒå›´',
            'æµç¨‹', 'æ–¹æ¡ˆ', 'å®ç°', 'æµ‹è¯•', 'éƒ¨ç½²', 'è¿ç»´', 'æ€»ç»“', 'é™„å½•', 'è°ƒæ•´', 'è¯´æ˜'
        ]):
            return True
        
        return False
    
    def _smart_segment_content(self, markdown_content: str, file_path: Path) -> List[Dict[str, Any]]:
        """æ™ºèƒ½åˆ†æ®µç­–ç•¥"""
        sections = []
        lines = markdown_content.split('\n')
        
        current_section = file_path.stem
        current_content = []
        pending_short_sections = []  # å­˜å‚¨å¾…åˆå¹¶çš„çŸ­æ®µè½
        
        def process_section(section_title: str, section_content: List[str]) -> List[Dict[str, Any]]:
            """å¤„ç†å•ä¸ªæ®µè½"""
            content_text = '\n'.join(section_content).strip()
            
            # è¿‡æ»¤æ— æ„ä¹‰å†…å®¹
            if not self._is_meaningful_content(content_text):
                return []
            
            results = []
            content_length = len(content_text)
            
            if content_length < self.config['min_chunk_length']:
                # å¤ªçŸ­çš„æ®µè½æš‚å­˜ï¼Œç­‰å¾…åˆå¹¶
                return [{'section': section_title, 'content': content_text, 'need_merge': True}]
            
            elif content_length > self.config['max_chunk_length']:
                # å¤ªé•¿çš„æ®µè½è¿›è¡ŒäºŒæ¬¡åˆ†å‰²
                logger.info(f"ğŸ“ æ®µè½è¿‡é•¿({content_length}å­—ç¬¦)ï¼Œè¿›è¡ŒäºŒæ¬¡åˆ†å‰²: {section_title}")
                
                # ä½¿ç”¨RecursiveCharacterTextSplitteråˆ†å‰²
                text_chunks = self.text_splitter.split_text(content_text)
                
                for i, chunk in enumerate(text_chunks):
                    if len(chunk.strip()) >= self.config['min_chunk_length']:
                        chunk_title = f"{section_title}" if len(text_chunks) == 1 else f"{section_title}(ç¬¬{i+1}éƒ¨åˆ†)"
                        results.append({
                            'section': chunk_title,
                            'content': chunk.strip(),
                            'need_merge': False
                        })
                
                return results
            
            else:
                # é•¿åº¦é€‚ä¸­çš„æ®µè½ç›´æ¥ä½¿ç”¨
                return [{'section': section_title, 'content': content_text, 'need_merge': False}]
        
        # ç¬¬ä¸€éï¼šæŒ‰æ ‡é¢˜åˆ†å‰²
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æµ‹æ ‡é¢˜è¡Œ
            if line.startswith('#'):
                # å¤„ç†ä¹‹å‰çš„æ®µè½
                if current_content:
                    section_results = process_section(current_section, current_content)
                    sections.extend(section_results)
                
                # å¼€å§‹æ–°æ®µè½
                current_section = line.lstrip('#').strip()
                current_content = []
            else:
                current_content.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
        if current_content:
            section_results = process_section(current_section, current_content)
            sections.extend(section_results)
        
        # ç¬¬äºŒéï¼šå¤„ç†éœ€è¦åˆå¹¶çš„çŸ­æ®µè½
        final_sections = []
        accumulated_content = []
        accumulated_titles = []
        
        for section in sections:
            if section.get('need_merge', False):
                # ç´¯ç§¯çŸ­æ®µè½
                accumulated_content.append(section['content'])
                accumulated_titles.append(section['section'])
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°åˆå¹¶é˜ˆå€¼
                total_length = sum(len(content) for content in accumulated_content)
                if total_length >= self.config['merge_threshold']:
                    # åˆå¹¶å¹¶è¾“å‡º
                    merged_title = f"{accumulated_titles[0]}ç­‰{len(accumulated_titles)}é¡¹" if len(accumulated_titles) > 1 else accumulated_titles[0]
                    merged_content = '\n\n'.join(accumulated_content)
                    
                    final_sections.append({
                        'section': merged_title,
                        'content': merged_content
                    })
                    
                    # æ¸…ç©ºç´¯ç§¯
                    accumulated_content = []
                    accumulated_titles = []
            else:
                # å…ˆè¾“å‡ºç´¯ç§¯çš„çŸ­æ®µè½ï¼ˆå¦‚æœæœ‰ï¼‰
                if accumulated_content:
                    merged_title = f"{accumulated_titles[0]}ç­‰{len(accumulated_titles)}é¡¹" if len(accumulated_titles) > 1 else accumulated_titles[0]
                    merged_content = '\n\n'.join(accumulated_content)
                    
                    final_sections.append({
                        'section': merged_title,
                        'content': merged_content
                    })
                    
                    accumulated_content = []
                    accumulated_titles = []
                
                # è¾“å‡ºå½“å‰æ®µè½
                final_sections.append({
                    'section': section['section'],
                    'content': section['content']
                })
        
        # å¤„ç†æœ€åå‰©ä½™çš„çŸ­æ®µè½
        if accumulated_content:
            merged_title = f"{accumulated_titles[0]}ç­‰{len(accumulated_titles)}é¡¹" if len(accumulated_titles) > 1 else accumulated_titles[0]
            merged_content = '\n\n'.join(accumulated_content)
            
            final_sections.append({
                'section': merged_title,
                'content': merged_content
            })
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ®µè½ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤æ®µè½
        if not final_sections:
            if self._is_meaningful_content(markdown_content):
                final_sections.append({
                    'section': file_path.stem,
                    'content': markdown_content
                })
        
        return final_sections
    
    def _docx_to_markdown(self, doc, file_path: Path) -> str:
        """å°†DOCXè½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        markdown_content = []
        
        try:
            # å¤„ç†æ®µè½æ–‡æœ¬
            for paragraph in doc.paragraphs:
                text = paragraph.text.strip()
                if not text:
                    continue
                
                # æ£€æµ‹æ ‡é¢˜
                if self._is_heading(paragraph, text):
                    # æ ¹æ®æ ‡é¢˜çº§åˆ«æ·»åŠ markdownæ ‡è®°
                    level = self._get_heading_level(paragraph, text)
                    markdown_content.append(f"{'#' * level} {text}")
                else:
                    # æ™®é€šæ®µè½
                    markdown_content.append(text)
            
            return "\n\n".join(markdown_content)
            
        except Exception as e:
            logger.error(f"DOCXè½¬Markdownå¤±è´¥ {file_path}: {e}")
            return ""
    
    def _get_heading_level(self, paragraph, text: str) -> int:
        """è·å–æ ‡é¢˜çº§åˆ«"""
        if paragraph.style.name.startswith('Heading'):
            try:
                return int(paragraph.style.name.replace('Heading ', ''))
            except:
                return 1
        
        # æ ¹æ®æ•°å­—æ ¼å¼åˆ¤æ–­çº§åˆ«
        match = re.match(r'^(\d+)\.?(\d*)\s+', text)
        if match:
            if match.group(2):  # æœ‰äºŒçº§æ•°å­—ï¼Œå¦‚ 3.1
                return 2
            else:  # åªæœ‰ä¸€çº§æ•°å­—ï¼Œå¦‚ 3
                return 1
        
        return 1
    
    def _extract_text_from_damaged_docx(self, file_path: Path) -> str:
        """ä»æŸåçš„å¤§DOCXæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
        import zipfile
        import xml.etree.ElementTree as ET
        
        try:
            text_content = []
            
            logger.info(f"ğŸ”§ å°è¯•ä½¿ç”¨zipfileæ–¹æ³•å¤„ç†æŸåçš„å¤§æ–‡æ¡£: {file_path.name}")
            
            with zipfile.ZipFile(file_path, 'r') as docx_zip:
                # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
                file_list = docx_zip.namelist()
                
                # å°è¯•è¯»å–ä¸»æ–‡æ¡£å†…å®¹
                main_doc_files = [
                    'word/document.xml',
                    'word/document2.xml', 
                    'document.xml'
                ]
                
                for doc_file in main_doc_files:
                    if doc_file in file_list:
                        try:
                            with docx_zip.open(doc_file) as xml_file:
                                xml_content = xml_file.read()
                                
                                # è§£æXMLå¹¶æå–æ–‡æœ¬
                                root = ET.fromstring(xml_content)
                                
                                # å®šä¹‰å‘½åç©ºé—´
                                namespaces = {
                                    'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
                                }
                                
                                # æå–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
                                for text_elem in root.findall('.//w:t', namespaces):
                                    if text_elem.text:
                                        text_content.append(text_elem.text)
                                
                                logger.info(f"âœ… æˆåŠŸä» {doc_file} æå–æ–‡æœ¬")
                                break
                                
                        except Exception as e:
                            logger.warning(f"âš ï¸ æ— æ³•å¤„ç† {doc_file}: {e}")
                            continue
                
                # å¦‚æœä¸»æ–‡æ¡£å¤±è´¥ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„æ–‡æœ¬æ–‡ä»¶
                if not text_content:
                    for file_name in file_list:
                        if file_name.endswith('.xml') and 'word' in file_name:
                            try:
                                with docx_zip.open(file_name) as xml_file:
                                    xml_content = xml_file.read().decode('utf-8', errors='ignore')
                                    # ç®€å•çš„æ­£åˆ™æå–æ–‡æœ¬
                                    import re
                                    text_matches = re.findall(r'<w:t[^>]*>([^<]+)</w:t>', xml_content)
                                    if text_matches:
                                        text_content.extend(text_matches)
                                        logger.info(f"âœ… é€šè¿‡æ­£åˆ™ä» {file_name} æå–æ–‡æœ¬")
                                        break
                            except Exception as e:
                                continue
            
            if text_content:
                # æ¸…ç†å’Œç»„ç»‡æ–‡æœ¬
                cleaned_text = []
                current_line = ""
                
                for text in text_content:
                    text = text.strip()
                    if not text:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æ®µè½çš„å¼€å§‹
                    if any(char in text for char in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']) and len(current_line) > 20:
                        current_line += text
                        cleaned_text.append(current_line)
                        current_line = ""
                    else:
                        current_line += text + " "
                
                # æ·»åŠ æœ€åä¸€è¡Œ
                if current_line.strip():
                    cleaned_text.append(current_line.strip())
                
                final_text = f"# {file_path.name}\n\n" + "\n\n".join(cleaned_text)
                
                logger.info(f"ğŸ“ æˆåŠŸæ¢å¤å¤§æ–‡æ¡£æ–‡æœ¬å†…å®¹ï¼Œå…± {len(cleaned_text)} æ®µè½")
                return final_text
            
        except Exception as e:
            logger.error(f"âŒ å¤§æ–‡æ¡£zipfileæ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
        
        return ""
    
    def _process_docx_file(self, file_path: Path) -> List[Document]:
        """å¤„ç† DOCX æ–‡ä»¶ - ä½¿ç”¨ä¼˜åŒ–çš„åˆ†æ®µç­–ç•¥ï¼Œæ”¯æŒå¤§æ–‡æ¡£å¤„ç†"""
        documents = []
        
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¤§æ–‡æ¡£éœ€è¦ç‰¹æ®Šå¤„ç†
            file_size = file_path.stat().st_size
            size_mb = file_size / (1024 * 1024)
            logger.info(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {file_path.name} ({size_mb:.1f}MB)")
            
            # å¤§æ–‡æ¡£é¢„è­¦å’Œå†…å­˜ä¼˜åŒ–
            if size_mb > 10:
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°å¤§æ–‡æ¡£ ({size_mb:.1f}MB)ï¼Œå¯ç”¨ç‰¹æ®Šå¤„ç†æ¨¡å¼")
                # å¯¹äºè¶…å¤§æ–‡æ¡£ï¼Œå¢åŠ å¤„ç†è¶…æ—¶å’Œå†…å­˜ä¼˜åŒ–
                import gc
                gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            
            # æ‰“å¼€DOCXæ–‡ä»¶
            try:
                # å¯¹äºå¤§æ–‡æ¡£ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„é”™è¯¯å¤„ç†
                if size_mb > 15:
                    logger.info(f"ğŸ“‹ è¶…å¤§æ–‡æ¡£å¤„ç†æ¨¡å¼: {file_path.name}")
                    # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
                    import signal
                    def timeout_handler(signum, frame):
                        raise TimeoutError("æ–‡æ¡£å¤„ç†è¶…æ—¶")
                    
                    signal.signal(signal.SIGALRM, timeout_handler)
                    signal.alarm(300)  # 5åˆ†é’Ÿè¶…æ—¶
                    
                    try:
                        doc = docx.Document(file_path)
                        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
                    except TimeoutError:
                        logger.error(f"âŒ æ–‡æ¡£å¤„ç†è¶…æ—¶: {file_path.name}")
                        return documents
                    except Exception as e:
                        signal.alarm(0)
                        raise e
                else:
                    doc = docx.Document(file_path)
                    
            except Exception as docx_error:
                logger.error(f"âŒ æ— æ³•æ‰“å¼€DOCXæ–‡ä»¶ {file_path}: {docx_error}")
                # å¯¹äºæŸåçš„å¤§æ–‡æ¡£ï¼Œå°è¯•ä½¿ç”¨zipfileæ–¹æ³•æ¢å¤
                if size_mb > 5:
                    logger.info(f"ğŸ”§ å°è¯•ä½¿ç”¨zipfileæ–¹æ³•æ¢å¤å¤§æ–‡æ¡£...")
                    try:
                        recovered_content = self._extract_text_from_damaged_docx(file_path)
                        if recovered_content:
                            logger.info(f"âœ… å¤§æ–‡æ¡£æ–‡æœ¬æ¢å¤æˆåŠŸ: {file_path.name}")
                            # åˆ›å»ºæ¢å¤æ–‡æ¡£
                            doc_obj = Document(
                                page_content=recovered_content,
                                metadata={
                                    'file_name': file_path.name,
                                    'project': 'LS',
                                    'file_type': 'docx',
                                    'source_type': 'requirement_doc',
                                    'title': f'{file_path.stem}_æ¢å¤å†…å®¹',
                                    'chunk_index': 0,
                                    'total_chunks': 1,
                                    'created_at': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
                                    'processing_strategy': 'recovered_large_doc',
                                    'file_size_mb': f"{size_mb:.1f}MB"
                                }
                            )
                            return [doc_obj]
                    except Exception as recovery_error:
                        logger.error(f"âŒ å¤§æ–‡æ¡£æ¢å¤ä¹Ÿå¤±è´¥: {recovery_error}")
                
                return documents
            
            # å°†DOCXè½¬æ¢ä¸ºMarkdown
            try:
                markdown_content = self._docx_to_markdown(doc, file_path)
                
                # å¤§æ–‡æ¡£å†…å­˜ä¼˜åŒ–ï¼šåŠæ—¶é‡Šæ”¾docå¯¹è±¡
                if size_mb > 10:
                    del doc
                    import gc
                    gc.collect()
                    
            except Exception as markdown_error:
                logger.error(f"âŒ Markdownè½¬æ¢å¤±è´¥ {file_path}: {markdown_error}")
                if size_mb > 10:
                    logger.info(f"ğŸ”§ å¤§æ–‡æ¡£è½¬æ¢å¤±è´¥ï¼Œå°è¯•ç®€åŒ–å¤„ç†...")
                    # å¯¹äºå¤§æ–‡æ¡£è½¬æ¢å¤±è´¥ï¼Œå°è¯•ç›´æ¥è¯»å–æ®µè½æ–‡æœ¬
                    try:
                        simple_content = []
                        for paragraph in doc.paragraphs:
                            text = paragraph.text.strip()
                            if text and len(text) > 10:  # è¿‡æ»¤çŸ­å†…å®¹
                                simple_content.append(text)
                        
                        if simple_content:
                            markdown_content = f"# {file_path.stem}\n\n" + "\n\n".join(simple_content)
                            logger.info(f"âœ… å¤§æ–‡æ¡£ç®€åŒ–å¤„ç†æˆåŠŸï¼Œæå–{len(simple_content)}ä¸ªæ®µè½")
                        else:
                            return documents
                    except Exception as e:
                        logger.error(f"âŒ å¤§æ–‡æ¡£ç®€åŒ–å¤„ç†ä¹Ÿå¤±è´¥: {e}")
                        return documents
                else:
                    return documents
            
            if not markdown_content:
                logger.warning(f"âš ï¸ æ–‡æ¡£å†…å®¹ä¸ºç©º: {file_path.name}")
                return documents
            
            # å¯¹äºè¶…å¤§æ–‡æ¡£ï¼Œé™åˆ¶å†…å®¹é•¿åº¦é¿å…å†…å­˜é—®é¢˜
            if size_mb > 15 and len(markdown_content) > 100000:
                logger.warning(f"âš ï¸ è¶…å¤§æ–‡æ¡£å†…å®¹æˆªæ–­: {file_path.name} (åŸé•¿åº¦: {len(markdown_content)})")
                markdown_content = markdown_content[:100000] + "\n\n[å†…å®¹å› æ–‡æ¡£è¿‡å¤§è¢«æˆªæ–­]"
            
            # ä½¿ç”¨æ™ºèƒ½åˆ†æ®µç­–ç•¥
            try:
                sections = self._smart_segment_content(markdown_content, file_path)
                
                # å¤§æ–‡æ¡£åˆ†æ®µæ•°é‡é™åˆ¶ï¼Œé¿å…äº§ç”Ÿè¿‡å¤šå‘é‡
                if size_mb > 10 and len(sections) > 100:
                    logger.warning(f"âš ï¸ å¤§æ–‡æ¡£åˆ†æ®µæ•°é‡é™åˆ¶: {file_path.name} (åŸåˆ†æ®µæ•°: {len(sections)}, é™åˆ¶ä¸ºå‰100ä¸ª)")
                    sections = sections[:100]
                    
            except Exception as segment_error:
                logger.error(f"âŒ æ™ºèƒ½åˆ†æ®µå¤±è´¥ {file_path}: {segment_error}")
                # é™çº§ä¸ºç®€å•åˆ†æ®µ
                sections = [{'section': file_path.stem, 'content': markdown_content[:5000]}]
            
            for i, section in enumerate(sections):
                # åˆ›å»ºLangChain Documentå¯¹è±¡
                document = Document(
                    page_content=section['content'],
                    metadata={
                        'file_name': file_path.name,
                        'project': 'LS',
                        'file_type': 'docx',
                        'source_type': 'requirement_doc',
                        'title': section['section'],
                        'chunk_index': i,
                        'total_chunks': len(sections),
                        'created_at': datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
                        'processing_strategy': 'optimized_hybrid',
                        'file_size_mb': f"{size_mb:.1f}MB"
                    }
                )
                documents.append(document)
            
            logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {file_path.name} ({len(documents)} ä¸ªæ®µè½)")
            
            # æ˜¾ç¤ºæ®µè½é•¿åº¦åˆ†å¸ƒ
            if documents:
                lengths = [len(doc.page_content) for doc in documents]
                logger.info(f"   ğŸ“ æ®µè½é•¿åº¦: æœ€çŸ­{min(lengths)}, æœ€é•¿{max(lengths)}, å¹³å‡{sum(lengths)//len(lengths)}")
                
                # å¤§æ–‡æ¡£é¢å¤–ç»Ÿè®¡
                if size_mb > 5:
                    short_count = sum(1 for l in lengths if l < 100)
                    long_count = sum(1 for l in lengths if l > 1000)
                    logger.info(f"   ğŸ“Š æ®µè½åˆ†å¸ƒ: çŸ­æ®µè½(<100å­—ç¬¦):{short_count}, é•¿æ®µè½(>1000å­—ç¬¦):{long_count}")
            
        except Exception as e:
            logger.error(f"âŒ DOCX æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
        
        return documents
    
    def _create_weaviate_schema(self):
        """åˆ›å»º Weaviate æ¨¡å¼"""
        try:
            collection_name = "Document"
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
            if self.weaviate_client.collections.exists(collection_name):
                logger.info(f"ğŸ“‹ é›†åˆ {collection_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return collection_name
            
            # åˆ›å»ºé›†åˆ
            self.weaviate_client.collections.create(
                name=collection_name,
                vectorizer_config=wvc.config.Configure.Vectorizer.none(),
                properties=[
                    wvc.config.Property(name="content", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="file_name", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="project", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="file_type", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="source_type", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="title", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="chunk_index", data_type=wvc.config.DataType.INT),
                    wvc.config.Property(name="total_chunks", data_type=wvc.config.DataType.INT),
                    wvc.config.Property(name="created_at", data_type=wvc.config.DataType.DATE),
                    wvc.config.Property(name="processing_strategy", data_type=wvc.config.DataType.TEXT)
                ]
            )
            
            logger.info(f"âœ… Weaviate é›†åˆåˆ›å»ºæˆåŠŸ: {collection_name}")
            return collection_name
            
        except Exception as e:
            logger.error(f"âŒ Weaviate é›†åˆåˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def _insert_documents_batch(self, documents: List[Document], collection_name: str):
        """æ‰¹é‡æ’å…¥æ–‡æ¡£åˆ°Weaviate"""
        try:
            collection = self.weaviate_client.collections.get(collection_name)
            
            # å‡†å¤‡æ‰¹é‡æ’å…¥çš„æ•°æ®
            with collection.batch.dynamic() as batch:
                for doc in documents:
                    # ç”Ÿæˆå‘é‡
                    vector = self.embeddings_model.encode(doc.page_content).tolist()
                    
                    # å‡†å¤‡å…ƒæ•°æ®
                    properties = {
                        "content": doc.page_content,
                        **doc.metadata
                    }
                    
                    # æ·»åŠ åˆ°æ‰¹æ¬¡
                    batch.add_object(
                        properties=properties,
                        vector=vector
                    )
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def scan_requirements_docs(self) -> List[Path]:
        """æ‰«æéœ€æ±‚æ–‡æ¡£ç›®å½•ï¼Œè·å–æ‰€æœ‰DOCXæ–‡ä»¶"""
        files = []
        
        try:
            if not self.requirements_docs_path.exists():
                logger.error(f"âŒ éœ€æ±‚æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨: {self.requirements_docs_path}")
                return files
            
            for file_path in self.requirements_docs_path.glob("*.docx"):
                if file_path.is_file() and not file_path.name.startswith('~'):  # æ’é™¤ä¸´æ—¶æ–‡ä»¶
                    files.append(file_path)
            
            # ç»Ÿè®¡æ–‡æ¡£å¤§å°ä¿¡æ¯
            if files:
                total_size = sum(f.stat().st_size for f in files)
                large_files = [f for f in files if f.stat().st_size > 10*1024*1024]  # >10MB
                
                logger.info(f"ğŸ“ æ‰«æå®Œæˆï¼Œæ‰¾åˆ° {len(files)} ä¸ªDOCXæ–‡ä»¶")
                logger.info(f"ğŸ“Š æ–‡æ¡£ç»Ÿè®¡:")
                logger.info(f"   ğŸ“‘ æ–‡æ¡£æ€»æ•°: {len(files)} ä¸ª")
                logger.info(f"   ğŸ“ æ€»å¤§å°: {total_size/(1024*1024):.1f}MB")
                logger.info(f"   âš ï¸  å¤§æ–‡æ¡£(>10MB): {len(large_files)} ä¸ª")
                
                if large_files:
                    logger.info(f"   ğŸ“‹ å¤§æ–‡æ¡£åˆ—è¡¨:")
                    for lf in large_files:
                        size_mb = lf.stat().st_size / (1024*1024)
                        logger.info(f"      - {lf.name}: {size_mb:.1f}MB")
            else:
                logger.info(f"ğŸ“ æ‰«æå®Œæˆï¼Œæœªæ‰¾åˆ°DOCXæ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"âŒ æ‰«æéœ€æ±‚æ–‡æ¡£ç›®å½•å¤±è´¥: {e}")
        
        return files
    
    def initialize_optimized_requirements_docs(self):
        """ä½¿ç”¨ä¼˜åŒ–ç­–ç•¥åˆå§‹åŒ–éœ€æ±‚æ–‡æ¡£å‘é‡åº“"""
        try:
            logger.info("ğŸš€ å¼€å§‹ä½¿ç”¨ä¼˜åŒ–ç­–ç•¥åˆå§‹åŒ–éœ€æ±‚æ–‡æ¡£å‘é‡åº“...")
            logger.info(f"ğŸ“‹ åˆ†æ®µç­–ç•¥é…ç½®: {self.config}")
            
            # 1. æ¸…ç©ºç°æœ‰çš„éœ€æ±‚æ–‡æ¡£æ•°æ®
            self._clear_weaviate_database()
            
            # 2. æ‰«æéœ€æ±‚æ–‡æ¡£æ–‡ä»¶
            files = self.scan_requirements_docs()
            if not files:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°DOCXæ–‡ä»¶")
                return
            
            # 3. åˆ›å»º Weaviate æ¨¡å¼
            collection_name = self._create_weaviate_schema()
            
            # 4. å¤„ç†æ–‡ä»¶
            all_documents = []
            successful_files = 0
            failed_files = 0
            
            logger.info(f"ğŸ”„ å¼€å§‹å¤„ç† {len(files)} ä¸ªæ–‡æ¡£æ–‡ä»¶...")
            logger.info("=" * 60)
            
            for i, file_path in enumerate(files, 1):
                file_size_mb = file_path.stat().st_size / (1024*1024)
                logger.info(f"ğŸ“„ [{i}/{len(files)}] å¼€å§‹å¤„ç†: {file_path.name} ({file_size_mb:.1f}MB)")
                
                try:
                    documents = self._process_docx_file(file_path)
                    if documents:
                        all_documents.extend(documents)
                        successful_files += 1
                        logger.info(f"âœ… [{i}/{len(files)}] å¤„ç†æˆåŠŸ: {file_path.name} -> {len(documents)} ä¸ªæ®µè½")
                    else:
                        failed_files += 1
                        logger.warning(f"âš ï¸ [{i}/{len(files)}] å¤„ç†ç»“æœä¸ºç©º: {file_path.name}")
                    
                except Exception as e:
                    failed_files += 1
                    logger.error(f"âŒ [{i}/{len(files)}] å¤„ç†å¤±è´¥: {file_path.name} - {e}")
                
                # æ˜¾ç¤ºè¿›åº¦ç»Ÿè®¡
                logger.info(f"ğŸ“Š å½“å‰è¿›åº¦: æˆåŠŸ{successful_files}ä¸ª, å¤±è´¥{failed_files}ä¸ª, ç´¯è®¡æ®µè½{len(all_documents)}ä¸ª")
                logger.info("-" * 60)
            
            # å¤„ç†å®Œæˆç»Ÿè®¡
            logger.info("ğŸ¯ æ–‡æ¡£å¤„ç†å®Œæˆï¼")
            logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            logger.info(f"   âœ… æˆåŠŸå¤„ç†: {successful_files} ä¸ªæ–‡ä»¶")
            logger.info(f"   âŒ å¤„ç†å¤±è´¥: {failed_files} ä¸ªæ–‡ä»¶")
            logger.info(f"   ğŸ“‘ ç”Ÿæˆæ®µè½: {len(all_documents)} ä¸ª")
            logger.info(f"   ğŸ“ˆ æˆåŠŸç‡: {successful_files/(successful_files+failed_files)*100:.1f}%")
            logger.info("=" * 60)
            
            if not all_documents:
                logger.warning("âš ï¸ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æ¡£")
                return
            
            # 5. æ‰¹é‡æ’å…¥åˆ°Weaviate
            logger.info(f"ğŸ“¤ å¼€å§‹æ‰¹é‡æ’å…¥æ–‡æ¡£åˆ°Weaviateï¼Œæ€»è®¡ {len(all_documents)} ä¸ªæ®µè½...")
            batch_size = 50
            
            for i in range(0, len(all_documents), batch_size):
                batch = all_documents[i:i + batch_size]
                try:
                    self._insert_documents_batch(batch, collection_name)
                    logger.info(f"âœ… æ‰¹æ¬¡ {i//batch_size + 1}/{(len(all_documents) + batch_size - 1)//batch_size} æ’å…¥æˆåŠŸ ({len(batch)} ä¸ªæ–‡æ¡£)")
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} æ’å…¥å¤±è´¥: {e}")
            
            # 6. ç»Ÿè®¡åˆ†æ
            lengths = [len(doc.page_content) for doc in all_documents]
            logger.info("ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
            logger.info(f"   ğŸ“‘ æ€»æ–‡æ¡£æ•°: {len(all_documents)}")
            logger.info(f"   ğŸ“ å¹³å‡é•¿åº¦: {sum(lengths)//len(lengths)} å­—ç¬¦")
            logger.info(f"   ğŸ“ é•¿åº¦èŒƒå›´: {min(lengths)} - {max(lengths)} å­—ç¬¦")
            
            # é•¿åº¦åˆ†å¸ƒ
            short_count = sum(1 for l in lengths if l < 100)
            medium_count = sum(1 for l in lengths if 100 <= l < 400)
            long_count = sum(1 for l in lengths if l >= 400)
            
            logger.info(f"   ğŸ“‹ é•¿åº¦åˆ†å¸ƒ:")
            logger.info(f"      çŸ­æ®µè½(<100å­—ç¬¦): {short_count} ä¸ª ({short_count/len(lengths)*100:.1f}%)")
            logger.info(f"      ä¸­æ®µè½(100-399å­—ç¬¦): {medium_count} ä¸ª ({medium_count/len(lengths)*100:.1f}%)")  
            logger.info(f"      é•¿æ®µè½(â‰¥400å­—ç¬¦): {long_count} ä¸ª ({long_count/len(lengths)*100:.1f}%)")
            
            logger.info("ğŸ‰ ä¼˜åŒ–ç‰ˆéœ€æ±‚æ–‡æ¡£å‘é‡åº“åˆå§‹åŒ–å®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"âŒ éœ€æ±‚æ–‡æ¡£å‘é‡åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.weaviate_client:
            self.weaviate_client.close()

def main():
    """ä¸»å‡½æ•°"""
    initializer = None
    
    try:
        # åˆ›å»ºä¼˜åŒ–ç‰ˆåˆå§‹åŒ–å™¨
        initializer = OptimizedRequirementsDocsInitializer()
        
        # ä½¿ç”¨ä¼˜åŒ–ç­–ç•¥åˆå§‹åŒ–éœ€æ±‚æ–‡æ¡£å‘é‡åº“
        initializer.initialize_optimized_requirements_docs()
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if initializer:
            initializer.close()

if __name__ == "__main__":
    main()