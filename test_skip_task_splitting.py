#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è·³è¿‡ä»»åŠ¡æ‹†è§£é˜¶æ®µçš„æµ‹è¯•è„šæœ¬
ç›´æ¥ä½¿ç”¨ç°æœ‰æ•°æ®åº“æ•°æ®æµ‹è¯•åç»­èŠ‚ç‚¹
"""

import asyncio
import logging
import sqlite3
import json
import uuid
from datetime import datetime
from typing import Dict, Any, List
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥å·¥ä½œæµç¨‹ç›¸å…³æ¨¡å—
from src.corder_integration.langgraph.workflow_orchestrator import (
    LangGraphWorkflowOrchestrator, 
    CodingAgentState
)
from src.corder_integration.langgraph.nodes.task_splitting_node import TaskStorageManager

class TaskDataParser:
    """ä»»åŠ¡æ•°æ®è§£æå™¨"""
    
    def parse_markdown_tasks(self, file_path: str) -> List[Dict[str, Any]]:
        """è§£æMarkdownè¡¨æ ¼ä¸­çš„ä»»åŠ¡æ•°æ®"""
        logger.info(f"ğŸ“Š å¼€å§‹è§£æä»»åŠ¡æ–‡ä»¶: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        tasks = []
        lines = content.strip().split('\n')
        
        # è·³è¿‡è¡¨å¤´å’Œåˆ†éš”çº¿
        data_lines = lines[2:]  # è·³è¿‡è¡¨å¤´å’Œåˆ†éš”çº¿
        
        for line in data_lines:
            if line.strip() and '|' in line:
                # è§£æè¡¨æ ¼è¡Œ
                columns = [col.strip() for col in line.split('|')[1:-1]]  # å»æ‰é¦–å°¾ç©ºå…ƒç´ 
                
                if len(columns) >= 11:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—
                    task = {
                        'task_id': columns[0],
                        'service_name': columns[1],
                        'task_type': columns[2],
                        'priority': int(columns[3]) if columns[3].isdigit() else 1,
                        'status': columns[4],
                        'dependencies': self._parse_json_field(columns[5]),
                        'estimated_duration': columns[6],
                        'description': columns[7],
                        'deliverables': self._parse_json_field(columns[8]),
                        'implementation_details': columns[9],
                        'completion_criteria': columns[10],
                        'parameters': self._parse_json_field(columns[11]) if len(columns) > 11 else {}
                    }
                    tasks.append(task)
        
        logger.info(f"âœ… æˆåŠŸè§£æ {len(tasks)} ä¸ªä»»åŠ¡")
        return tasks
    
    def _parse_json_field(self, field_str: str) -> Any:
        """è§£æJSONå­—æ®µ"""
        try:
            # å¤„ç†è½¬ä¹‰å­—ç¬¦
            cleaned = field_str.replace('\\u', '\\u').replace('\\"', '"')
            return json.loads(cleaned)
        except (json.JSONDecodeError, ValueError):
            # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ç®€å•çš„åˆ—è¡¨è§£æ
            if field_str.startswith('[') and field_str.endswith(']'):
                try:
                    # ç®€å•çš„æ•°ç»„è§£æ
                    content = field_str[1:-1].strip()
                    if not content:
                        return []
                    items = [item.strip(' "') for item in content.split(',')]
                    return items
                except:
                    return []
            return field_str if field_str != '[]' else []

class TestWorkflowManager:
    """æµ‹è¯•å·¥ä½œæµç¨‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.db_path = "coding_agent_workflow.db"
        self.task_storage = TaskStorageManager(self.db_path)
        self.orchestrator = LangGraphWorkflowOrchestrator(
            use_sqlite=True,
            db_path="workflow_checkpoints.db"
        )
    
    def load_tasks_from_database(self) -> List[Dict[str, Any]]:
        """ä»æ•°æ®åº“åŠ è½½task_001åˆ°task_012çš„ä»»åŠ¡æ•°æ®"""
        logger.info("ğŸ“‹ ä»æ•°æ®åº“åŠ è½½ç°æœ‰ä»»åŠ¡æ•°æ®...")
        
        try:
            # ç›´æ¥ä»æ•°æ®åº“è¯»å–æŒ‡å®šçš„ä»»åŠ¡
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # ğŸ” è°ƒè¯•ï¼šå…ˆæŸ¥çœ‹æ‰€æœ‰ä»»åŠ¡ID
                cursor.execute('SELECT task_id FROM execution_tasks ORDER BY task_id')
                all_task_ids = [row[0] for row in cursor.fetchall()]
                logger.info(f"ğŸ” æ•°æ®åº“ä¸­æ‰€æœ‰ä»»åŠ¡ID: {all_task_ids}")
                
                # æŸ¥è¯¢task_001åˆ°task_012çš„æ•°æ®
                task_ids = [f'task_{str(i).zfill(3)}' for i in range(1, 13)]  # task_001 åˆ° task_012
                logger.info(f"ğŸ” è¦æŸ¥è¯¢çš„ä»»åŠ¡ID: {task_ids}")
                
                # ğŸ” å…ˆæµ‹è¯•ç®€å•æŸ¥è¯¢
                cursor.execute("SELECT task_id FROM execution_tasks WHERE task_id = 'task_001'")
                test_result = cursor.fetchone()
                logger.info(f"ğŸ” task_001å•ç‹¬æŸ¥è¯¢ç»“æœ: {test_result}")
                
                placeholders = ','.join(['?' for _ in task_ids])
                query = f"""
                    SELECT task_id, service_name, task_type, priority, status, dependencies,
                           estimated_duration, description, deliverables,
                           implementation_details, completion_criteria, parameters
                    FROM execution_tasks 
                    WHERE task_id IN ({placeholders})
                    ORDER BY task_id
                """
                logger.info(f"ğŸ” æ‰§è¡ŒSQLæŸ¥è¯¢: {query}")
                logger.info(f"ğŸ” æŸ¥è¯¢å‚æ•°: {task_ids}")
                
                cursor.execute(query, task_ids)
                
                tasks = []
                for row in cursor.fetchall():
                    task = {
                        'task_id': row[0],
                        'service_name': row[1],
                        'task_type': row[2],
                        'priority': row[3],
                        'status': 'pending',  # é‡ç½®çŠ¶æ€ä¸ºpending
                        'dependencies': json.loads(row[5] or '[]'),
                        'estimated_duration': row[6],
                        'description': row[7],
                        'deliverables': json.loads(row[8] or '[]'),
                        'implementation_details': row[9],
                        'completion_criteria': row[10],
                        'parameters': json.loads(row[11] or '{}')
                    }
                    tasks.append(task)
                
                logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(tasks)} ä¸ªä»»åŠ¡")
                return tasks
                
        except Exception as e:
            logger.error(f"âŒ ä»æ•°æ®åº“åŠ è½½ä»»åŠ¡å¤±è´¥: {e}")
            return []
    
    def reset_task_status(self, task_ids: List[str]) -> bool:
        """é‡ç½®æŒ‡å®šä»»åŠ¡çš„çŠ¶æ€ä¸ºpending"""
        logger.info("ğŸ”„ é‡ç½®ä»»åŠ¡çŠ¶æ€ä¸ºpending...")
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # é‡ç½®çŠ¶æ€
                placeholders = ','.join(['?' for _ in task_ids])
                cursor.execute(f"""
                    UPDATE execution_tasks 
                    SET status = 'pending', updated_at = CURRENT_TIMESTAMP
                    WHERE task_id IN ({placeholders})
                """, task_ids)
                
                logger.info(f"âœ… æˆåŠŸé‡ç½® {len(task_ids)} ä¸ªä»»åŠ¡çŠ¶æ€")
                return True
                
        except Exception as e:
            logger.error(f"âŒ é‡ç½®ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def setup_test_database(self, tasks: List[Dict[str, Any]]) -> bool:
        """è®¾ç½®æµ‹è¯•æ•°æ®åº“ï¼Œé‡ç½®ä»»åŠ¡çŠ¶æ€"""
        logger.info("ğŸ”§ è®¾ç½®æµ‹è¯•ç¯å¢ƒ...")
        
        try:
            # æå–ä»»åŠ¡IDåˆ—è¡¨
            task_ids = [task['task_id'] for task in tasks]
            
            # é‡ç½®ä»»åŠ¡çŠ¶æ€
            success = self.reset_task_status(task_ids)
            
            if success:
                logger.info(f"âœ… æˆåŠŸé‡ç½® {len(tasks)} ä¸ªä»»åŠ¡çŠ¶æ€")
                
                # éªŒè¯æ•°æ®
                pending_tasks = self.task_storage.get_pending_tasks()
                logger.info(f"ğŸ“‹ æ•°æ®åº“ä¸­å…±æœ‰ {len(pending_tasks)} ä¸ªå¾…æ‰§è¡Œä»»åŠ¡")
                return True
            else:
                logger.error("âŒ é‡ç½®ä»»åŠ¡çŠ¶æ€å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ è®¾ç½®æµ‹è¯•æ•°æ®åº“å¤±è´¥: {e}")
            return False
    
    def create_initial_state(self, tasks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ›å»ºåˆå§‹çŠ¶æ€ï¼Œæ¨¡æ‹Ÿä»»åŠ¡æ‹†è§£å®Œæˆçš„ç»“æœ"""
        logger.info("ğŸ—ï¸ åˆ›å»ºåˆå§‹çŠ¶æ€...")
        
        # ä»ä»»åŠ¡ä¸­æå–æœåŠ¡ä¿¡æ¯
        services = list(set([task['service_name'] for task in tasks if task['service_name'] != 'ç³»ç»Ÿ']))
        
        # æ„å»ºæœåŠ¡ä¾èµ–å…³ç³»
        service_dependencies = {}
        for service in services:
            service_dependencies[service] = []
        
        # ä»ä»»åŠ¡ä¾èµ–ä¸­æ¨æ–­æœåŠ¡ä¾èµ–
        for task in tasks:
            if task['dependencies']:
                # æŸ¥æ‰¾ä¾èµ–ä»»åŠ¡çš„æœåŠ¡
                for dep_task_id in task['dependencies']:
                    dep_task = next((t for t in tasks if t['task_id'] == dep_task_id), None)
                    if dep_task and dep_task['service_name'] != task['service_name']:
                        if task['service_name'] not in service_dependencies:
                            service_dependencies[task['service_name']] = []
                        if dep_task['service_name'] not in service_dependencies[task['service_name']]:
                            service_dependencies[task['service_name']].append(dep_task['service_name'])
        
        # æ„å»ºä»»åŠ¡æ‰§è¡Œè®¡åˆ’
        task_execution_plan = {
            'total_tasks': len(tasks),
            'services_count': len(services),
            'phases': [
                {'phase': 'git_management', 'tasks': [t for t in tasks if t['task_type'] in ['git_extraction', 'git_clone']]},
                {'phase': 'intelligent_coding', 'tasks': [t for t in tasks if t['task_type'] in ['code_analysis', 'database', 'api', 'config']]},
                {'phase': 'testing', 'tasks': [t for t in tasks if t['task_type'] in ['integration_test']]},
                {'phase': 'deployment', 'tasks': [t for t in tasks if t['task_type'] in ['deployment']]}
            ]
        }
        
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = {
            # è¾“å…¥çŠ¶æ€
            'design_doc': "æ¨¡æ‹Ÿè®¾è®¡æ–‡æ¡£ - ç”¨æˆ·æœåŠ¡å’Œç¡®æƒå¼€ç«‹æœåŠ¡çš„å¾®æœåŠ¡æ¶æ„è®¾è®¡",
            'project_name': "zqyl_microservices_test",
            
            # ä»»åŠ¡æ‹†åˆ†ç»“æœï¼ˆæ¨¡æ‹Ÿå®Œæˆï¼‰
            'identified_services': services,
            'service_dependencies': service_dependencies,
            'task_execution_plan': task_execution_plan,
            'parallel_tasks': [t for t in tasks if not t['dependencies']],
            
            # Gitç®¡ç†çŠ¶æ€
            'git_repo_url': None,
            'target_branch': "feature/test_implementation",
            'project_paths': {
                'ç”¨æˆ·æœåŠ¡': './zqyl-user-center-service',
                'ç¡®æƒå¼€ç«‹æœåŠ¡': './crcl-open'
            },
            'output_path': './workspace/test_project',
            'repo_initialized': False,
            
            # ä»£ç ç”ŸæˆçŠ¶æ€
            'generated_services': {},
            'generated_apis': {},
            'generated_sql': {},
            'service_interconnections': {},
            
            # æµ‹è¯•çŠ¶æ€
            'unit_test_results': {},
            'test_coverage': {},
            'interface_compatibility': {},
            
            # è´¨é‡æ£€æŸ¥çŠ¶æ€
            'code_review_results': {},
            'static_analysis_results': {},
            'security_scan_results': {},
            
            # Gitæäº¤çŠ¶æ€
            'commit_hashes': {},
            'push_results': {},
            'pr_urls': {},
            
            # æ‰§è¡Œæ§åˆ¶çŠ¶æ€
            'current_phase': 'git_management',  # è·³è¿‡task_splittingï¼Œç›´æ¥ä»git_managementå¼€å§‹
            'completed_services': [],
            'failed_services': [],
            'retry_count': 0,
            'execution_errors': []
        }
        
        logger.info(f"âœ… åˆå§‹çŠ¶æ€åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(services)} ä¸ªæœåŠ¡")
        return initial_state
    
    async def run_test_workflow(self, initial_state: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œæµ‹è¯•å·¥ä½œæµç¨‹ï¼Œä»git_managementèŠ‚ç‚¹å¼€å§‹"""
        logger.info("ğŸš€ å¼€å§‹æµ‹è¯•å·¥ä½œæµç¨‹...")
        
        try:
            # è·å–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
            checkpointer_context = self.orchestrator._get_checkpointer_context()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            if hasattr(checkpointer_context, '__aenter__'):
                # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                async with checkpointer_context as checkpointer:
                    return await self._execute_with_checkpointer(checkpointer, initial_state)
            else:
                # ç›´æ¥ä½¿ç”¨æ£€æŸ¥ç‚¹ç®¡ç†å™¨
                return await self._execute_with_checkpointer(checkpointer_context, initial_state)
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    async def _execute_with_checkpointer(self, checkpointer, initial_state: Dict[str, Any]) -> Dict[str, Any]:
        """ä½¿ç”¨æ£€æŸ¥ç‚¹ç®¡ç†å™¨æ‰§è¡Œå·¥ä½œæµç¨‹"""
        # ç¼–è¯‘å·¥ä½œæµå›¾
        compiled_graph = self.orchestrator.graph.compile(checkpointer=checkpointer)
        
        # ç”Ÿæˆé…ç½®å’Œçº¿ç¨‹ID
        config = {
            "configurable": {
                "thread_id": f"test_workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            }
        }
        
        # ä»git_managementèŠ‚ç‚¹å¼€å§‹æ‰§è¡Œï¼ˆè·³è¿‡task_splittingï¼‰
        logger.info("â­ï¸ è·³è¿‡ä»»åŠ¡æ‹†è§£ï¼Œä»Gitç®¡ç†èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ...")
        
        # ç›´æ¥è°ƒç”¨git_managementèŠ‚ç‚¹å¼€å§‹
        final_state = await compiled_graph.ainvoke(
            initial_state,
            config=config
        )
        
        logger.info("âœ… æµ‹è¯•å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆ")
        return final_state
    
    def generate_test_report(self, final_state: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        
        report = {
            'test_summary': {
                'start_phase': 'git_management',
                'final_phase': final_state.get('current_phase', 'unknown'),
                'completed_services': final_state.get('completed_services', []),
                'failed_services': final_state.get('failed_services', []),
                'execution_errors': final_state.get('execution_errors', [])
            },
            'git_management_results': {
                'repo_initialized': final_state.get('repo_initialized', False),
                'project_paths': final_state.get('project_paths', {})
            },
            'coding_results': {
                'generated_services': len(final_state.get('generated_services', {})),
                'generated_apis': len(final_state.get('generated_apis', {})),
                'generated_sql': len(final_state.get('generated_sql', {}))
            },
            'test_results': {
                'unit_test_results': final_state.get('unit_test_results', {}),
                'test_coverage': final_state.get('test_coverage', {})
            }
        }
        
        logger.info("âœ… æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return report

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ¯ å¼€å§‹è·³è¿‡ä»»åŠ¡æ‹†è§£çš„å·¥ä½œæµç¨‹æµ‹è¯•")
    
    try:
        # 1. è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        test_manager = TestWorkflowManager()
        
        # 2. ä»æ•°æ®åº“åŠ è½½ç°æœ‰ä»»åŠ¡æ•°æ® (task_001åˆ°task_012)
        tasks = test_manager.load_tasks_from_database()
        
        if not tasks:
            logger.error("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°task_001åˆ°task_012çš„ä»»åŠ¡æ•°æ®")
            return
        
        logger.info(f"ğŸ“‹ æˆåŠŸåŠ è½½ {len(tasks)} ä¸ªä»»åŠ¡ï¼š{[t['task_id'] for t in tasks]}")
        
        # 3. é‡ç½®ä»»åŠ¡çŠ¶æ€
        if not test_manager.setup_test_database(tasks):
            logger.error("âŒ ä»»åŠ¡çŠ¶æ€é‡ç½®å¤±è´¥")
            return
        
        # 4. åˆ›å»ºåˆå§‹çŠ¶æ€
        initial_state = test_manager.create_initial_state(tasks)
        
        # 5. è¿è¡Œå·¥ä½œæµç¨‹
        final_state = await test_manager.run_test_workflow(initial_state)
        
        # 6. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        report = test_manager.generate_test_report(final_state)
        
        # 7. è¾“å‡ºç»“æœ
        print("\n" + "="*60)
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print("="*60)
        print(f"æœ€ç»ˆé˜¶æ®µ: {report['test_summary']['final_phase']}")
        print(f"å®Œæˆçš„æœåŠ¡: {report['test_summary']['completed_services']}")
        print(f"å¤±è´¥çš„æœåŠ¡: {report['test_summary']['failed_services']}")
        print(f"æ‰§è¡Œé”™è¯¯: {len(report['test_summary']['execution_errors'])}")
        print("="*60)
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        import json
        report_file = f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'test_report': report,
                'final_state': final_state
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main()) 