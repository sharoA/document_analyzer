#!/usr/bin/env python3
"""
OpenAI APIå®¢æˆ·ç«¯
ä¸“é—¨ç”¨äºOpenAI APIçš„å°è£…å’Œè°ƒç”¨
"""

import os
import logging
import time
from typing import List, Dict, Optional, Any, Generator
from dataclasses import dataclass

try:
    from openai import OpenAI
except ImportError:
    raise ImportError("è¯·å®‰è£…OpenAIåŒ…: pip install openai")

try:
    from ..config import settings
    from .llm_logger import log_llm_request, log_llm_response, log_llm_stream_chunk
except ImportError:
    from dotenv import load_dotenv
    load_dotenv()
    
    class MockSettings:
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
        OPENAI_MODEL_ID = os.getenv("OPENAI_MODEL_ID", "gpt-3.5-turbo")
        OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        DEFAULT_TEMPERATURE = 0.7
        DEFAULT_MAX_TOKENS = 2000
        DEFAULT_TIMEOUT = 120
    
    settings = MockSettings()
    
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºç©ºçš„æ—¥å¿—å‡½æ•°
    def log_llm_request(*args, **kwargs):
        return f"openai_{int(time.time() * 1000)}"
    
    def log_llm_response(*args, **kwargs):
        pass
    
    def log_llm_stream_chunk(*args, **kwargs):
        pass

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

@dataclass
class OpenAIConfig:
    """OpenAIé…ç½®"""
    api_key: str
    model_id: str
    base_url: str = "https://api.openai.com/v1"
    temperature: float = 0.7
    max_tokens: int = 2000
    timeout: int = 120

class OpenAIClient:
    """OpenAIå®¢æˆ·ç«¯"""
    
    def __init__(self, config: Optional[OpenAIConfig] = None):
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        
        Args:
            config: OpenAIé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        if config is None:
            config = OpenAIConfig(
                api_key=settings.OPENAI_API_KEY,
                model_id=settings.OPENAI_MODEL_ID,
                base_url=settings.OPENAI_BASE_URL,
                temperature=settings.DEFAULT_TEMPERATURE,
                max_tokens=settings.DEFAULT_MAX_TOKENS,
                timeout=settings.DEFAULT_TIMEOUT
            )
        
        self.config = config
        
        if not self.config.api_key:
            raise ValueError("OpenAI APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®OPENAI_API_KEY")
        
        self.client = OpenAI(
            api_key=self.config.api_key,
            base_url=self.config.base_url,
            timeout=self.config.timeout
        )
        
        logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        **kwargs
    ) -> str:
        """
        èŠå¤©å¯¹è¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            AIå›å¤å†…å®¹
        """
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        request_params = {
            "temperature": temperature or self.config.temperature,
            "max_tokens": max_tokens or self.config.max_tokens,
            "stream": stream,
            **kwargs
        }
        
        # è®°å½•è¯·æ±‚æ—¥å¿—
        request_id = log_llm_request(
            provider="openai",
            model=self.config.model_id,
            messages=messages,
            parameters=request_params
        )
        
        start_time = time.time()
        
        try:
            response = self.client.chat.completions.create(
                model=self.config.model_id,
                messages=messages,
                **request_params
            )
            
            response_time = time.time() - start_time
            
            if stream:
                # è®°å½•æµå¼å“åº”å¼€å§‹
                log_llm_response(
                    request_id=request_id,
                    response_content="[STREAM_START]",
                    response_time=response_time,
                    token_usage=None
                )
                return response
            else:
                response_content = response.choices[0].message.content
                
                # æå–tokenä½¿ç”¨æƒ…å†µ
                token_usage = None
                if hasattr(response, 'usage') and response.usage:
                    token_usage = {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    }
                
                # è®°å½•å“åº”æ—¥å¿—
                log_llm_response(
                    request_id=request_id,
                    response_content=response_content,
                    response_time=response_time,
                    token_usage=token_usage
                )
                
                return response_content
                
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = str(e)
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            log_llm_response(
                request_id=request_id,
                response_content="",
                response_time=response_time,
                error=error_msg
            )
            
            logger.error(f"OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def stream_chat(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Generator[str, None, None]:
        """
        æµå¼èŠå¤©
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            æµå¼å“åº”å†…å®¹
        """
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        request_params = {
            "temperature": temperature or self.config.temperature,
            "max_tokens": max_tokens or self.config.max_tokens,
            "stream": True,
            **kwargs
        }
        
        # è®°å½•è¯·æ±‚æ—¥å¿—
        request_id = log_llm_request(
            provider="openai",
            model=self.config.model_id,
            messages=messages,
            parameters=request_params
        )
        
        start_time = time.time()
        full_content = ""
        
        try:
            response = self.client.chat.completions.create(
                model=self.config.model_id,
                messages=messages,
                **request_params
            )
            
            for chunk in response:
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    full_content += content
                    
                    # è®°å½•æµå¼å—
                    log_llm_stream_chunk(
                        request_id=request_id,
                        chunk_content=content,
                        chunk_index=len(full_content)
                    )
                    
                    yield content
            
            response_time = time.time() - start_time
            
            # è®°å½•å®Œæ•´å“åº”
            log_llm_response(
                request_id=request_id,
                response_content=full_content,
                response_time=response_time,
                token_usage=None  # æµå¼å“åº”é€šå¸¸ä¸è¿”å›tokenä½¿ç”¨æƒ…å†µ
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = str(e)
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            log_llm_response(
                request_id=request_id,
                response_content=full_content,
                response_time=response_time,
                error=error_msg
            )
            
            logger.error(f"OpenAIæµå¼APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def test_connection(self) -> bool:
        """
        æµ‹è¯•è¿æ¥
        
        Returns:
            è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            test_messages = [
                {
                    "role": "user",
                    "content": "Hello"
                }
            ]
            
            response = self.chat(test_messages, max_tokens=10)
            return bool(response)
            
        except Exception as e:
            logger.error(f"OpenAIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_openai_client = None

def get_openai_client() -> OpenAIClient:
    """è·å–å…¨å±€OpenAIå®¢æˆ·ç«¯å®ä¾‹"""
    global _openai_client
    if _openai_client is None:
        _openai_client = OpenAIClient()
    return _openai_client

# ä¾¿æ·å‡½æ•°
def openai_chat(messages: List[Dict[str, str]], **kwargs) -> str:
    """ä¾¿æ·çš„èŠå¤©å‡½æ•°"""
    return get_openai_client().chat(messages, **kwargs)

def openai_stream_chat(messages: List[Dict[str, str]], **kwargs) -> Generator[str, None, None]:
    """ä¾¿æ·çš„æµå¼èŠå¤©å‡½æ•°"""
    return get_openai_client().stream_chat(messages, **kwargs)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    try:
        client = OpenAIClient()
        
        # æµ‹è¯•è¿æ¥
        if client.test_connection():
            print("âœ… OpenAIè¿æ¥æµ‹è¯•æˆåŠŸ")
            
            # æµ‹è¯•èŠå¤©
            test_messages = [
                {
                    "role": "user",
                    "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
                }
            ]
            
            response = client.chat(test_messages)
            print(f"ğŸ¤– AIå›å¤: {response}")
            
        else:
            print("âŒ OpenAIè¿æ¥æµ‹è¯•å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}") 