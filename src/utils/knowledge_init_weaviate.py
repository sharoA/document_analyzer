#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†åº“åˆå§‹åŒ–è„šæœ¬ - Weaviate ç‰ˆæœ¬
å°† D:\knowledge_base ä¸‹çš„çŸ¥è¯†åº“å†…å®¹è½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨åˆ° Weaviate ä¸­
æ ¹æ® knowledge_init_weaviate.md è¦æ±‚å®ç°ï¼Œä½¿ç”¨ LangChain æ¡†æ¶è¿›è¡Œ RAG
"""

import os
import re
import uuid
import json
import logging
import hashlib
import shutil
import base64
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from io import BytesIO

# æ–‡ä»¶å¤„ç†ç›¸å…³
import docx
from openpyxl import load_workbook
from PIL import Image
import pytesseract
from transformers import BlipProcessor, BlipForConditionalGeneration

# å‘é‡åŒ–å’ŒåµŒå…¥
from sentence_transformers import SentenceTransformer

# Weaviate å’Œ LangChain
import weaviate
import weaviate.classes as wvc
from weaviate.auth import AuthApiKey
from langchain_community.vectorstores import Weaviate as LangChainWeaviate
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# é¡¹ç›®é…ç½®å’Œ Redis
try:
    from ..resource.config import get_config
    from .redis_util import get_redis_manager
    from .weaviate_helper import get_weaviate_client
except ImportError:
    import sys
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    sys.path.insert(0, project_root)
    from src.resource.config import get_config
    from src.utils.redis_util import get_redis_manager
    from src.utils.weaviate_helper import get_weaviate_client

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class KnowledgeBaseInitializer:
    """çŸ¥è¯†åº“åˆå§‹åŒ–å™¨"""
    
    def __init__(self, knowledge_base_path: str = "D:\\knowledge_base"):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“å¤„ç†å™¨
        
        Args:
            knowledge_base_path: çŸ¥è¯†åº“æ ¹ç›®å½•è·¯å¾„
        """
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ä»¥ä½¿ç”¨_call_llmæ–¹æ³•
        self.llm_client = None
        self._initialize_llm_client()
        
        self.knowledge_base_path = Path(knowledge_base_path)
        self.weaviate_client = None
        self.redis_manager = None
        self.embeddings_model = None
        self.langchain_embeddings = None
        self.langchain_vectorstore = None
        self.text_splitter = None
        self.blip_processor = None
        self.blip_model = None
        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        self.supported_extensions = {'.docx', '.xlsx', '.java', '.xml'}
        
        # å›¾ç‰‡è¾“å‡ºç›®å½• - æŒ‰è¦æ±‚è®¾ç½®ä¸ºæŒ‡å®šè·¯å¾„
        self.image_output_dir = self.knowledge_base_path / "é“¾æ•°_LS" / "éœ€æ±‚æ–‡æ¡£" / "éœ€æ±‚æ–‡æ¡£å›¾ç‰‡"
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
    
    def _initialize_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            from ..utils.volcengine_client import VolcengineClient
            config = get_config()
            volcengine_config = config.get_volcengine_config()
            self.llm_client = VolcengineClient(volcengine_config)
            logger.info("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.llm_client = None
    
    def _call_llm(self, prompt: str, system_prompt: str = None, max_tokens: int = 4000) -> Optional[str]:
        """
        è°ƒç”¨LLMè¿›è¡Œåˆ†æ
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬
        """
        if not self.llm_client:
            logger.warning("LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return None
        
        try:
            response = self.llm_client.chat(
                messages=[
                    {"role": "system", "content": system_prompt or "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens
            )
            return response
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _initialize_components(self):
        """åˆå§‹åŒ–å„ç§ç»„ä»¶"""
        try:
            # åˆå§‹åŒ– Weaviate å®¢æˆ·ç«¯
            self.weaviate_client = get_weaviate_client()
            logger.info("âœ… Weaviate å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ– Redis ç®¡ç†å™¨
            self.redis_manager = get_redis_manager()
            logger.info("âœ… Redis ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - ä½¿ç”¨bge-large-zhï¼ˆ1024ç»´ï¼Œä¸­æ–‡ä¼˜åŒ–ï¼‰
            self.embeddings_model = SentenceTransformer('BAAI/bge-large-zh')
            logger.info("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ (bge-large-zh, 1024ç»´ï¼Œä¸­æ–‡ä¼˜åŒ–)")
            
            # åˆå§‹åŒ– LangChain åµŒå…¥æ¨¡å‹
            self.langchain_embeddings = SentenceTransformerEmbeddings(
                model_name='BAAI/bge-large-zh'
            )
            logger.info("âœ… LangChain åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
            )
            logger.info("âœ… æ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆ›å»ºå›¾ç‰‡è¾“å‡ºç›®å½•
            self.image_output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"âœ… å›¾ç‰‡è¾“å‡ºç›®å½•åˆ›å»ºæˆåŠŸ: {self.image_output_dir}")
            
            logger.info("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _load_image_caption_model(self):
        """å»¶è¿ŸåŠ è½½å›¾ç‰‡æè¿°æ¨¡å‹"""
        if self.blip_processor is None:
            try:
                self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
                self.blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
                logger.info("âœ… å›¾ç‰‡æè¿°æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ å›¾ç‰‡æè¿°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.blip_processor = None
                self.blip_model = None
    
    def _get_cache_key(self, file_path: str, cache_type: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”® - æŒ‰è¦æ±‚ä½¿ç”¨æŒ‡å®šæ ¼å¼"""
        return f"file:{file_path}:{cache_type}"
    
    def _get_cached_result(self, file_path: str, cache_type: str) -> Optional[Any]:
        """è·å–ç¼“å­˜ç»“æœ"""
        try:
            cache_key = self._get_cache_key(file_path, cache_type)
            return self.redis_manager.get(cache_key, use_prefix=False)
        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜å¤±è´¥ {file_path}:{cache_type} - {e}")
            return None
    
    def _set_cached_result(self, file_path: str, cache_type: str, result: Any, ttl: int = 86400):
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        try:
            cache_key = self._get_cache_key(file_path, cache_type)
            self.redis_manager.set(cache_key, result, ttl=ttl, use_prefix=False)
        except Exception as e:
            logger.warning(f"è®¾ç½®ç¼“å­˜å¤±è´¥ {file_path}:{cache_type} - {e}")
    
    def _extract_project_name(self, file_path: Path) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æå–é¡¹ç›®åç§°"""
        try:
            # è·å–ç›¸å¯¹äºçŸ¥è¯†åº“æ ¹ç›®å½•çš„è·¯å¾„
            relative_path = file_path.relative_to(self.knowledge_base_path)
            parts = relative_path.parts
            
            # æŒ‰è¦æ±‚ï¼Œå°†ç›®å½•ç»“æ„å­˜å‚¨ä¸ºLS
            if len(parts) >= 1:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šçš„é¡¹ç›®æ ‡è¯†
                for part in parts:
                    if 'zqyl-ls' in part.lower():
                        return 'zqyl-ls'
                    elif 'ls' in part.lower() or 'é“¾æ•°' in part:
                        return 'LS'
            
            # é»˜è®¤è¿”å›LS
            return "LS"
            
        except Exception as e:
            logger.warning(f"æå–é¡¹ç›®åç§°å¤±è´¥ {file_path}: {e}")
            return "LS"
    
    def _clear_weaviate_database(self):
        """æ¸…ç©ºWeaviateæ•°æ®åº“æ‰€æœ‰æ•°æ®"""
        try:
            logger.info("ğŸ—‘ï¸ å¼€å§‹æ¸…ç©ºWeaviateæ•°æ®åº“...")
            
            # è·å–æ‰€æœ‰é›†åˆ
            collections = self.weaviate_client.collections.list_all()
            
            for collection_name in collections:
                try:
                    # åˆ é™¤é›†åˆ
                    self.weaviate_client.collections.delete(collection_name)
                    logger.info(f"âœ… åˆ é™¤é›†åˆ: {collection_name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ é™¤é›†åˆå¤±è´¥ {collection_name}: {e}")
            
            logger.info("âœ… Weaviateæ•°æ®åº“æ¸…ç©ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºWeaviateæ•°æ®åº“å¤±è´¥: {e}")
            raise
    
    def _docx_to_markdown(self, doc, file_path: Path) -> Tuple[str, List[Dict[str, Any]]]:
        """å°†DOCXè½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œå¹¶æå–å›¾ç‰‡ä¿¡æ¯"""
        markdown_content = []
        image_info = []
        
        # ç¡®ä¿å›¾ç‰‡è¾“å‡ºç›®å½•å­˜åœ¨
        self.image_output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # é¦–å…ˆæå–æ‰€æœ‰å›¾ç‰‡
            image_info = self._extract_images_from_docx(doc, file_path)
            
            # å¤„ç†æ®µè½æ–‡æœ¬
            for paragraph in doc.paragraphs:
                text = paragraph.text.strip()
                if not text:
                    continue
                
                # æ£€æµ‹æ ‡é¢˜
                if self._is_heading(paragraph, text):
                    # æ ¹æ®æ ‡é¢˜çº§åˆ«æ·»åŠ markdownæ ‡è®°
                    level = self._get_heading_level(paragraph, text)
                    markdown_content.append(f"{'#' * level} {text}")
                else:
                    # æ™®é€šæ®µè½
                    markdown_content.append(text)
            
            # å¦‚æœæå–åˆ°å›¾ç‰‡ï¼Œåœ¨å†…å®¹æœ«å°¾æ·»åŠ å›¾ç‰‡å¼•ç”¨
            if image_info:
                markdown_content.append("\n## æ–‡æ¡£å›¾ç‰‡")
                for img in image_info:
                    markdown_content.append(f"![{img['name']}]({img['path']})")
            
            return "\n\n".join(markdown_content), image_info
            
        except Exception as e:
            logger.error(f"DOCXè½¬Markdownå¤±è´¥ {file_path}: {e}")
            return "", []
    
    def _extract_images_from_docx(self, doc, file_path: Path) -> List[Dict[str, Any]]:
        """ä»DOCXæ–‡æ¡£ä¸­æå–å›¾ç‰‡"""
        image_info = []
        image_counter = 1
        
        try:
            # è·å–æ–‡æ¡£çš„å…³ç³»éƒ¨åˆ†
            document_part = doc.part
            
            # éå†æ‰€æœ‰å…³ç³»ï¼ŒæŸ¥æ‰¾å›¾ç‰‡
            for rel_id, relationship in document_part.rels.items():
                if "image" in relationship.target_ref:
                    try:
                        # è·å–å›¾ç‰‡æ•°æ®
                        image_part = relationship.target_part
                        image_data = image_part.blob
                        
                        # ç¡®å®šå›¾ç‰‡æ‰©å±•å
                        content_type = image_part.content_type
                        if 'png' in content_type:
                            ext = '.png'
                        elif 'jpeg' in content_type or 'jpg' in content_type:
                            ext = '.jpg'
                        elif 'gif' in content_type:
                            ext = '.gif'
                        else:
                            ext = '.png'  # é»˜è®¤ä½¿ç”¨png
                        
                        # ç”Ÿæˆå›¾ç‰‡æ–‡ä»¶åå’Œè·¯å¾„
                        image_name = f"{file_path.stem}_image_{image_counter:03d}{ext}"
                        image_path = self.image_output_dir / image_name
                        
                        # ä¿å­˜å¹¶å¯èƒ½å‹ç¼©å›¾ç‰‡
                        original_size = len(image_data)
                        if original_size > 1024 * 1024:  # å¤§äº1MBçš„å›¾ç‰‡è¿›è¡Œå‹ç¼©
                            try:
                                # å…ˆä¿å­˜åŸå›¾
                                with open(image_path, 'wb') as img_file:
                                    img_file.write(image_data)
                                
                                # ä½¿ç”¨PILå‹ç¼©å›¾ç‰‡
                                from PIL import Image
                                with Image.open(image_path) as img:
                                    # å¦‚æœå›¾ç‰‡å¾ˆå¤§ï¼Œè°ƒæ•´å°ºå¯¸
                                    if img.width > 1920 or img.height > 1080:
                                        img.thumbnail((1920, 1080), Image.Resampling.LANCZOS)
                                    
                                    # ä¿å­˜å‹ç¼©åçš„å›¾ç‰‡
                                    if ext.lower() in ['.jpg', '.jpeg']:
                                        img.save(image_path, 'JPEG', quality=85, optimize=True)
                                    else:
                                        img.save(image_path, 'PNG', optimize=True)
                                
                                compressed_size = image_path.stat().st_size
                                compression_ratio = (1 - compressed_size / original_size) * 100
                                logger.info(f"âœ… æå–å¹¶å‹ç¼©å›¾ç‰‡: {image_name} (å‹ç¼©ç‡: {compression_ratio:.1f}%)")
                            except Exception as compress_error:
                                # å‹ç¼©å¤±è´¥ï¼Œä¿å­˜åŸå›¾
                                with open(image_path, 'wb') as img_file:
                                    img_file.write(image_data)
                                logger.warning(f"âš ï¸ å›¾ç‰‡å‹ç¼©å¤±è´¥ï¼Œä¿å­˜åŸå›¾: {image_name} - {compress_error}")
                        else:
                            # å°å›¾ç‰‡ç›´æ¥ä¿å­˜
                            with open(image_path, 'wb') as img_file:
                                img_file.write(image_data)
                            logger.info(f"âœ… æå–å›¾ç‰‡: {image_name}")
                        
                        image_info.append({
                            'name': image_name,
                            'path': str(image_path),
                            'rel_id': rel_id,
                            'content_type': content_type,
                            'original_size': original_size,
                            'final_size': image_path.stat().st_size
                        })
                        
                        image_counter += 1
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ æå–å›¾ç‰‡å¤±è´¥ (rel_id: {rel_id}): {e}")
            
            if image_info:
                logger.info(f"ğŸ“¸ ä» {file_path.name} æå–äº† {len(image_info)} å¼ å›¾ç‰‡")
            
        except Exception as e:
            logger.error(f"âŒ å›¾ç‰‡æå–è¿‡ç¨‹å¤±è´¥ {file_path}: {e}")
        
        return image_info
    
    def _extract_text_from_damaged_docx(self, file_path: Path) -> str:
        """ä»æŸåçš„DOCXæ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
        import zipfile
        import xml.etree.ElementTree as ET
        
        try:
            text_content = []
            
            with zipfile.ZipFile(file_path, 'r') as docx_zip:
                # åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶
                file_list = docx_zip.namelist()
                logger.info(f"ğŸ“‹ DOCXæ–‡ä»¶å†…å®¹: {file_list}")
                
                # å°è¯•è¯»å–ä¸»æ–‡æ¡£å†…å®¹
                main_doc_files = [
                    'word/document.xml',
                    'word/document2.xml', 
                    'document.xml'
                ]
                
                for doc_file in main_doc_files:
                    if doc_file in file_list:
                        try:
                            with docx_zip.open(doc_file) as xml_file:
                                xml_content = xml_file.read()
                                
                                # è§£æXMLå¹¶æå–æ–‡æœ¬
                                root = ET.fromstring(xml_content)
                                
                                # å®šä¹‰å‘½åç©ºé—´
                                namespaces = {
                                    'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
                                }
                                
                                # æå–æ‰€æœ‰æ–‡æœ¬èŠ‚ç‚¹
                                for text_elem in root.findall('.//w:t', namespaces):
                                    if text_elem.text:
                                        text_content.append(text_elem.text)
                                
                                logger.info(f"âœ… æˆåŠŸä» {doc_file} æå–æ–‡æœ¬")
                                break
                                
                        except Exception as e:
                            logger.warning(f"âš ï¸ æ— æ³•å¤„ç† {doc_file}: {e}")
                            continue
                
                # å¦‚æœä¸»æ–‡æ¡£å¤±è´¥ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„æ–‡æœ¬æ–‡ä»¶
                if not text_content:
                    for file_name in file_list:
                        if file_name.endswith('.xml') and 'word' in file_name:
                            try:
                                with docx_zip.open(file_name) as xml_file:
                                    xml_content = xml_file.read().decode('utf-8', errors='ignore')
                                    # ç®€å•çš„æ­£åˆ™æå–æ–‡æœ¬
                                    import re
                                    text_matches = re.findall(r'<w:t[^>]*>([^<]+)</w:t>', xml_content)
                                    if text_matches:
                                        text_content.extend(text_matches)
                                        logger.info(f"âœ… é€šè¿‡æ­£åˆ™ä» {file_name} æå–æ–‡æœ¬")
                                        break
                            except Exception as e:
                                continue
            
            if text_content:
                # æ¸…ç†å’Œç»„ç»‡æ–‡æœ¬
                cleaned_text = []
                current_line = ""
                
                for text in text_content:
                    text = text.strip()
                    if not text:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°æ®µè½çš„å¼€å§‹
                    if any(char in text for char in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']) and len(current_line) > 20:
                        current_line += text
                        cleaned_text.append(current_line)
                        current_line = ""
                    else:
                        current_line += text + " "
                
                # æ·»åŠ æœ€åä¸€è¡Œ
                if current_line.strip():
                    cleaned_text.append(current_line.strip())
                
                final_text = f"# {file_path.name}\n\n" + "\n\n".join(cleaned_text)
                
                logger.info(f"ğŸ“ æˆåŠŸæ¢å¤æ–‡æœ¬å†…å®¹ï¼Œå…± {len(cleaned_text)} æ®µè½")
                return final_text
            
        except Exception as e:
            logger.error(f"âŒ zipfileæ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
        
        return ""
    
    def _process_docx_with_pandoc(self, file_path: Path) -> List[Document]:
        """ä½¿ç”¨Pandocå¤„ç†DOCXæ–‡ä»¶"""
        try:
            import pypandoc
            
            # ç¡®ä¿Pandocå¯ç”¨
            try:
                pypandoc.get_pandoc_version()
            except OSError:
                logger.info("ğŸ”„ Pandocæœªæ‰¾åˆ°ï¼Œå°è¯•è‡ªåŠ¨ä¸‹è½½...")
                try:
                    pypandoc.download_pandoc()
                    logger.info("âœ… Pandocä¸‹è½½å®Œæˆ")
                except Exception as download_error:
                    logger.error(f"âŒ Pandocä¸‹è½½å¤±è´¥: {download_error}")
                    return []
            
            # ç¡®ä¿å›¾ç‰‡è¾“å‡ºç›®å½•å­˜åœ¨
            self.image_output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨Pandocå°†DOCXè½¬æ¢ä¸ºMarkdown
            logger.info(f"ğŸ“„ ä½¿ç”¨Pandocè½¬æ¢DOCXåˆ°Markdown...")
            
            # è®¾ç½®Pandocå‚æ•°
            extra_args = [
                '--extract-media', str(self.image_output_dir.parent),  # æå–å›¾ç‰‡åˆ°æŒ‡å®šç›®å½•
                '--wrap=none',  # ä¸è‡ªåŠ¨æ¢è¡Œ
                '--markdown-headings=atx'  # ä½¿ç”¨ATXæ ·å¼æ ‡é¢˜
            ]
            
            try:
                # å°è¯•è½¬æ¢ä¸ºmarkdown
                markdown_content = pypandoc.convert_file(
                    str(file_path), 
                    'markdown',
                    extra_args=extra_args
                )
                
                if not markdown_content.strip():
                    logger.warning(f"âš ï¸ Pandocè½¬æ¢ç»“æœä¸ºç©º")
                    return []
                
                logger.info(f"âœ… Pandocè½¬æ¢æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(markdown_content)}")
                
                # å¤„ç†æå–çš„å›¾ç‰‡
                image_info = self._process_extracted_images(file_path)
                
                # æŒ‰ç« èŠ‚åˆ†å‰²å†…å®¹
                sections = self._split_pandoc_markdown(markdown_content, file_path)
                
                documents = []
                for i, section in enumerate(sections):
                    doc_obj = Document(
                        page_content=section['content'],
                        metadata={
                            'file_path': str(file_path),
                            'file_name': file_path.name,
                            'project': self._extract_project_name(file_path),
                            'file_type': 'docx',
                            'source_type': 'pandoc_markdown',
                            'section': section['title'],
                            'chunk_index': i,
                            'total_chunks': len(sections),
                            'processor': 'pandoc'
                        }
                    )
                    documents.append(doc_obj)
                
                # æ·»åŠ å›¾ç‰‡æ–‡æ¡£
                for img_info in image_info:
                    img_doc = self._process_image_file(Path(img_info['path']), file_path)
                    if img_doc:
                        documents.append(img_doc)
                
                # ç¼“å­˜ç»“æœ
                cache_data = [{'page_content': doc.page_content, 'metadata': doc.metadata} for doc in documents]
                self._set_cached_result(str(file_path), "docx_content", cache_data)
                
                logger.info(f"âœ… Pandocå¤„ç†å®Œæˆ: {file_path.name} ({len(documents)} ä¸ªæ–‡æ¡£)")
                return documents
                
            except Exception as convert_error:
                logger.error(f"âŒ Pandocè½¬æ¢å¤±è´¥: {convert_error}")
                
                # å°è¯•ç®€å•æ–‡æœ¬æå–
                try:
                    text_content = pypandoc.convert_file(str(file_path), 'plain')
                    if text_content.strip():
                        simple_doc = Document(
                            page_content=f"# {file_path.name}\n\n{text_content}",
                            metadata={
                                'file_path': str(file_path),
                                'file_name': file_path.name,
                                'project': self._extract_project_name(file_path),
                                'file_type': 'docx',
                                'source_type': 'pandoc_plain',
                                'section': 'æ–‡æ¡£å†…å®¹',
                                'processor': 'pandoc_fallback'
                            }
                        )
                        logger.info(f"âœ… Pandocç®€å•æ–‡æœ¬æå–æˆåŠŸ: {file_path.name}")
                        return [simple_doc]
                except Exception as plain_error:
                    logger.error(f"âŒ Pandocç®€å•æ–‡æœ¬æå–ä¹Ÿå¤±è´¥: {plain_error}")
                
                return []
                
        except ImportError:
            logger.error("âŒ pypandocæœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨Pandocå¤„ç†")
            return []
        except Exception as e:
            logger.error(f"âŒ Pandocå¤„ç†å¼‚å¸¸: {e}")
            return []
    
    def _process_extracted_images(self, file_path: Path) -> List[Dict[str, Any]]:
        """å¤„ç†Pandocæå–çš„å›¾ç‰‡"""
        image_info = []
        
        try:
            # Pandocä¼šå°†å›¾ç‰‡æå–åˆ°mediaç›®å½•
            media_dir = self.image_output_dir.parent / 'media'
            if media_dir.exists():
                image_counter = 1
                for img_file in media_dir.glob('**/*'):
                    if img_file.is_file() and img_file.suffix.lower() in ['.png', '.jpg', '.jpeg', '.gif']:
                        # ç§»åŠ¨å›¾ç‰‡åˆ°æˆ‘ä»¬çš„å›¾ç‰‡ç›®å½•å¹¶é‡å‘½å
                        new_name = f"{file_path.stem}_pandoc_image_{image_counter:03d}{img_file.suffix}"
                        new_path = self.image_output_dir / new_name
                        
                        try:
                            # å¤åˆ¶å¹¶å¯èƒ½å‹ç¼©å›¾ç‰‡
                            import shutil
                            shutil.copy2(img_file, new_path)
                            
                            # å‹ç¼©å¤§å›¾ç‰‡
                            if new_path.stat().st_size > 1024 * 1024:  # å¤§äº1MB
                                self._compress_image(new_path)
                            
                            image_info.append({
                                'name': new_name,
                                'path': str(new_path),
                                'original_path': str(img_file),
                                'size': new_path.stat().st_size
                            })
                            
                            image_counter += 1
                            logger.info(f"ğŸ“¸ å¤„ç†Pandocæå–çš„å›¾ç‰‡: {new_name}")
                            
                        except Exception as e:
                            logger.warning(f"âš ï¸ å¤„ç†å›¾ç‰‡å¤±è´¥ {img_file}: {e}")
                
                # æ¸…ç†mediaç›®å½•
                try:
                    shutil.rmtree(media_dir)
                except:
                    pass
                    
        except Exception as e:
            logger.warning(f"âš ï¸ å¤„ç†æå–å›¾ç‰‡æ—¶å‡ºé”™: {e}")
        
        return image_info
    
    def _compress_image(self, image_path: Path):
        """å‹ç¼©å›¾ç‰‡"""
        try:
            from PIL import Image
            with Image.open(image_path) as img:
                # é™åˆ¶æœ€å¤§å°ºå¯¸
                if img.width > 1920 or img.height > 1080:
                    img.thumbnail((1920, 1080), Image.Resampling.LANCZOS)
                
                # ä¿å­˜å‹ç¼©åçš„å›¾ç‰‡
                if image_path.suffix.lower() in ['.jpg', '.jpeg']:
                    img.save(image_path, 'JPEG', quality=85, optimize=True)
                else:
                    img.save(image_path, 'PNG', optimize=True)
                    
        except Exception as e:
            logger.warning(f"âš ï¸ å›¾ç‰‡å‹ç¼©å¤±è´¥: {e}")
    
    def _split_pandoc_markdown(self, markdown_content: str, file_path: Path) -> List[Dict[str, Any]]:
        """åˆ†å‰²Pandocç”Ÿæˆçš„Markdownå†…å®¹"""
        sections = []
        lines = markdown_content.split('\n')
        
        current_title = "æ–‡æ¡£å¼€å§‹"
        current_content = []
        
        for line in lines:
            line = line.strip()
            
            # æ£€æµ‹æ ‡é¢˜è¡Œ
            if line.startswith('#'):
                # ä¿å­˜ä¹‹å‰çš„ç« èŠ‚
                if current_content:
                    sections.append({
                        'title': current_title,
                        'content': '\n'.join(current_content).strip()
                    })
                
                # å¼€å§‹æ–°ç« èŠ‚
                current_title = line.lstrip('#').strip() or "æ— æ ‡é¢˜ç« èŠ‚"
                current_content = []
            else:
                if line:  # éç©ºè¡Œ
                    current_content.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªç« èŠ‚
        if current_content:
            sections.append({
                'title': current_title,
                'content': '\n'.join(current_content).strip()
            })
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç« èŠ‚ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤ç« èŠ‚
        if not sections:
            sections.append({
                'title': file_path.stem,
                'content': markdown_content
            })
        
        return sections
    
    def _is_heading(self, paragraph, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜"""
        # æ£€æŸ¥æ®µè½æ ·å¼
        if paragraph.style.name.startswith('Heading'):
            return True
        
        # æ£€æŸ¥æ–‡æœ¬æ ¼å¼ï¼ˆæ•°å­—å¼€å¤´çš„æ ‡é¢˜ï¼‰
        if re.match(r'^\d+\.?\d*\s+', text):
            return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºçŸ­æ–‡æœ¬ä¸”å¯èƒ½æ˜¯æ ‡é¢˜
        if len(text) < 50 and any(keyword in text for keyword in ['ç³»ç»Ÿ', 'æ¨¡å—', 'åŠŸèƒ½', 'æ¥å£', 'è®¾è®¡', 'æ¶æ„']):
            return True
        
        return False
    
    def _get_heading_level(self, paragraph, text: str) -> int:
        """è·å–æ ‡é¢˜çº§åˆ«"""
        if paragraph.style.name.startswith('Heading'):
            try:
                return int(paragraph.style.name.replace('Heading ', ''))
            except:
                return 1
        
        # æ ¹æ®æ•°å­—æ ¼å¼åˆ¤æ–­çº§åˆ«
        match = re.match(r'^(\d+)\.?(\d*)\s+', text)
        if match:
            if match.group(2):  # æœ‰äºŒçº§æ•°å­—ï¼Œå¦‚ 3.1
                return 2
            else:  # åªæœ‰ä¸€çº§æ•°å­—ï¼Œå¦‚ 3
                return 1
        
        return 1
    
    def _split_markdown_by_sections(self, markdown_content: str, file_path: Path) -> List[Dict[str, Any]]:
        """æŒ‰æ ‡é¢˜åˆ†å‰²Markdownå†…å®¹"""
        sections = []
        lines = markdown_content.split('\n')
        
        current_section = ""
        current_content = []
        current_images = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æµ‹æ ‡é¢˜è¡Œ
            if line.startswith('#'):
                # ä¿å­˜ä¹‹å‰çš„æ®µè½
                if current_section and current_content:
                    sections.append({
                        'section': current_section,
                        'content': '\n'.join(current_content),
                        'image_refs': current_images.copy()
                    })
                
                # å¼€å§‹æ–°æ®µè½
                current_section = line.lstrip('#').strip()
                current_content = []
                current_images = []
            
            elif line.startswith('!['):
                # å›¾ç‰‡å¼•ç”¨
                image_match = re.match(r'!\[(.*?)\]\((.*?)\)', line)
                if image_match:
                    current_images.append(image_match.group(2))
            
            else:
                # æ™®é€šå†…å®¹
                current_content.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
        if current_section and current_content:
            sections.append({
                'section': current_section,
                'content': '\n'.join(current_content),
                'image_refs': current_images
            })
        
        return sections
    
    def _process_docx_file(self, file_path: Path) -> List[Document]:
        """å¤„ç† DOCX æ–‡ä»¶ - è¿”å›LangChain Documentå¯¹è±¡"""
        documents = []
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cached_result = self._get_cached_result(str(file_path), "docx_content")
            if cached_result:
                logger.info(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„ DOCX å†…å®¹: {file_path.name}")
                return [Document(**doc) for doc in cached_result]
            
            # å°è¯•ä½¿ç”¨å¤šç§æ–¹æ³•å¤„ç†DOCXæ–‡ä»¶
            doc = None
            markdown_content = ""
            image_info = []
            
            # æ–¹æ³•1: å°è¯•ä½¿ç”¨Pandocå¤„ç†ï¼ˆæ¨èï¼Œå®¹é”™æ€§æœ€å¥½ï¼‰
            try:
                logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨Pandocå¤„ç†DOCXæ–‡ä»¶: {file_path.name}")
                pandoc_result = self._process_docx_with_pandoc(file_path)
                if pandoc_result:
                    return pandoc_result
            except Exception as pandoc_error:
                logger.warning(f"âš ï¸ Pandocå¤„ç†å¤±è´¥: {pandoc_error}")
            
                # æ–¹æ³•2: å°è¯•ä½¿ç”¨python-docx
                try:
                    doc = docx.Document(file_path)
                    logger.info(f"âœ… python-docxæˆåŠŸæ‰“å¼€æ–‡ä»¶: {file_path.name}")
                except Exception as docx_error:
                    logger.warning(f"âš ï¸ python-docxæ— æ³•æ‰“å¼€æ–‡ä»¶ {file_path}: {docx_error}")
                    
                    # æ–¹æ³•3: å°è¯•ä½¿ç”¨zipfileç›´æ¥æå–æ–‡æœ¬å†…å®¹
                    try:
                        logger.info(f"ğŸ”„ å°è¯•ä½¿ç”¨zipfileæ–¹æ³•å¤„ç†æŸåçš„DOCXæ–‡ä»¶...")
                        text_content = self._extract_text_from_damaged_docx(file_path)
                        if text_content:
                            # åˆ›å»ºåŸºäºæå–æ–‡æœ¬çš„æ–‡æ¡£
                            recovered_doc = Document(
                                page_content=text_content,
                                metadata={
                                    'file_path': str(file_path),
                                    'file_name': file_path.name,
                                    'project': self._extract_project_name(file_path),
                                    'file_type': 'docx',
                                    'source_type': 'recovered_text',
                                    'section': 'æ¢å¤çš„æ–‡æ¡£å†…å®¹',
                                    'status': 'recovered'
                                }
                            )
                            
                            # ç¼“å­˜ç»“æœ
                            cache_data = [{'page_content': recovered_doc.page_content, 'metadata': recovered_doc.metadata}]
                            self._set_cached_result(str(file_path), "docx_content", cache_data)
                            
                            logger.info(f"âœ… ä»æŸåçš„DOCXæ–‡ä»¶æ¢å¤æ–‡æœ¬å†…å®¹: {file_path.name}")
                            return [recovered_doc]
                        
                    except Exception as recovery_error:
                        logger.error(f"âŒ æ–‡æœ¬æ¢å¤ä¹Ÿå¤±è´¥: {recovery_error}")
                    
                # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œåˆ›å»ºé”™è¯¯ä¿¡æ¯æ–‡æ¡£
                logger.error(f"âŒ å®Œå…¨æ— æ³•å¤„ç†DOCXæ–‡ä»¶ {file_path}")
                error_doc = Document(
                    page_content=f"æ–‡æ¡£: {file_path.name}\nçŠ¶æ€: æ–‡ä»¶æŸåï¼Œæ— æ³•è¯»å–\né”™è¯¯: {str(docx_error)}\nè·¯å¾„: {file_path}\n\nå»ºè®®: è¯·ä½¿ç”¨Wordé‡æ–°ä¿å­˜æ­¤æ–‡ä»¶",
                    metadata={
                        'file_path': str(file_path),
                        'file_name': file_path.name,
                        'project': self._extract_project_name(file_path),
                        'file_type': 'docx',
                        'source_type': 'error',
                        'section': 'æ–‡æ¡£ä¿¡æ¯',
                        'status': 'corrupted'
                    }
                )
                return [error_doc]
            
            # å°†DOCXè½¬æ¢ä¸ºMarkdown
            markdown_content, image_info = self._docx_to_markdown(doc, file_path)
            
            # æŒ‰æ ‡é¢˜åˆ†å‰²å†…å®¹
            sections = self._split_markdown_by_sections(markdown_content, file_path)
            
            for section in sections:
                # åˆ›å»ºLangChain Documentå¯¹è±¡
                document = Document(
                    page_content=section['content'],
                    metadata={
                        'file_path': str(file_path),
                        'file_name': file_path.name,
                        'project': self._extract_project_name(file_path),
                        'file_type': 'docx',
                        'source_type': 'text',
                        'section': section['section'],
                        'image_refs': section.get('image_refs', [])
                    }
                )
                documents.append(document)
            
            # å¤„ç†å›¾ç‰‡
            for img_info in image_info:
                img_doc = self._process_image_file(Path(img_info['path']), file_path)
                if img_doc:
                    documents.append(img_doc)
            
            # ç¼“å­˜ç»“æœ
            cache_data = [{'page_content': doc.page_content, 'metadata': doc.metadata} for doc in documents]
            self._set_cached_result(str(file_path), "docx_content", cache_data)
            
            logger.info(f"âœ… DOCX æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path.name} ({len(documents)} ä¸ªæ–‡æ¡£)")
            
        except Exception as e:
            logger.error(f"âŒ DOCX æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
        
        return documents
    
    def _process_image_file(self, image_path: Path, source_file: Path) -> Optional[Document]:
        """å¤„ç†å›¾ç‰‡æ–‡ä»¶ - OCRå’Œæè¿°ç”Ÿæˆ"""
        try:
            if not image_path.exists():
                return None
            
            # æ£€æŸ¥ç¼“å­˜
            cached_result = self._get_cached_result(str(image_path), "image_analysis")
            if cached_result:
                return Document(**cached_result)
            
            # å»¶è¿ŸåŠ è½½å›¾ç‰‡æè¿°æ¨¡å‹
            self._load_image_caption_model()
            
            # åŠ è½½å›¾ç‰‡
            image = Image.open(image_path)
            
            # OCRæ–‡æœ¬æå–
            ocr_text = ""
            try:
                ocr_text = pytesseract.image_to_string(image, lang='chi_sim+eng')
            except Exception as e:
                logger.warning(f"OCRå¤±è´¥ {image_path}: {e}")
            
            # å›¾ç‰‡æè¿°ç”Ÿæˆ
            caption = ""
            if self.blip_processor and self.blip_model:
                try:
                    inputs = self.blip_processor(image, return_tensors="pt")
                    out = self.blip_model.generate(**inputs, max_length=50)
                    caption = self.blip_processor.decode(out[0], skip_special_tokens=True)
                except Exception as e:
                    logger.warning(f"å›¾ç‰‡æè¿°ç”Ÿæˆå¤±è´¥ {image_path}: {e}")
            
            # åˆå¹¶æ–‡æœ¬å†…å®¹
            combined_text = f"å›¾ç‰‡æè¿°: {caption}\nOCRæ–‡æœ¬: {ocr_text}".strip()
            
            # åˆ›å»ºDocumentå¯¹è±¡
            document = Document(
                page_content=combined_text,
                metadata={
                    'file_path': str(source_file),
                    'file_name': source_file.name,
                    'project': self._extract_project_name(source_file),
                    'file_type': 'image',
                    'source_type': 'image_description',
                    'image_path': str(image_path),
                    'ocr_text': ocr_text,
                    'caption': caption
                }
            )
            
            # ç¼“å­˜ç»“æœ
            cache_data = {'page_content': document.page_content, 'metadata': document.metadata}
            self._set_cached_result(str(image_path), "image_analysis", cache_data)
            
            return document
            
        except Exception as e:
            logger.error(f"å›¾ç‰‡å¤„ç†å¤±è´¥ {image_path}: {e}")
            return None
    
    def _process_xlsx_file(self, file_path: Path) -> List[Document]:
        """å¤„ç† XLSX æ–‡ä»¶"""
        documents = []
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cached_result = self._get_cached_result(str(file_path), "xlsx_content")
            if cached_result:
                logger.info(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„ XLSX å†…å®¹: {file_path.name}")
                return [Document(**doc) for doc in cached_result]
            
            workbook = load_workbook(file_path, read_only=True)
            
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                
                # æå–è¡¨æ ¼å†…å®¹
                sheet_content = []
                headers = []
                
                # è·å–è¡¨å¤´
                for cell in sheet[1]:
                    if cell.value:
                        headers.append(str(cell.value))
                
                if headers:
                    sheet_content.append("è¡¨å¤´: " + " | ".join(headers))
                
                # è·å–æ•°æ®è¡Œ
                for row in sheet.iter_rows(min_row=2, values_only=True):
                    row_data = []
                    for cell_value in row:
                        if cell_value is not None:
                            row_data.append(str(cell_value))
                    
                    if row_data:
                        sheet_content.append(" | ".join(row_data))
                
                if sheet_content:
                    full_content = f"å·¥ä½œè¡¨: {sheet_name}\n" + "\n".join(sheet_content)
                    
                    # åˆ†å‰²å†…å®¹
                    text_chunks = self.text_splitter.split_text(full_content)
                    
                    for i, chunk in enumerate(text_chunks):
                        document = Document(
                            page_content=chunk,
                            metadata={
                                'file_path': str(file_path),
                                'file_name': file_path.name,
                                'project': self._extract_project_name(file_path),
                                'file_type': 'xlsx',
                                'source_type': 'excel',
                                'sheet_name': sheet_name,
                                'chunk_index': i
                            }
                        )
                        documents.append(document)
            
            workbook.close()
            
            # ç¼“å­˜ç»“æœ
            cache_data = [{'page_content': doc.page_content, 'metadata': doc.metadata} for doc in documents]
            self._set_cached_result(str(file_path), "xlsx_content", cache_data)
            
            logger.info(f"âœ… XLSX æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path.name} ({len(documents)} ä¸ªæ–‡æ¡£)")
            
        except Exception as e:
            logger.error(f"âŒ XLSX æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
        
        return documents
    
    def _process_java_file(self, file_path: Path) -> List[Document]:
        """å¤„ç† Java æ–‡ä»¶"""
        documents = []
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cached_result = self._get_cached_result(str(file_path), "java_content")
            if cached_result:
                logger.info(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„ Java å†…å®¹: {file_path.name}")
                return [Document(**doc) for doc in cached_result]
            
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # æŒ‰ç±»å’Œæ–¹æ³•åˆ†å‰²Javaä»£ç 
            code_blocks = self._split_java_code(content)
            
            for i, block in enumerate(code_blocks):
                if len(block.strip()) > 50:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                    document = Document(
                        page_content=block.strip(),
                        metadata={
                            'file_path': str(file_path),
                            'file_name': file_path.name,
                            'project': self._extract_project_name(file_path),
                            'file_type': 'java',
                            'source_type': 'code',
                            'chunk_index': i
                        }
                    )
                    documents.append(document)
            
            # ç¼“å­˜ç»“æœ
            cache_data = [{'page_content': doc.page_content, 'metadata': doc.metadata} for doc in documents]
            self._set_cached_result(str(file_path), "java_content", cache_data)
            
            logger.info(f"âœ… Java æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path.name} ({len(documents)} ä¸ªæ–‡æ¡£)")
            
        except Exception as e:
            logger.error(f"âŒ Java æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
        
        return documents
    
    def _split_java_code(self, content: str) -> List[str]:
        """åˆ†å‰²Javaä»£ç """
        blocks = []
        
        # æŒ‰ç±»åˆ†å‰²
        class_pattern = r'(public\s+class\s+\w+.*?(?=public\s+class|\Z))'
        classes = re.findall(class_pattern, content, re.DOTALL)
        
        if classes:
            blocks.extend(classes)
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç±»ï¼ŒæŒ‰æ–¹æ³•åˆ†å‰²
            method_pattern = r'(public\s+\w+.*?\{.*?\n\s*\})'
            methods = re.findall(method_pattern, content, re.DOTALL)
            
            if methods:
                blocks.extend(methods)
            else:
                # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼ŒæŒ‰æ–‡æœ¬åˆ†å‰²
                text_chunks = self.text_splitter.split_text(content)
                blocks.extend(text_chunks)
        
        return blocks
    
    def _process_xml_file(self, file_path: Path) -> List[Document]:
        """å¤„ç† XML æ–‡ä»¶"""
        documents = []
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cached_result = self._get_cached_result(str(file_path), "xml_content")
            if cached_result:
                logger.info(f"ğŸ“‹ ä½¿ç”¨ç¼“å­˜çš„ XML å†…å®¹: {file_path.name}")
                return [Document(**doc) for doc in cached_result]
            
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # æŒ‰æ–‡æœ¬åˆ†å‰² XML å†…å®¹
            text_chunks = self.text_splitter.split_text(content)
            
            for i, chunk in enumerate(text_chunks):
                document = Document(
                    page_content=chunk,
                    metadata={
                        'file_path': str(file_path),
                        'file_name': file_path.name,
                        'project': self._extract_project_name(file_path),
                        'file_type': 'xml',
                        'source_type': 'config',
                        'chunk_index': i
                    }
                )
                documents.append(document)
            
            # ç¼“å­˜ç»“æœ
            cache_data = [{'page_content': doc.page_content, 'metadata': doc.metadata} for doc in documents]
            self._set_cached_result(str(file_path), "xml_content", cache_data)
            
            logger.info(f"âœ… XML æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path.name} ({len(documents)} ä¸ªæ–‡æ¡£)")
            
        except Exception as e:
            logger.error(f"âŒ XML æ–‡ä»¶å¤„ç†å¤±è´¥ {file_path}: {e}")
        
        return documents
    
    def _create_weaviate_schema(self):
        """åˆ›å»º Weaviate æ¨¡å¼ - æŒ‰è¦æ±‚å®šä¹‰Documentç±»"""
        try:
            collection_name = "Document"
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
            if self.weaviate_client.collections.exists(collection_name):
                logger.info(f"ğŸ“‹ é›†åˆ {collection_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return collection_name
            
            # åˆ›å»ºé›†åˆ - æŒ‰è¦æ±‚è®¾ç½®vectorizer: none
            self.weaviate_client.collections.create(
                name=collection_name,
                vectorizer_config=wvc.config.Configure.Vectorizer.none(),
                properties=[
                    wvc.config.Property(name="content", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="file_path", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="file_name", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="project", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="file_type", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="source_type", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="image_path", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="section", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="sheet_name", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="chunk_index", data_type=wvc.config.DataType.INT),
                    wvc.config.Property(name="created_at", data_type=wvc.config.DataType.DATE),
                    wvc.config.Property(name="ocr_text", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="caption", data_type=wvc.config.DataType.TEXT)
                ]
            )
            
            logger.info(f"âœ… Weaviate é›†åˆåˆ›å»ºæˆåŠŸ: {collection_name}")
            return collection_name
            
        except Exception as e:
            logger.error(f"âŒ Weaviate é›†åˆåˆ›å»ºå¤±è´¥: {e}")
            raise
    
    def _initialize_langchain_vectorstore(self, collection_name: str):
        """åˆå§‹åŒ–LangChainå‘é‡å­˜å‚¨ - ä½¿ç”¨æ‰‹åŠ¨RAGå®ç°"""
        try:
            # ç”±äºç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œä½¿ç”¨æ‰‹åŠ¨RAGå®ç°
            logger.info("âœ… ä½¿ç”¨æ‰‹åŠ¨RAGå®ç° (è·³è¿‡LangChainå‘é‡å­˜å‚¨)")
            self.langchain_vectorstore = None
            
        except Exception as e:
            logger.error(f"âŒ LangChain å‘é‡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.langchain_vectorstore = None
    
    def _insert_documents_batch(self, documents: List[Document], collection_name: str):
        """æ‰‹åŠ¨æ‰¹é‡æ’å…¥æ–‡æ¡£åˆ°Weaviate"""
        try:
            collection = self.weaviate_client.collections.get(collection_name)
            
            # å‡†å¤‡æ‰¹é‡æ’å…¥çš„æ•°æ®
            objects = []
            for doc in documents:
                # ç”Ÿæˆå‘é‡
                vector = self.embeddings_model.encode(doc.page_content).tolist()
                
                # å‡†å¤‡å¯¹è±¡æ•°æ®
                obj_data = {
                    "content": doc.page_content,
                    "vector": vector,
                    **doc.metadata  # å±•å¼€æ‰€æœ‰å…ƒæ•°æ®
                }
                objects.append(obj_data)
            
            # æ‰¹é‡æ’å…¥
            with collection.batch.dynamic() as batch:
                for obj in objects:
                    batch.add_object(
                        properties={k: v for k, v in obj.items() if k != "vector"},
                        vector=obj["vector"]
                    )
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ’å…¥æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def scan_knowledge_base(self) -> List[Path]:
        """æ‰«æçŸ¥è¯†åº“ç›®å½•ï¼Œè·å–æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶"""
        files = []
        
        try:
            for file_path in self.knowledge_base_path.rglob("*"):
                if file_path.is_file() and file_path.suffix.lower() in self.supported_extensions:
                    files.append(file_path)
            
            logger.info(f"ğŸ“ æ‰«æå®Œæˆï¼Œæ‰¾åˆ° {len(files)} ä¸ªæ”¯æŒçš„æ–‡ä»¶")
            
            # æŒ‰æ–‡ä»¶ç±»å‹ç»Ÿè®¡
            stats = {}
            for file_path in files:
                ext = file_path.suffix.lower()
                stats[ext] = stats.get(ext, 0) + 1
            
            for ext, count in stats.items():
                logger.info(f"   {ext}: {count} ä¸ªæ–‡ä»¶")
            
        except Exception as e:
            logger.error(f"âŒ æ‰«æçŸ¥è¯†åº“å¤±è´¥: {e}")
        
        return files
    
    def process_files(self, files: List[Path]) -> List[Document]:
        """å¤„ç†æ‰€æœ‰æ–‡ä»¶"""
        all_documents = []
        
        for i, file_path in enumerate(files, 1):
            logger.info(f"ğŸ”„ å¤„ç†æ–‡ä»¶ {i}/{len(files)}: {file_path.name}")
            
            try:
                documents = []
                
                if file_path.suffix.lower() == '.docx':
                    documents = self._process_docx_file(file_path)
                elif file_path.suffix.lower() == '.xlsx':
                    documents = self._process_xlsx_file(file_path)
                elif file_path.suffix.lower() == '.java':
                    documents = self._process_java_file(file_path)
                elif file_path.suffix.lower() == '.xml':
                    documents = self._process_xml_file(file_path)
                
                all_documents.extend(documents)
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        logger.info(f"ğŸ“Š æ–‡ä»¶å¤„ç†å®Œæˆï¼Œæ€»è®¡ç”Ÿæˆ {len(all_documents)} ä¸ªæ–‡æ¡£")
        return all_documents
    
    def initialize_knowledge_base(self):
        """åˆå§‹åŒ–çŸ¥è¯†åº“"""
        try:
            logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ–çŸ¥è¯†åº“...")
            
            # 1. æ¸…ç©ºWeaviateæ•°æ®åº“
            self._clear_weaviate_database()
            
            # 2. æ£€æŸ¥çŸ¥è¯†åº“ç›®å½•
            if not self.knowledge_base_path.exists():
                raise FileNotFoundError(f"çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {self.knowledge_base_path}")
            
            # 3. æ‰«ææ–‡ä»¶
            files = self.scan_knowledge_base()
            if not files:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡ä»¶")
                return
            
            # 4. åˆ›å»º Weaviate æ¨¡å¼
            collection_name = self._create_weaviate_schema()
            
            # 5. åˆå§‹åŒ–LangChainå‘é‡å­˜å‚¨
            self._initialize_langchain_vectorstore(collection_name)
            
            # 6. å¤„ç†æ–‡ä»¶
            documents = self.process_files(files)
            if not documents:
                logger.warning("âš ï¸ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æ¡£")
                return
            
            # 7. æ‰‹åŠ¨æ‰¹é‡æ’å…¥åˆ°Weaviate - æŒ‰è¦æ±‚batch_size=100
            logger.info("ğŸ“¤ å¼€å§‹æ‰¹é‡æ’å…¥æ–‡æ¡£åˆ°Weaviate...")
            batch_size = 100
            
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i + batch_size]
                try:
                    # æ‰‹åŠ¨æ’å…¥æ–‡æ¡£
                    self._insert_documents_batch(batch, collection_name)
                    logger.info(f"âœ… æ‰¹æ¬¡ {i//batch_size + 1}/{(len(documents) + batch_size - 1)//batch_size} æ’å…¥æˆåŠŸ ({len(batch)} ä¸ªæ–‡æ¡£)")
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡ {i//batch_size + 1} æ’å…¥å¤±è´¥: {e}")
            
            logger.info("ğŸ‰ çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def query_knowledge_base(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """æ‰‹åŠ¨æŸ¥è¯¢çŸ¥è¯†åº“"""
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.embeddings_model.encode(query).tolist()
            
            # ä½¿ç”¨Weaviateè¿›è¡Œå‘é‡æœç´¢
            collection = self.weaviate_client.collections.get("Document")
            
            results = collection.query.near_vector(
                near_vector=query_vector,
                limit=k,
                return_metadata=["distance"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for obj in results.objects:
                formatted_results.append({
                    'content': obj.properties.get('content', ''),
                    'metadata': {k: v for k, v in obj.properties.items() if k != 'content'},
                    'similarity_score': 1 - obj.metadata.distance,  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                    'distance': obj.metadata.distance
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢çŸ¥è¯†åº“å¤±è´¥: {e}")
            return []
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.weaviate_client:
            self.weaviate_client.close()
        if self.redis_manager:
            self.redis_manager.close()

def main():
    """ä¸»å‡½æ•° - æ£€æŸ¥ç‰¹å®šæ–‡æ¡£"""
    initializer = None
    
    try:
        # åˆ›å»ºåˆå§‹åŒ–å™¨
        initializer = KnowledgeBaseInitializer()
        
        # æ£€æŸ¥ç‰¹å®šæ–‡æ¡£
        file_name = "LS-1(YS-72)_éœ€æ±‚æ–‡æ¡£-é“¾æ•°ä¸€æœŸV1.8.docx"
        print(f"ğŸ” æ£€æŸ¥æ–‡æ¡£: {file_name}")
       

        collection = initializer.weaviate_client.collections.get("Document")
        print(f"ğŸ” æŸ¥è¯¢å‡ºå½“å‰æ–‡æ¡£: {collection}")

        # ä½¿ç”¨æ­£ç¡®çš„æŸ¥è¯¢è¯­æ³•ï¼Œä½¿ç”¨filterså‚æ•°è€Œä¸æ˜¯where
        existing_docs = collection.query.fetch_objects(
            filters=wvc.query.Filter.by_property("file_name").equal(file_name)
        )
        print(f"ğŸ” æŸ¥è¯¢å‡ºå½“å‰æ–‡ä»¶åæ–‡æ¡£: {existing_docs}")
        
        # å…ˆè·å–æ‰€æœ‰æ–‡æ¡£ï¼Œç„¶åè¿‡æ»¤
        print("ğŸ“‹ æ­£åœ¨æŸ¥è¯¢æ•°æ®åº“...")
        all_results = collection.query.fetch_objects(limit=1000)
        
        if not all_results.objects:
            print("âŒ æ•°æ®åº“ä¸ºç©ºï¼Œæ²¡æœ‰ä»»ä½•å‘é‡åŒ–æ–‡æ¡£")
            return
        
        # æ‰‹åŠ¨è¿‡æ»¤ç‰¹å®šæ–‡ä»¶
        target_docs = []
        all_files = set()
        
        for obj in all_results.objects:
            props = obj.properties
            current_file = props.get('file_name', 'unknown')
            all_files.add(current_file)
            
            if file_name in current_file or current_file == file_name:
                target_docs.append(obj)
        
        if not target_docs:
            print(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶ '{file_name}' çš„å‘é‡åŒ–è®°å½•")
            print(f"\nğŸ“‹ æ•°æ®åº“ä¸­çš„æ‰€æœ‰æ–‡ä»¶ (å…±{len(all_files)}ä¸ª):")
            for fname in sorted(all_files):
                print(f"  - {fname}")
        else:
            print(f"âœ… æ‰¾åˆ°æ–‡ä»¶ '{file_name}' çš„ {len(target_docs)} ä¸ªå‘é‡åŒ–æ–‡æ¡£å—")
            
            # ç»Ÿè®¡ä¿¡æ¯
            processors = set()
            source_types = set()
            sections = set()
            
            # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            for i, obj in enumerate(target_docs[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                props = obj.properties
                print(f"\n{i+1}. ç« èŠ‚: {props.get('section', 'N/A')}")
                print(f"   å¤„ç†å™¨: {props.get('processor', 'N/A')}")
                print(f"   æ¥æºç±»å‹: {props.get('source_type', 'N/A')}")
                print(f"   æ–‡ä»¶ç±»å‹: {props.get('file_type', 'N/A')}")
                print(f"   é¡¹ç›®: {props.get('project', 'N/A')}")
                print(f"   å†…å®¹é¢„è§ˆ: {props.get('content', '')[:200]}...")
                
                processors.add(props.get('processor', 'unknown'))
                source_types.add(props.get('source_type', 'unknown'))
                if props.get('section'):
                    sections.add(props.get('section'))
            
            if len(target_docs) > 5:
                print(f"\n... è¿˜æœ‰ {len(target_docs) - 5} ä¸ªæ–‡æ¡£å—")
            
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  - æ€»æ–‡æ¡£å—æ•°: {len(target_docs)}")
            print(f"  - ä½¿ç”¨çš„å¤„ç†å™¨: {list(processors)}")
            print(f"  - æ¥æºç±»å‹: {list(source_types)}")
            print(f"  - ç« èŠ‚æ•°é‡: {len(sections)}")
            
            if sections:
                print(f"  - ç« èŠ‚åˆ—è¡¨: {list(sections)[:10]}{'...' if len(sections) > 10 else ''}")
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if initializer:
            initializer.close()

if __name__ == "__main__":
    main() 