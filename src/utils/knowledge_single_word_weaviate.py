#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥è¯†åº“åˆå§‹åŒ–è„šæœ¬ - Weaviate ç‰ˆæœ¬
å°†D:\knowledge_base\é“¾æ•°_LS\éœ€æ±‚æ–‡æ¡£\LS-1(YS-72)_éœ€æ±‚æ–‡æ¡£-é“¾æ•°ä¸€æœŸV1.8.docxå†…å®¹è½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨åˆ° Weaviate ä¸­
ä½¿ç”¨ LangChain æ¡†æ¶è¿›è¡Œ RAG
"""


import os
import re
import uuid
import json
import logging
import hashlib
import shutil
import base64
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from io import BytesIO


# æ–‡ä»¶å¤„ç†ç›¸å…³
try:
    import docx2txt
    DOCX2TXT_AVAILABLE = True
except ImportError:
    DOCX2TXT_AVAILABLE = False

try:
    import mammoth
    MAMMOTH_AVAILABLE = True
except ImportError:
    MAMMOTH_AVAILABLE = False

# å‘é‡åŒ–å’ŒåµŒå…¥
from sentence_transformers import SentenceTransformer

# Weaviate å’Œ LangChain
import weaviate
import weaviate.classes as wvc
from weaviate.auth import AuthApiKey
from langchain_community.vectorstores import Weaviate as LangChainWeaviate
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

# é¡¹ç›®é…ç½®å’Œ Redis
try:
    from ..resource.config import get_config
    from .redis_util import get_redis_manager
    from .weaviate_helper import get_weaviate_client
except ImportError:
    import sys
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    sys.path.insert(0, project_root)
    from src.resource.config import get_config
    from src.utils.redis_util import get_redis_manager
    from src.utils.weaviate_helper import get_weaviate_client

# å·²åœ¨ä¸Šé¢å®šä¹‰äº†ç›¸å…³çš„å¯¼å…¥æ£€æŸ¥


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

class KnowledgeBaseInitializer:
    """çŸ¥è¯†åº“åˆå§‹åŒ–å™¨"""
    
    def __init__(self, knowledge_base_path: str = "D:\\knowledge_base"):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“å¤„ç†å™¨
        
        Args:
            knowledge_base_path: çŸ¥è¯†åº“æ ¹ç›®å½•è·¯å¾„
        """
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ä»¥ä½¿ç”¨_call_llmæ–¹æ³•
        self.llm_client = None
        self._initialize_llm_client()
        
        self.knowledge_base_path = Path(knowledge_base_path)
        self.weaviate_client = None
        self.redis_manager = None
        self.embeddings_model = None
        self.langchain_embeddings = None
        self.langchain_vectorstore = None
        self.text_splitter = None

        
        # æ”¯æŒçš„æ–‡ä»¶ç±»å‹
        self.supported_extensions = {'.docx'}
        
        # å›¾ç‰‡è¾“å‡ºç›®å½• - æŒ‰è¦æ±‚è®¾ç½®ä¸ºæŒ‡å®šè·¯å¾„
        self.image_output_dir = self.knowledge_base_path / "é“¾æ•°_LS" / "éœ€æ±‚æ–‡æ¡£" / "éœ€æ±‚æ–‡æ¡£å›¾ç‰‡"
        
        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()
    
    def _initialize_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            from ..utils.volcengine_client import VolcengineClient
            config = get_config()
            volcengine_config = config.get_volcengine_config()
            self.llm_client = VolcengineClient(volcengine_config)
            logger.info("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.llm_client = None
    
    def _call_llm(self, prompt: str, system_prompt: str = None, max_tokens: int = 4000) -> Optional[str]:
        """
        è°ƒç”¨LLMè¿›è¡Œåˆ†æ
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            æ¨¡å‹å“åº”æ–‡æœ¬
        """
        if not self.llm_client:
            logger.warning("LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            return None
        
        try:
            response = self.llm_client.chat(
                messages=[
                    {"role": "system", "content": system_prompt or "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens
            )
            return response
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _initialize_components(self):
        """åˆå§‹åŒ–å„ç§ç»„ä»¶"""
        try:
            # åˆå§‹åŒ– Weaviate å®¢æˆ·ç«¯
            self.weaviate_client = get_weaviate_client()
            logger.info("âœ… Weaviate å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ– Redis ç®¡ç†å™¨
            self.redis_manager = get_redis_manager()
            logger.info("âœ… Redis ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ - ä½¿ç”¨bge-large-zhï¼ˆ1024ç»´ï¼Œä¸­æ–‡ä¼˜åŒ–ï¼‰
            self.embeddings_model = SentenceTransformer('BAAI/bge-large-zh')
            logger.info("âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ (bge-large-zh, 1024ç»´ï¼Œä¸­æ–‡ä¼˜åŒ–)")
            
            # åˆå§‹åŒ– LangChain åµŒå…¥æ¨¡å‹
            self.langchain_embeddings = SentenceTransformerEmbeddings(
                model_name='BAAI/bge-large-zh'
            )
            logger.info("âœ… LangChain åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50,
                length_function=len,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
            )
            logger.info("âœ… æ–‡æœ¬åˆ†å‰²å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # åˆ›å»ºå›¾ç‰‡è¾“å‡ºç›®å½•
            self.image_output_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"âœ… å›¾ç‰‡è¾“å‡ºç›®å½•åˆ›å»ºæˆåŠŸ: {self.image_output_dir}")
            
            logger.info("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise


    def _create_weaviate_schema(self):
        """åˆ›å»º Weaviate æ¨¡å¼ - æŒ‰è¦æ±‚å®šä¹‰Documentç±»"""
        try:
            collection_name = "Document"
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å·²å­˜åœ¨
            if self.weaviate_client.collections.exists(collection_name):
                logger.info(f"ğŸ“‹ é›†åˆ {collection_name} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
                return collection_name
            
            # åˆ›å»ºé›†åˆ - æŒ‰è¦æ±‚è®¾ç½®vectorizer: none
            self.weaviate_client.collections.create(
                name=collection_name,
                vectorizer_config=wvc.config.Configure.Vectorizer.none(),
                properties=[
                    wvc.config.Property(name="content", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="file_path", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="file_name", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="project", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="file_type", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="source_type", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="image_path", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="section", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="sheet_name", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="chunk_index", data_type=wvc.config.DataType.INT),
                    wvc.config.Property(name="created_at", data_type=wvc.config.DataType.DATE),
                    wvc.config.Property(name="ocr_text", data_type=wvc.config.DataType.TEXT),
                    wvc.config.Property(name="caption", data_type=wvc.config.DataType.TEXT)
                ]
            )
            
            logger.info(f"âœ… Weaviate é›†åˆåˆ›å»ºæˆåŠŸ: {collection_name}")
            return collection_name
            
        except Exception as e:
            logger.error(f"âŒ Weaviate é›†åˆåˆ›å»ºå¤±è´¥: {e}")
            raise
     
    def _load_image_caption_model(self):
        """å»¶è¿ŸåŠ è½½å›¾ç‰‡æè¿°æ¨¡å‹"""
        if self.blip_processor is None:
            try:
                self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
                self.blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
                logger.info("âœ… å›¾ç‰‡æè¿°æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ å›¾ç‰‡æè¿°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.blip_processor = None
                self.blip_model = None
    
    def _get_cache_key(self, file_path: str, cache_type: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”® - æŒ‰è¦æ±‚ä½¿ç”¨æŒ‡å®šæ ¼å¼"""
        return f"file:{file_path}:{cache_type}"
    
    def _get_cached_result(self, file_path: str, cache_type: str) -> Optional[Any]:
        """è·å–ç¼“å­˜ç»“æœ"""
        try:
            cache_key = self._get_cache_key(file_path, cache_type)
            return self.redis_manager.get(cache_key, use_prefix=False)
        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜å¤±è´¥ {file_path}:{cache_type} - {e}")
            return None
    
    def _set_cached_result(self, file_path: str, cache_type: str, result: Any, ttl: int = 86400):
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        try:
            cache_key = self._get_cache_key(file_path, cache_type)
            self.redis_manager.set(cache_key, result, ttl=ttl, use_prefix=False)
        except Exception as e:
            logger.warning(f"è®¾ç½®ç¼“å­˜å¤±è´¥ {file_path}:{cache_type} - {e}")
    
    def _extract_project_name(self, file_path: Path) -> str:
        """ä»æ–‡ä»¶è·¯å¾„æå–é¡¹ç›®åç§°"""
        try:
            # è·å–ç›¸å¯¹äºçŸ¥è¯†åº“æ ¹ç›®å½•çš„è·¯å¾„
            relative_path = file_path.relative_to(self.knowledge_base_path)
            parts = relative_path.parts
            
            # æŒ‰è¦æ±‚ï¼Œå°†ç›®å½•ç»“æ„å­˜å‚¨ä¸ºLS
            if len(parts) >= 1:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šçš„é¡¹ç›®æ ‡è¯†
                for part in parts:
                    if 'zqyl-ls' in part.lower():
                        return 'zqyl-ls'
                    elif 'ls' in part.lower() or 'é“¾æ•°' in part:
                        return 'LS'
            
            # é»˜è®¤è¿”å›LS
            return "LS"
            
        except Exception as e:
            logger.warning(f"æå–é¡¹ç›®åç§°å¤±è´¥ {file_path}: {e}")
            return "LS"
        
    def _preprocess_document(self, document_content: str) -> List[Dict[str, Any]]:
        
        """
        Step 1: æ–‡æ¡£é¢„å¤„ç†
        å°†markdownæ–‡æ¡£æˆ–æ™®é€šæ–‡æœ¬æ‹†è§£ä¸ºç»“æ„åŒ–å†…å®¹å—
        
        Args:
            document_content: markdownæ ¼å¼æˆ–æ™®é€šæ–‡æœ¬çš„æ–‡æ¡£å†…å®¹
            
        Returns:
            ç»“æ„åŒ–å†…å®¹å—åˆ—è¡¨
        """
        structured_chunks = []
        
        if not document_content or not document_content.strip():
            logger.warning("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆç»“æ„åŒ–å—")
            return structured_chunks
        
        # æŒ‰è¡Œåˆ†å‰²æ–‡æ¡£
        lines = document_content.split('\n')
        current_section = ""
        current_content = []
        current_level = 0
        has_markdown_headers = any(line.strip().startswith('#') for line in lines)
        
        logger.info(f"æ–‡æ¡£æ€»è¡Œæ•°: {len(lines)}, æ˜¯å¦åŒ…å«Markdownæ ‡é¢˜: {has_markdown_headers}")
        
        if has_markdown_headers:
            # å¤„ç†Markdownæ ¼å¼æ–‡æ¡£
            for line in lines:
                line = line.strip()
                if not line:
                    continue
                    
                # æ£€æµ‹æ ‡é¢˜çº§åˆ«
                if line.startswith('#'):
                    # ä¿å­˜ä¹‹å‰çš„å†…å®¹å—
                    if current_section and current_content:
                        content_text = '\n'.join(current_content).strip()
                        if content_text:
                            # ç”Ÿæˆå‘é‡åµŒå…¥
                            embedding = self.embeddings_model.encode(f"{current_section}\n{content_text}").tolist()
                            
                            chunk = {
                                "section": current_section,
                                "content": content_text,
                                "level": current_level,
                                "embedding": embedding,
                                "image_refs": self._extract_image_refs(content_text)
                            }
                            structured_chunks.append(chunk)
                    
                    # å¼€å§‹æ–°çš„æ®µè½
                    current_level = len(line) - len(line.lstrip('#'))
                    current_section = line.lstrip('# ').strip()
                    current_content = []
                else:
                    # æ™®é€šå†…å®¹è¡Œ
                    current_content.append(line)
            
            # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
            if current_section and current_content:
                content_text = '\n'.join(current_content).strip()
                if content_text:
                    embedding = self.embeddings_model.encode(f"{current_section}\n{content_text}").tolist()
                    
                    chunk = {
                        "section": current_section,
                        "content": content_text,
                        "level": current_level,
                        "embedding": embedding,
                        "image_refs": self._extract_image_refs(content_text)
                    }
                    structured_chunks.append(chunk)
        else:
            # å¤„ç†æ™®é€šæ–‡æœ¬æ–‡æ¡£ï¼ŒæŒ‰æ®µè½åˆ†å‰²
            logger.info("å¤„ç†æ™®é€šæ–‡æœ¬æ–‡æ¡£ï¼ŒæŒ‰æ®µè½åˆ†å‰²")
            paragraphs = document_content.split('\n\n')
            
            for i, paragraph in enumerate(paragraphs):
                paragraph = paragraph.strip()
                if paragraph and len(paragraph) > 20:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ®µè½
                    # å°è¯•ä»æ®µè½å¼€å¤´æå–æ ‡é¢˜
                    lines_in_para = paragraph.split('\n')
                    first_line = lines_in_para[0].strip()
                    
                    # ç”Ÿæˆæ®µè½æ ‡é¢˜
                    if len(first_line) < 100 and len(lines_in_para) > 1:
                        section_title = first_line
                        content = '\n'.join(lines_in_para[1:]).strip()
                    else:
                        section_title = f"æ®µè½ {i + 1}"
                        content = paragraph
                    
                    if content:
                        # ç”Ÿæˆå‘é‡åµŒå…¥
                        embedding = self.embeddings_model.encode(f"{section_title}\n{content}").tolist()
                        
                        chunk = {
                            "section": section_title,
                            "content": content,
                            "level": 1,
                            "embedding": embedding,
                            "image_refs": self._extract_image_refs(content)
                        }
                        structured_chunks.append(chunk)
            
            # å¦‚æœæŒ‰æ®µè½åˆ†å‰²æ²¡æœ‰ç»“æœï¼Œä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨
            if not structured_chunks:
                logger.info("æ®µè½åˆ†å‰²æ— ç»“æœï¼Œä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨")
                text_chunks = self.text_splitter.split_text(document_content)
                
                for i, text_chunk in enumerate(text_chunks):
                    if text_chunk.strip():
                        # ç”Ÿæˆå‘é‡åµŒå…¥
                        embedding = self.embeddings_model.encode(text_chunk).tolist()
                        
                        chunk = {
                            "section": f"æ–‡æœ¬å— {i + 1}",
                            "content": text_chunk.strip(),
                            "level": 1,
                            "embedding": embedding,
                            "image_refs": self._extract_image_refs(text_chunk)
                        }
                        structured_chunks.append(chunk)
        
        logger.info(f"æœ€ç»ˆç”Ÿæˆ {len(structured_chunks)} ä¸ªç»“æ„åŒ–å—")
        return structured_chunks
    
    def _extract_image_refs(self, content: str) -> List[str]:
        """æå–å†…å®¹ä¸­çš„å›¾ç‰‡å¼•ç”¨"""
        image_refs = []
        # åŒ¹é…markdownå›¾ç‰‡è¯­æ³•: ![alt](path)
        image_pattern = re.compile(r'!\[.*?\]\((.*?)\)')
        matches = image_pattern.findall(content)
        image_refs.extend(matches)
        
        # åŒ¹é…å…¶ä»–å¯èƒ½çš„å›¾ç‰‡å¼•ç”¨æ ¼å¼
        ref_pattern = re.compile(r'\[å›¾ç‰‡[ï¼š:]\s*(.*?)\]')
        matches = ref_pattern.findall(content)
        image_refs.extend(matches)
        
        return image_refs
    
    def _delete_existing_document(self, file_name: str):
        """åˆ é™¤å·²å­˜åœ¨çš„åŒåæ–‡æ¡£è®°å½•"""
        try:
            collection = self.weaviate_client.collections.get("Document")
            
            # ä½¿ç”¨æ­£ç¡®çš„æŸ¥è¯¢è¯­æ³•ï¼Œä½¿ç”¨filterså‚æ•°è€Œä¸æ˜¯where
            existing_docs = collection.query.fetch_objects(
                filters=wvc.query.Filter.by_property("file_name").equal(file_name)
            )
            
            if existing_docs.objects:
                logger.info(f"ğŸ—‘ï¸ æ‰¾åˆ° {len(existing_docs.objects)} æ¡ç°æœ‰è®°å½•ï¼Œå¼€å§‹åˆ é™¤...")
                
                # åˆ é™¤æ‰€æœ‰åŒ¹é…çš„è®°å½•
                for doc in existing_docs.objects:
                    collection.data.delete_by_id(doc.uuid)
                    
                logger.info(f"âœ… å·²åˆ é™¤ {len(existing_docs.objects)} æ¡è®°å½•")
            else:
                logger.info(f"ğŸ“ æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶åä¸º {file_name} çš„ç°æœ‰è®°å½•")
                
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ç°æœ‰è®°å½•å¤±è´¥: {e}")
            # å¦‚æœåˆ é™¤å¤±è´¥ï¼Œæˆ‘ä»¬ç»§ç»­å¤„ç†ï¼Œä¸è¦è®©è¿™ä¸ªé”™è¯¯é˜»æ­¢æ•´ä¸ªæµç¨‹
            logger.warning(f"âš ï¸ è·³è¿‡åˆ é™¤æ­¥éª¤ï¼Œç»§ç»­å¤„ç†æ–‡æ¡£")
            pass
    
    def _process_docx_file(self, file_path: Path) -> List[Document]:
        """å¤„ç†Wordæ–‡æ¡£æ–‡ä»¶"""
        try:
            logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†Wordæ–‡æ¡£: {file_path}")
            
            # åˆå§‹åŒ–documentsåˆ—è¡¨
            documents = []
            
            #è¯»å–å½“å‰wordæ–‡æ¡£ï¼Œå¯¹å½“å‰æ–‡æ¡£æå–æ–‡æœ¬è½¬æ¢æˆmarkdownæ ¼å¼      
            document_content = self.extract_text_from_file(file_path)
            
            #å¯¹å½“å‰æ–‡æ¡£è¿›è¡Œé¢„å¤„ç†ï¼Œè½¬æ¢æˆç»“æ„åŒ–å†…å®¹å—
            structured_chunks = self._preprocess_document(document_content)
                
            logger.info(f"ğŸ“Š æ–‡æ¡£é¢„å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(structured_chunks)} ä¸ªç»“æ„åŒ–å—")
            
            # ä¸ºæ¯ä¸ªç»“æ„åŒ–å—åˆ›å»ºDocumentå¯¹è±¡
            for i, chunk in enumerate(structured_chunks):
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    "file_path": str(file_path),
                    "file_name": file_path.name,
                    "project": "LS",
                    "file_type": "docx",
                    "source_type": "requirement_doc",
                    "section": chunk["section"],
                    "chunk_index": i,
                                            "created_at": datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ'),
                    "image_refs": json.dumps(chunk["image_refs"], ensure_ascii=False),
                    "level": chunk["level"]
                }
                
                # åˆ›å»ºDocumentå¯¹è±¡
                doc_obj = Document(
                    page_content=chunk["content"],
                    metadata=metadata
                )
                
                # æ·»åŠ å‘é‡åµŒå…¥
                doc_obj.metadata["embedding"] = chunk["embedding"]
                
                documents.append(doc_obj)
            
            logger.info(f"âœ… Wordæ–‡æ¡£å¤„ç†å®Œæˆ: {file_path.name}ï¼Œç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£å—")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†Wordæ–‡æ¡£å¤±è´¥ {file_path}: {e}")
            raise
    def extract_text_from_file(self, file_path: str) -> str:
        """
        ä»æ–‡ä»¶è·¯å¾„è¯»å–æ–‡ä»¶å†…å®¹å¹¶è½¬æ¢ä¸ºMarkdownæˆ–æ–‡æœ¬å†…å®¹
        :param file_path: æ–‡ä»¶è·¯å¾„
        :return: extracted_text
        """
        file_content = None
        if file_path and os.path.exists(file_path):
            try:
                with open(file_path, 'rb') as f:
                    file_content = f.read()
                logger.info(f"ä»æ–‡ä»¶è·¯å¾„è¯»å–: {file_path}")
            except Exception as e:
                logger.warning(f"ä»æ–‡ä»¶è·¯å¾„è¯»å–å¤±è´¥: {e}")
        else:
            raise ValueError("æ— æ³•è·å–æ–‡ä»¶å†…å®¹ï¼Œæ–‡ä»¶å¯èƒ½å·²è¢«åˆ é™¤")

        # å¤„ç†æ–‡ä»¶å†…å®¹
        try:
            file_name = os.path.basename(file_path)
            file_type = file_name.split('.')[-1] if '.' in file_name else ''

            if file_name.lower().endswith(('.doc', '.docx')) or 'word' in file_type.lower():
                logger.info(f"æ£€æµ‹åˆ°Wordæ–‡æ¡£ï¼Œä½¿ç”¨Markdownè½¬æ¢: {file_name}")
                transform_result = self.parse_word_document(file_content, file_name)
                extracted_text = transform_result.get("text_content", "Wordæ–‡æ¡£è§£æå¤±è´¥")
                logger.info(f"è§£æç»“æœé•¿åº¦: {len(extracted_text) if extracted_text else 0}")
                logger.info(f"è§£æç»“æœå‰200å­—ç¬¦: {extracted_text[:200] if extracted_text else 'None'}")
                if extracted_text and any(marker in extracted_text for marker in ['#', '|', '**', '*', '-']):
                    logger.info(f"Wordæ–‡æ¡£å·²æˆåŠŸè½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œé•¿åº¦: {len(extracted_text)} å­—ç¬¦")
                else:
                    logger.warning("Wordæ–‡æ¡£è½¬æ¢ç»“æœå¯èƒ½ä¸æ˜¯æ ‡å‡†Markdownæ ¼å¼")
                    # å³ä½¿ä¸æ˜¯æ ‡å‡†Markdownæ ¼å¼ï¼Œåªè¦æœ‰å†…å®¹å°±ç»§ç»­å¤„ç†
                    if extracted_text and extracted_text.strip() and extracted_text != "Wordæ–‡æ¡£è§£æå¤±è´¥":
                        logger.info(f"ç»§ç»­å¤„ç†æ–‡æ¡£å†…å®¹ï¼Œé•¿åº¦: {len(extracted_text)} å­—ç¬¦")
            else:
                logger.info(f"ä½¿ç”¨é€šç”¨æ–‡ä»¶åˆ†ææ–¹æ³•: {file_name}")
                # å¯¹äºéWordæ–‡æ¡£ï¼Œæš‚æ—¶è¿”å›åŸºç¡€æ–‡æœ¬å†…å®¹
                try:
                    extracted_text = file_content.decode('utf-8', errors='ignore')
                except:
                    extracted_text = "æ–‡ä»¶è§£æå¤±è´¥ï¼šæ— æ³•è¯»å–æ–‡ä»¶å†…å®¹"
        except Exception as e:
            logger.error(f"EnhancedAnalyzerä½¿ç”¨å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€è§£æ")
            if file_name.lower().endswith(('.doc', '.docx')) or 'word' in file_type.lower():
                extracted_text = f"Wordæ–‡æ¡£è§£æå¤±è´¥: {str(e)}ï¼Œå»ºè®®æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–å®‰è£…python-docxåº“"
            else:
                extracted_text = f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}"
        return extracted_text
    
    def _get_fallback_result(self, file_name: str, analysis_type: str, reason: str) -> Dict[str, Any]:
        """è·å–å›é€€ç»“æœ"""
        return {
            "text_content": f"{analysis_type}å¤±è´¥: {reason}",
            "file_type": "unknown",
            "file_name": file_name,
            "analysis_method": "fallback",
            "extraction_summary": {
                "status": "failed",
                "reason": reason
            }
        }
    
    def _handle_legacy_doc_file(self, file_content: bytes, file_name: str) -> Dict[str, Any]:
        """å¤„ç†æ—§æ ¼å¼çš„.docæ–‡ä»¶"""
        return self._get_fallback_result(file_name, "æ—§æ ¼å¼DOCæ–‡ä»¶è§£æ", "ä¸æ”¯æŒ.docæ ¼å¼ï¼Œè¯·è½¬æ¢ä¸º.docxæ ¼å¼")
    
    def _handle_invalid_word_file(self, file_content: bytes, file_name: str) -> Dict[str, Any]:
        """å¤„ç†æ— æ•ˆçš„Wordæ–‡ä»¶"""
        return self._get_fallback_result(file_name, "æ— æ•ˆWordæ–‡ä»¶", "æ–‡ä»¶ç»“æ„ä¸ç¬¦åˆWordæ–‡æ¡£æ ¼å¼")
    
    def _handle_non_zip_file(self, file_content: bytes, file_name: str) -> Dict[str, Any]:
        """å¤„ç†éZIPæ ¼å¼æ–‡ä»¶"""
        return self._get_fallback_result(file_name, "éZIPæ ¼å¼æ–‡ä»¶", "æ–‡ä»¶ä¸æ˜¯æœ‰æ•ˆçš„ZIPæ ¼å¼")

    def parse_word_document(self, file_content: bytes, file_name: str) -> Dict[str, Any]:
        """
        è§£æWordæ–‡æ¡£ - ä½¿ç”¨å¤šç§åº“çš„å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            file_content: Wordæ–‡æ¡£çš„äºŒè¿›åˆ¶å†…å®¹
            file_name: æ–‡ä»¶å
            
        Returns:
            è§£æç»“æœå­—å…¸
        """
        # ä¼˜å…ˆä½¿ç”¨ mammoth è½¬æ¢ä¸º HTML å†è½¬ Markdown
        if MAMMOTH_AVAILABLE:
            try:
                logger.info(f"å°è¯•ä½¿ç”¨ mammoth è§£æ: {file_name}")
                file_stream = BytesIO(file_content)
                result = mammoth.convert_to_html(file_stream)
                html_content = result.value
                
                # ç®€å•çš„HTMLåˆ°Markdownè½¬æ¢
                markdown_content = self._html_to_markdown(html_content)
                
                return {
                    "text_content": markdown_content,
                    "file_type": "word",
                    "file_name": file_name,
                    "file_size": len(file_content),
                    "analysis_method": "mammoth_html_to_markdown",
                    "document_structure": {
                        "format": "markdown",
                        "conversion_messages": result.messages
                    },
                    "extraction_summary": {
                        "total_text_length": len(markdown_content),
                        "output_format": "markdown",
                        "method": "mammoth"
                    }
                }
            except Exception as e:
                logger.warning(f"mammoth è§£æå¤±è´¥: {e}ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ docx2txt
        if DOCX2TXT_AVAILABLE:
            try:
                logger.info(f"å°è¯•ä½¿ç”¨ docx2txt è§£æ: {file_name}")
                
                # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                import tempfile
                import os
                
                with tempfile.NamedTemporaryFile(suffix='.docx', delete=False) as tmp_file:
                    tmp_file.write(file_content)
                    tmp_file_path = tmp_file.name
                
                try:
                    # ä½¿ç”¨ docx2txt æå–æ–‡æœ¬
                    text_content = docx2txt.process(tmp_file_path)
                    
                    # ç®€å•çš„æ–‡æœ¬æ ¼å¼åŒ–ä¸ºMarkdown
                    markdown_content = self._text_to_markdown(text_content)
                    
                    return {
                        "text_content": markdown_content,
                        "file_type": "word",
                        "file_name": file_name,
                        "file_size": len(file_content),
                        "analysis_method": "docx2txt_to_markdown",
                        "document_structure": {
                            "format": "markdown"
                        },
                        "extraction_summary": {
                            "total_text_length": len(markdown_content),
                            "output_format": "markdown",
                            "method": "docx2txt"
                        }
                    }
                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if os.path.exists(tmp_file_path):
                        os.unlink(tmp_file_path)
                        
            except Exception as e:
                logger.warning(f"docx2txt è§£æå¤±è´¥: {e}")
        
        # æœ€åçš„å¤‡ç”¨æ–¹æ¡ˆï¼šè¿”å›é”™è¯¯ä¿¡æ¯
        return self._get_fallback_result(file_name, "Wordè§£æ", "æ‰€æœ‰Wordè§£æåº“éƒ½ä¸å¯ç”¨æˆ–è§£æå¤±è´¥")
    
    def _html_to_markdown(self, html_content: str) -> str:
        """
        å°†HTMLå†…å®¹è½¬æ¢ä¸ºMarkdownæ ¼å¼
        
        Args:
            html_content: HTMLæ ¼å¼çš„å†…å®¹
            
        Returns:
            Markdownæ ¼å¼çš„æ–‡æœ¬
        """
        try:
            import re
            
            # ç§»é™¤HTMLæ ‡ç­¾å¹¶è½¬æ¢ä¸ºMarkdown
            markdown_content = html_content
            
            # è½¬æ¢æ ‡é¢˜
            for i in range(6, 0, -1):
                markdown_content = re.sub(f'<h{i}[^>]*>(.*?)</h{i}>', f'{"#" * i} \\1', markdown_content, flags=re.IGNORECASE | re.DOTALL)
            
            # è½¬æ¢æ®µè½
            markdown_content = re.sub('<p[^>]*>(.*?)</p>', '\\1\n', markdown_content, flags=re.IGNORECASE | re.DOTALL)
            
            # è½¬æ¢ç²—ä½“
            markdown_content = re.sub('<strong[^>]*>(.*?)</strong>', '**\\1**', markdown_content, flags=re.IGNORECASE | re.DOTALL)
            markdown_content = re.sub('<b[^>]*>(.*?)</b>', '**\\1**', markdown_content, flags=re.IGNORECASE | re.DOTALL)
            
            # è½¬æ¢æ–œä½“
            markdown_content = re.sub('<em[^>]*>(.*?)</em>', '*\\1*', markdown_content, flags=re.IGNORECASE | re.DOTALL)
            markdown_content = re.sub('<i[^>]*>(.*?)</i>', '*\\1*', markdown_content, flags=re.IGNORECASE | re.DOTALL)
            
            # è½¬æ¢åˆ—è¡¨
            markdown_content = re.sub('<li[^>]*>(.*?)</li>', '- \\1', markdown_content, flags=re.IGNORECASE | re.DOTALL)
            markdown_content = re.sub('<[uo]l[^>]*>', '', markdown_content, flags=re.IGNORECASE)
            markdown_content = re.sub('</[uo]l>', '\n', markdown_content, flags=re.IGNORECASE)
            
            # è½¬æ¢è¡¨æ ¼ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            markdown_content = re.sub('<table[^>]*>.*?</table>', '[è¡¨æ ¼å†…å®¹]', markdown_content, flags=re.IGNORECASE | re.DOTALL)
            
            # ç§»é™¤å‰©ä½™çš„HTMLæ ‡ç­¾
            markdown_content = re.sub('<[^>]+>', '', markdown_content)
            
            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
            markdown_content = re.sub('\n\s*\n\s*\n', '\n\n', markdown_content)
            
            # è§£ç HTMLå®ä½“
            import html
            markdown_content = html.unescape(markdown_content)
            
            return markdown_content.strip()
            
        except Exception as e:
            logger.error(f"HTMLåˆ°Markdownè½¬æ¢å¤±è´¥: {e}")
            return html_content
    
    def _text_to_markdown(self, text_content: str) -> str:
        """
        å°†çº¯æ–‡æœ¬è½¬æ¢ä¸ºMarkdownæ ¼å¼
        
        Args:
            text_content: çº¯æ–‡æœ¬å†…å®¹
            
        Returns:
            Markdownæ ¼å¼çš„æ–‡æœ¬
        """
        try:
            if not text_content or not text_content.strip():
                return "æ–‡æ¡£å†…å®¹ä¸ºç©º"
            
            lines = text_content.split('\n')
            markdown_lines = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    markdown_lines.append('')
                    continue
                
                # æ£€æµ‹å¯èƒ½çš„æ ‡é¢˜ï¼ˆå…¨å¤§å†™ã€è¾ƒçŸ­ã€æˆ–åŒ…å«æ•°å­—ç¼–å·ï¼‰
                if (len(line) < 100 and 
                    (line.isupper() or 
                     re.match(r'^\d+[\.\)]\s*', line) or
                     re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.\)ã€]\s*', line) or
                     re.match(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚æ¡]\s*', line))):
                    
                    # åˆ¤æ–­æ ‡é¢˜çº§åˆ«
                    if re.match(r'^\d+[\.\)]\s*', line):
                        level = 2
                    elif re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.\)ã€]\s*', line):
                        level = 3
                    elif line.isupper():
                        level = 1
                    else:
                        level = 2
                        
                    markdown_lines.append('#' * level + ' ' + line)
                else:
                    # æ™®é€šæ®µè½
                    markdown_lines.append(line)
            
            result = '\n'.join(markdown_lines)
            
            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
            result = re.sub('\n\s*\n\s*\n', '\n\n', result)
            
            return result.strip()
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬åˆ°Markdownè½¬æ¢å¤±è´¥: {e}")
            return text_content
    
    def _convert_word_to_markdown(self, doc, file_content: bytes, file_name: str) -> str:
        """
        å°†Wordæ–‡æ¡£è½¬æ¢ä¸ºMarkdownæ ¼å¼ - ç®€åŒ–ç‰ˆæœ¬
        
        Args:
            doc: python-docx Documentå¯¹è±¡
            file_content: åŸå§‹æ–‡ä»¶å†…å®¹
            file_name: æ–‡ä»¶å
            
        Returns:
            Markdownæ ¼å¼çš„æ–‡æœ¬å†…å®¹
        """
        markdown_parts = []
        
        try:
            # å¤„ç†æ®µè½
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text = paragraph.text.strip()
                    
                    # æ£€æµ‹æ ‡é¢˜çº§åˆ«ï¼ˆåŸºäºæ ·å¼æˆ–æ ¼å¼ï¼‰
                    if paragraph.style and paragraph.style.name:
                        style_name = paragraph.style.name.lower()
                        if 'heading' in style_name or 'æ ‡é¢˜' in style_name:
                            # æå–æ ‡é¢˜çº§åˆ«
                            level = 1
                            if 'heading 1' in style_name or 'æ ‡é¢˜ 1' in style_name:
                                level = 1
                            elif 'heading 2' in style_name or 'æ ‡é¢˜ 2' in style_name:
                                level = 2
                            elif 'heading 3' in style_name or 'æ ‡é¢˜ 3' in style_name:
                                level = 3
                            elif 'heading 4' in style_name or 'æ ‡é¢˜ 4' in style_name:
                                level = 4
                            elif 'heading 5' in style_name or 'æ ‡é¢˜ 5' in style_name:
                                level = 5
                            elif 'heading 6' in style_name or 'æ ‡é¢˜ 6' in style_name:
                                level = 6
                            
                            markdown_parts.append('#' * level + ' ' + text)
                        else:
                            markdown_parts.append(text)
                    else:
                        # æ™®é€šæ®µè½
                        markdown_parts.append(text)
            
            # å¤„ç†è¡¨æ ¼
            for table_idx, table in enumerate(doc.tables):
                if table.rows:
                    table_md = []
                    
                    # è¡¨å¤´
                    header_row = table.rows[0]
                    headers = []
                    for cell in header_row.cells:
                        headers.append(cell.text.strip() or "åˆ—")
                    
                    if headers:
                        table_md.append("| " + " | ".join(headers) + " |")
                        table_md.append("| " + " | ".join(["---"] * len(headers)) + " |")
                        
                        # æ•°æ®è¡Œ
                        for row in table.rows[1:]:
                            row_data = []
                            for cell in row.cells:
                                row_data.append(cell.text.strip().replace('\n', ' ') or "")
                            
                            # ç¡®ä¿è¡Œæ•°æ®é•¿åº¦ä¸è¡¨å¤´ä¸€è‡´
                            while len(row_data) < len(headers):
                                row_data.append("")
                            
                            table_md.append("| " + " | ".join(row_data) + " |")
                    
                    if table_md:
                        markdown_parts.append('\n'.join(table_md))
            
            return '\n\n'.join(markdown_parts) if markdown_parts else "æ–‡æ¡£å†…å®¹ä¸ºç©º"
            
        except Exception as e:
            logger.error(f"Markdownè½¬æ¢å¤±è´¥: {e}")
            # å›é€€åˆ°åŸºæœ¬æ–‡æœ¬æå–
            text_content = []
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    text_content.append(paragraph.text.strip())
            
            return '\n\n'.join(text_content) if text_content else "æ–‡æ¡£è§£æå¤±è´¥"
    



    def store_documents_to_weaviate(self, documents: List[Document]):
        """å°†æ–‡æ¡£å­˜å‚¨åˆ°Weaviate"""
        try:
            collection = self.weaviate_client.collections.get("Document")
            
            # æ‰¹é‡æ’å…¥æ•°æ®
            batch_size = 100
            total_stored = 0
            
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i + batch_size]
                
                # å‡†å¤‡æ‰¹é‡æ•°æ®
                objects_to_insert = []
                
                for doc in batch:
                    # æå–åµŒå…¥å‘é‡
                    embedding = doc.metadata.pop("embedding", None)
                    
                    # å‡†å¤‡æ•°æ®å¯¹è±¡
                    data_object = {
                        "content": doc.page_content,
                        "file_path": doc.metadata.get("file_path", ""),
                        "file_name": doc.metadata.get("file_name", ""),
                        "project": doc.metadata.get("project", ""),
                        "file_type": doc.metadata.get("file_type", ""),
                        "source_type": doc.metadata.get("source_type", ""),
                        "image_path": doc.metadata.get("image_path", ""),
                        "section": doc.metadata.get("section", ""),
                        "sheet_name": doc.metadata.get("sheet_name", ""),
                        "chunk_index": doc.metadata.get("chunk_index", 0),
                        "created_at": doc.metadata.get("created_at", datetime.now().strftime('%Y-%m-%dT%H:%M:%SZ')),
                        "ocr_text": doc.metadata.get("ocr_text", ""),
                        "caption": doc.metadata.get("caption", "")
                    }
                    
                    objects_to_insert.append(wvc.data.DataObject(
                        properties=data_object,
                        vector=embedding
                    ))
                
                # æ‰¹é‡æ’å…¥
                response = collection.data.insert_many(objects_to_insert)
                
                # æ£€æŸ¥å“åº”ç±»å‹å¹¶å¤„ç†é”™è¯¯
                failed_count = 0
                if hasattr(response, 'failed_objects'):
                    # æ—§ç‰ˆæœ¬API
                    failed_objects = response.failed_objects
                    if failed_objects:
                        failed_count = len(failed_objects)
                        logger.warning(f"âš ï¸ æ‰¹æ¬¡ {i//batch_size + 1} ä¸­æœ‰ {failed_count} ä¸ªå¯¹è±¡æ’å…¥å¤±è´¥")
                        for failed_obj in failed_objects:
                            logger.warning(f"   å¤±è´¥åŸå› : {failed_obj.message}")
                elif hasattr(response, 'all_responses'):
                    # æ–°ç‰ˆæœ¬API
                    for resp in response.all_responses:
                        if hasattr(resp, 'errors') and resp.errors:
                            failed_count += 1
                            logger.warning(f"   æ’å…¥å¤±è´¥: {resp.errors}")
                elif hasattr(response, 'errors') and response.errors:
                    # å¦ä¸€ç§æ–°ç‰ˆæœ¬APIæ ¼å¼
                    failed_count = len(response.errors)
                    logger.warning(f"âš ï¸ æ‰¹æ¬¡ {i//batch_size + 1} ä¸­æœ‰ {failed_count} ä¸ªå¯¹è±¡æ’å…¥å¤±è´¥")
                    for error in response.errors:
                        logger.warning(f"   å¤±è´¥åŸå› : {error}")
                else:
                    # å‡è®¾å…¨éƒ¨æˆåŠŸ
                    logger.info(f"âœ… æ‰¹æ¬¡ {i//batch_size + 1} å…¨éƒ¨æ’å…¥æˆåŠŸ")
                
                successful_count = len(batch) - failed_count
                total_stored += successful_count
                
                logger.info(f"âœ… æ‰¹æ¬¡ {i//batch_size + 1}/{(len(documents) + batch_size - 1) // batch_size} å®Œæˆï¼ŒæˆåŠŸå­˜å‚¨ {successful_count} ä¸ªæ–‡æ¡£")
            
            logger.info(f"ğŸ‰ æ–‡æ¡£å­˜å‚¨å®Œæˆï¼æ€»è®¡å­˜å‚¨ {total_stored}/{len(documents)} ä¸ªæ–‡æ¡£åˆ°Weaviate")
            return total_stored
            
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨æ–‡æ¡£åˆ°Weaviateå¤±è´¥: {e}")
            raise
    
    def process_single_document(self, document_path: str):
        """å¤„ç†å•ä¸ªæ–‡æ¡£çš„å®Œæ•´æµç¨‹"""
        try:
            file_path = Path(document_path)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not file_path.exists():
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {document_path}")
            
            # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
            if file_path.suffix.lower() != '.docx':
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
            
            logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path.name}")
            
            # 1. åˆ›å»ºWeaviateæ¨¡å¼
            collection_name = self._create_weaviate_schema()
            
            # 2. åˆ é™¤å·²å­˜åœ¨çš„åŒåæ–‡ä»¶è®°å½•
            self._delete_existing_document(file_path.name)
            
            # 3. å¤„ç†æ–‡æ¡£
            documents = self._process_docx_file(file_path)
            
            # 4. å­˜å‚¨åˆ°Weaviate
            stored_count = self.store_documents_to_weaviate(documents)
            
            logger.info(f"ğŸ‰ æ–‡æ¡£å¤„ç†å®Œæˆï¼")
            logger.info(f"   ğŸ“„ æ–‡ä»¶: {file_path.name}")
            logger.info(f"   ğŸ“Š ç”Ÿæˆæ–‡æ¡£å—: {len(documents)}")
            logger.info(f"   ğŸ’¾ æˆåŠŸå­˜å‚¨: {stored_count}")
            
            return {
                "success": True,
                "file_name": file_path.name,
                "total_chunks": len(documents),
                "stored_chunks": stored_count,
                "collection": collection_name
            }
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡æ¡£å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "file_name": Path(document_path).name if document_path else "unknown"
            }
    
    def close(self):
        """å…³é—­è¿æ¥"""
        try:
            if self.weaviate_client:
                self.weaviate_client.close()
                logger.info("âœ… Weaviate å®¢æˆ·ç«¯è¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.warning(f"âš ï¸ å…³é—­è¿æ¥æ—¶å‡ºç°è­¦å‘Š: {e}")


def main():
    """ä¸»å‡½æ•° - å¤„ç†ç‰¹å®šæ–‡æ¡£å‘é‡åŒ–"""
    initializer = None
    
    try:
        # æŒ‡å®šè¦å¤„ç†çš„æ–‡æ¡£è·¯å¾„
        document_path = r"D:\knowledge_base\é“¾æ•°_LS\éœ€æ±‚æ–‡æ¡£\LS-1(YS-72)_éœ€æ±‚æ–‡æ¡£-é“¾æ•°ä¸€æœŸV1.8.docx"
        
        print(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£å‘é‡åŒ–...")
        print(f"ğŸ“„ ç›®æ ‡æ–‡æ¡£: {document_path}")
        
        # åˆ›å»ºåˆå§‹åŒ–å™¨
        initializer = KnowledgeBaseInitializer()
        
        # å¤„ç†å•ä¸ªæ–‡æ¡£
        result = initializer.process_single_document(document_path)
        
        # è¾“å‡ºç»“æœ
        if result["success"]:
            print(f"\nğŸ‰ æ–‡æ¡£å‘é‡åŒ–æˆåŠŸï¼")
            print(f"   ğŸ“„ æ–‡ä»¶å: {result['file_name']}")
            print(f"   ğŸ“Š ç”Ÿæˆå—æ•°: {result['total_chunks']}")
            print(f"   ğŸ’¾ å­˜å‚¨å—æ•°: {result['stored_chunks']}")
            print(f"   ğŸ—„ï¸ é›†åˆåç§°: {result['collection']}")
        else:
            print(f"\nâŒ æ–‡æ¡£å‘é‡åŒ–å¤±è´¥!")
            print(f"   ğŸ“„ æ–‡ä»¶å: {result['file_name']}")
            print(f"   ğŸ”¥ é”™è¯¯ä¿¡æ¯: {result['error']}")
            
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if initializer:
            initializer.close()


if __name__ == "__main__":
    main() 