#!/usr/bin/env python3
"""
OpenAIå®¢æˆ·ç«¯ç»„ä»¶
æ”¯æŒç«å±±å¼•æ“ã€OpenAIã€DeepSeekç­‰å¤šç§å…¼å®¹API
é»˜è®¤ä½¿ç”¨ç«å±±å¼•æ“API
"""

import os
import time
import logging
from typing import Optional, Dict, Any, List, Union
from dataclasses import dataclass

try:
    from openai import OpenAI
except ImportError:
    raise ImportError("è¯·å®‰è£…OpenAIåŒ…: pip install openai")

# å¯¼å…¥é…ç½®
try:
    from .config import settings
    from .llm_logger import log_llm_request, log_llm_response, log_llm_stream_chunk
except ImportError:
    # å¦‚æœåœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼Œç›´æ¥åŠ è½½ç¯å¢ƒå˜é‡
    from dotenv import load_dotenv
    load_dotenv()
    
    class MockSettings:
        VOLCENGINE_API_KEY = os.getenv("VOLCENGINE_API_KEY", "")
        VOLCENGINE_MODEL_ID = os.getenv("VOLCENGINE_MODEL_ID", "ep-20250528194304-wbvcf")
        VOLCENGINE_BASE_URL = os.getenv("VOLCENGINE_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3")
        DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
        DEEPSEEK_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
        OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
        OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        DEFAULT_TEMPERATURE = 0.7
        DEFAULT_MAX_TOKENS = 2000
        DEFAULT_TIMEOUT = 120  # è°ƒæ•´ä¸º2åˆ†é’Ÿï¼Œé€‚åº”å¤§æ¨¡å‹å“åº”æ—¶é—´
    
    settings = MockSettings()
    
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºç©ºçš„æ—¥å¿—å‡½æ•°
    def log_llm_request(*args, **kwargs):
        return f"openai_{int(time.time() * 1000)}"
    
    def log_llm_response(*args, **kwargs):
        pass
    
    def log_llm_stream_chunk(*args, **kwargs):
        pass

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AIClientConfig:
    """AIå®¢æˆ·ç«¯é…ç½®"""
    api_key: str
    base_url: str
    model: str
    temperature: float = 0.7
    max_tokens: int = 2000
    timeout: int = 120  # è°ƒæ•´ä¸º2åˆ†é’Ÿï¼Œé€‚åº”å¤§æ¨¡å‹å“åº”æ—¶é—´

class OpenAIClient:
    """OpenAIå…¼å®¹å®¢æˆ·ç«¯"""
    
    def __init__(self, config: AIClientConfig):
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        
        Args:
            config: APIé…ç½®
        """
        self.config = config
        self.client = OpenAI(
            api_key=config.api_key,
            base_url=config.base_url,
            timeout=config.timeout
        )
        
        logger.info(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {config.model}")
    
    def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        **kwargs
    ) -> Union[str, Any]:
        """
        èŠå¤©å®Œæˆ
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            å“åº”å†…å®¹æˆ–æµå¯¹è±¡
        """
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        used_model = model or self.config.model
        request_params = {
            "model": used_model,
            "temperature": temperature or self.config.temperature,
            "max_tokens": max_tokens or self.config.max_tokens,
            "stream": stream,
            **kwargs
        }
        
        # è®°å½•è¯·æ±‚æ—¥å¿—
        request_id = log_llm_request(
            provider=self.config.model.lower().replace(" ", "_"),
            model=used_model,
            messages=messages,
            parameters=request_params
        )
        
        start_time = time.time()
        
        try:
            response = self.client.chat.completions.create(
                messages=messages,
                **request_params
            )
            
            response_time = time.time() - start_time
            
            if stream:
                # è®°å½•æµå¼å“åº”å¼€å§‹
                log_llm_response(
                    request_id=request_id,
                    response_content="[STREAM_START]",
                    response_time=response_time,
                    token_usage=None
                )
                return response
            else:
                response_content = response.choices[0].message.content
                
                # æå–tokenä½¿ç”¨æƒ…å†µ
                token_usage = None
                if hasattr(response, 'usage') and response.usage:
                    token_usage = {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    }
                
                # è®°å½•å“åº”æ—¥å¿—
                log_llm_response(
                    request_id=request_id,
                    response_content=response_content,
                    response_time=response_time,
                    token_usage=token_usage
                )
                
                return response_content
                
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = str(e)
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            log_llm_response(
                request_id=request_id,
                response_content="",
                response_time=response_time,
                error=error_msg
            )
            
            logger.error(f"APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def stream_chat(
        self,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        **kwargs
    ):
        """
        æµå¼èŠå¤©
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            æµå¼å“åº”å†…å®¹
        """
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        used_model = model or self.config.model
        request_params = {
            "model": used_model,
            "temperature": self.config.temperature,
            "max_tokens": self.config.max_tokens,
            "stream": True,
            **kwargs
        }
        
        # è®°å½•è¯·æ±‚æ—¥å¿—
        request_id = log_llm_request(
            provider=self.config.model.lower().replace(" ", "_"),
            model=used_model,
            messages=messages,
            parameters=request_params
        )
        
        start_time = time.time()
        chunk_index = 0
        full_response = ""
        
        try:
            stream = self.client.chat.completions.create(
                messages=messages,
                **request_params
            )
            
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    chunk_content = chunk.choices[0].delta.content
                    full_response += chunk_content
                    
                    # è®°å½•æµå¼å“åº”å—
                    log_llm_stream_chunk(
                        request_id=request_id,
                        chunk_content=chunk_content,
                        chunk_index=chunk_index
                    )
                    
                    chunk_index += 1
                    yield chunk_content
            
            # è®°å½•å®Œæ•´å“åº”
            response_time = time.time() - start_time
            log_llm_response(
                request_id=request_id,
                response_content=full_response,
                response_time=response_time,
                token_usage=None  # æµå¼å“åº”é€šå¸¸ä¸è¿”å›tokenä½¿ç”¨æƒ…å†µ
            )
                    
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = str(e)
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            log_llm_response(
                request_id=request_id,
                response_content=full_response,
                response_time=response_time,
                error=error_msg
            )
            
            logger.error(f"æµå¼APIè°ƒç”¨å¤±è´¥: {e}")
            raise

class MultiAPIManager:
    """å¤šAPIç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤šAPIç®¡ç†å™¨"""
        self.clients = {}
        self.default_client = None
        self._load_configs()
    
    def _load_configs(self):
        """åŠ è½½APIé…ç½®"""
        # ç«å±±å¼•æ“é…ç½®ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        if settings.VOLCENGINE_API_KEY:
            config = AIClientConfig(
                api_key=settings.VOLCENGINE_API_KEY,
                base_url=settings.VOLCENGINE_BASE_URL,
                model=settings.VOLCENGINE_MODEL_ID,
                temperature=settings.DEFAULT_TEMPERATURE,
                max_tokens=settings.DEFAULT_MAX_TOKENS,
                timeout=settings.DEFAULT_TIMEOUT
            )
            self.add_client("volcengine", config)
        
        # DeepSeeké…ç½®
        if settings.DEEPSEEK_API_KEY:
            config = AIClientConfig(
                api_key=settings.DEEPSEEK_API_KEY,
                base_url=settings.DEEPSEEK_BASE_URL,
                model="deepseek-chat",
                temperature=settings.DEFAULT_TEMPERATURE,
                max_tokens=settings.DEFAULT_MAX_TOKENS,
                timeout=settings.DEFAULT_TIMEOUT
            )
            self.add_client("deepseek", config)
        
        # OpenAIé…ç½®
        if settings.OPENAI_API_KEY:
            config = AIClientConfig(
                api_key=settings.OPENAI_API_KEY,
                base_url=settings.OPENAI_BASE_URL,
                model="gpt-3.5-turbo",
                temperature=settings.DEFAULT_TEMPERATURE,
                max_tokens=settings.DEFAULT_MAX_TOKENS,
                timeout=settings.DEFAULT_TIMEOUT
            )
            self.add_client("openai", config)
        
        # è®¾ç½®é»˜è®¤å®¢æˆ·ç«¯ï¼ˆä¼˜å…ˆä½¿ç”¨ç«å±±å¼•æ“ï¼‰
        if "volcengine" in self.clients:
            self.default_client = "volcengine"
        elif "deepseek" in self.clients:
            self.default_client = "deepseek"
        elif "openai" in self.clients:
            self.default_client = "openai"
    
    def add_client(self, name: str, config: AIClientConfig):
        """
        æ·»åŠ å®¢æˆ·ç«¯
        
        Args:
            name: å®¢æˆ·ç«¯åç§°
            config: APIé…ç½®
        """
        try:
            client = OpenAIClient(config)
            self.clients[name] = client
            logger.info(f"æ·»åŠ å®¢æˆ·ç«¯æˆåŠŸ: {name}")
        except Exception as e:
            logger.error(f"æ·»åŠ å®¢æˆ·ç«¯å¤±è´¥ {name}: {e}")
    
    def get_client(self, name: Optional[str] = None) -> OpenAIClient:
        """
        è·å–å®¢æˆ·ç«¯
        
        Args:
            name: å®¢æˆ·ç«¯åç§°ï¼Œä¸ºNoneæ—¶ä½¿ç”¨é»˜è®¤å®¢æˆ·ç«¯
            
        Returns:
            OpenAIå®¢æˆ·ç«¯
        """
        client_name = name or self.default_client
        
        if not client_name or client_name not in self.clients:
            available = list(self.clients.keys())
            raise ValueError(f"å®¢æˆ·ç«¯ {client_name} ä¸å­˜åœ¨ï¼Œå¯ç”¨å®¢æˆ·ç«¯: {available}")
        
        return self.clients[client_name]
    
    def list_clients(self) -> List[str]:
        """è·å–å¯ç”¨å®¢æˆ·ç«¯åˆ—è¡¨"""
        return list(self.clients.keys())
    
    def test_client(self, name: str) -> bool:
        """
        æµ‹è¯•å®¢æˆ·ç«¯è¿æ¥
        
        Args:
            name: å®¢æˆ·ç«¯åç§°
            
        Returns:
            æµ‹è¯•æ˜¯å¦æˆåŠŸ
        """
        try:
            client = self.get_client(name)
            messages = [{"role": "user", "content": "Hello"}]
            response = client.chat_completion(messages)
            logger.info(f"å®¢æˆ·ç«¯ {name} æµ‹è¯•æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"å®¢æˆ·ç«¯ {name} æµ‹è¯•å¤±è´¥: {e}")
            return False

# å…¨å±€APIç®¡ç†å™¨å®ä¾‹
api_manager = MultiAPIManager()

def get_openai_client(name: Optional[str] = None) -> OpenAIClient:
    """
    è·å–OpenAIå®¢æˆ·ç«¯ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        name: å®¢æˆ·ç«¯åç§°ï¼Œé»˜è®¤ä½¿ç”¨ç«å±±å¼•æ“
        
    Returns:
        OpenAIå®¢æˆ·ç«¯
    """
    return api_manager.get_client(name)

def chat_with_ai(
    messages: List[Dict[str, str]],
    client_name: Optional[str] = None,
    **kwargs
) -> str:
    """
    ä¸AIèŠå¤©ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        client_name: å®¢æˆ·ç«¯åç§°ï¼Œé»˜è®¤ä½¿ç”¨ç«å±±å¼•æ“
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        AIå›å¤
    """
    client = get_openai_client(client_name)
    return client.chat_completion(messages, **kwargs)

def stream_chat_with_ai(
    messages: List[Dict[str, str]],
    client_name: Optional[str] = None,
    **kwargs
):
    """
    æµå¼èŠå¤©ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        client_name: å®¢æˆ·ç«¯åç§°ï¼Œé»˜è®¤ä½¿ç”¨ç«å±±å¼•æ“
        **kwargs: å…¶ä»–å‚æ•°
        
    Yields:
        æµå¼å“åº”
    """
    client = get_openai_client(client_name)
    yield from client.stream_chat(messages, **kwargs)

# é¢„å®šä¹‰çš„ä»»åŠ¡å‡½æ•°
def analyze_requirement(content: str, client_name: Optional[str] = None) -> str:
    """
    åˆ†æéœ€æ±‚æ–‡æ¡£
    
    Args:
        content: éœ€æ±‚å†…å®¹
        client_name: å®¢æˆ·ç«¯åç§°ï¼Œé»˜è®¤ä½¿ç”¨ç«å±±å¼•æ“
        
    Returns:
        åˆ†æç»“æœ
    """
    messages = [
        {
            "role": "system",
            "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹éœ€æ±‚æ–‡æ¡£ï¼Œæå–ï¼š
1. æ ¸å¿ƒåŠŸèƒ½ç‚¹
2. ä¸šåŠ¡å…³é”®è¯
3. æŠ€æœ¯å…³é”®è¯
4. æ•°æ®å­—æ®µéœ€æ±‚
5. æ½œåœ¨é—®é¢˜å’Œå»ºè®®

è¯·ç”¨ç»“æ„åŒ–çš„æ–¹å¼è¾“å‡ºåˆ†æç»“æœã€‚"""
        },
        {
            "role": "user",
            "content": f"éœ€æ±‚æ–‡æ¡£ï¼š\n{content}"
        }
    ]
    
    return chat_with_ai(messages, client_name, temperature=0.3)

def generate_api_design(requirement: str, client_name: Optional[str] = None) -> str:
    """
    ç”ŸæˆAPIè®¾è®¡
    
    Args:
        requirement: éœ€æ±‚å†…å®¹
        client_name: å®¢æˆ·ç«¯åç§°ï¼Œé»˜è®¤ä½¿ç”¨ç«å±±å¼•æ“
        
    Returns:
        APIè®¾è®¡
    """
    messages = [
        {
            "role": "system",
            "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„APIè®¾è®¡å¸ˆã€‚è¯·æ ¹æ®éœ€æ±‚è®¾è®¡RESTful APIï¼ŒåŒ…æ‹¬ï¼š
1. æ¥å£åˆ—è¡¨
2. è¯·æ±‚/å“åº”æ ¼å¼
3. æ•°æ®æ¨¡å‹
4. é”™è¯¯å¤„ç†

è¯·ç”¨JSONæ ¼å¼è¾“å‡ºAPIè®¾è®¡ã€‚"""
        },
        {
            "role": "user",
            "content": f"éœ€æ±‚ï¼š\n{requirement}"
        }
    ]
    
    return chat_with_ai(messages, client_name, temperature=0.4)

def generate_code(description: str, language: str = "python", client_name: Optional[str] = None) -> str:
    """
    ç”Ÿæˆä»£ç 
    
    Args:
        description: ä»£ç æè¿°
        language: ç¼–ç¨‹è¯­è¨€
        client_name: å®¢æˆ·ç«¯åç§°ï¼Œé»˜è®¤ä½¿ç”¨ç«å±±å¼•æ“
        
    Returns:
        ç”Ÿæˆçš„ä»£ç 
    """
    messages = [
        {
            "role": "system",
            "content": f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{language}ç¨‹åºå‘˜ã€‚è¯·æ ¹æ®æè¿°ç”Ÿæˆé«˜è´¨é‡çš„ä»£ç ï¼ŒåŒ…æ‹¬ï¼š
1. å®Œæ•´çš„ä»£ç å®ç°
2. å¿…è¦çš„æ³¨é‡Š
3. é”™è¯¯å¤„ç†
4. ä½¿ç”¨ç¤ºä¾‹

è¯·ç¡®ä¿ä»£ç è§„èŒƒã€å¯è¯»æ€§å¼ºã€‚"""
        },
        {
            "role": "user",
            "content": f"è¯·ç”¨{language}å®ç°ï¼š\n{description}"
        }
    ]
    
    return chat_with_ai(messages, client_name, temperature=0.2)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ¤– OpenAIå®¢æˆ·ç«¯ç»„ä»¶æµ‹è¯•")
    print("=" * 50)
    
    # åˆ—å‡ºå¯ç”¨å®¢æˆ·ç«¯
    clients = api_manager.list_clients()
    print(f"ğŸ“‹ å¯ç”¨å®¢æˆ·ç«¯: {clients}")
    
    if not clients:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„å®¢æˆ·ç«¯ï¼Œè¯·è®¾ç½®APIå¯†é’¥")
    else:
        # æµ‹è¯•é»˜è®¤å®¢æˆ·ç«¯
        default_client = api_manager.default_client
        print(f"ğŸ¯ é»˜è®¤å®¢æˆ·ç«¯: {default_client}")
        
        # æµ‹è¯•èŠå¤©åŠŸèƒ½
        try:
            response = chat_with_ai([
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
            ])
            print(f"âœ… èŠå¤©æµ‹è¯•æˆåŠŸ: {response[:100]}...")
        except Exception as e:
            print(f"âŒ èŠå¤©æµ‹è¯•å¤±è´¥: {e}")
        
        # æµ‹è¯•éœ€æ±‚åˆ†æ
        try:
            analysis = analyze_requirement("å¼€å‘ä¸€ä¸ªç”¨æˆ·ç®¡ç†ç³»ç»Ÿï¼ŒåŒ…å«æ³¨å†Œã€ç™»å½•ã€ä¸ªäººä¿¡æ¯ç®¡ç†åŠŸèƒ½")
            print(f"âœ… éœ€æ±‚åˆ†ææµ‹è¯•æˆåŠŸ: {analysis[:100]}...")
        except Exception as e:
            print(f"âŒ éœ€æ±‚åˆ†ææµ‹è¯•å¤±è´¥: {e}") 