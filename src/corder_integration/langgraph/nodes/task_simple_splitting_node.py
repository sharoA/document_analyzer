#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆä»»åŠ¡æ‹†åˆ†èŠ‚ç‚¹ - ä½¿ç”¨ç«å±±å¼•æ“LLMå°†è®¾è®¡æ–‡æ¡£æ‹†è§£æˆSQLæ ¼å¼çš„æ‰§è¡Œä»»åŠ¡
"""

import logging
import json
import re
import uuid
import yaml
import os
from datetime import datetime
from typing import Dict, Any, List, Optional

# å¯¼å…¥LLMå®¢æˆ·ç«¯
from src.utils.volcengine_client import VolcengineClient, VolcengineConfig

logger = logging.getLogger(__name__)

class TaskSimpleSplittingNode:
    """ç®€åŒ–ç‰ˆä»»åŠ¡æ‹†åˆ†èŠ‚ç‚¹ - ä½¿ç”¨LLMæ™ºèƒ½è§£æ"""
    
    def __init__(self):
        self.task_types = [
            'git_extraction',   # æå–Gitåœ°å€
            'git_clone',        # ä¸‹è½½ä»£ç   
            'code_analysis',    # åˆ†æç»“æ„
            'config',           # é…ç½®ç¯å¢ƒ
            'database',         # å»ºè¡¨
            'api',              # å¼€å‘æ¥å£
            'integration_test', # é›†æˆæµ‹è¯•
            'deployment'        # ä»£ç æäº¤
        ]
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        self.llm_client = None
        self.llm_provider = None
        self._init_llm_client()
        
    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        config = {}
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„é…ç½®æ–‡ä»¶è·¯å¾„
        config_paths = [
            'config.yaml',
            os.path.join(os.getcwd(), 'config.yaml'),
            os.path.join(os.path.dirname(os.path.abspath(__file__)), '../../../config.yaml'),
            '/Users/renyu/Documents/ai_project/document_analyzer/config.yaml'
        ]
        
        config_loaded = False
        for config_path in config_paths:
            try:
                if os.path.exists(config_path):
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = yaml.safe_load(f) or {}
                    logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                    config_loaded = True
                    break
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ {config_path}: {e}")
        
        if not config_loaded:
            logger.error(f"âŒ æ‰€æœ‰é…ç½®æ–‡ä»¶è·¯å¾„éƒ½åŠ è½½å¤±è´¥: {config_paths}")
        
        # ä¼˜å…ˆä½¿ç”¨ç«å±±å¼•æ“
        if config and config.get('volcengine', {}).get('api_key'):
            try:
                # ğŸ†• ä¼˜å…ˆä½¿ç”¨ç¼–ç æ™ºèƒ½ä½“ä¸“ç”¨æ¨¡å‹é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é€šç”¨é…ç½®
                coder_model = config.get('coder_agent', {}).get('code_generation_model')
                model_id = coder_model if coder_model else config['volcengine']['model']
                
                volcengine_config = VolcengineConfig(
                    api_key=config['volcengine']['api_key'],
                    model_id=model_id,
                    base_url=config['volcengine']['endpoint'],
                    temperature=config['volcengine'].get('temperature', 0.1),
                    max_tokens=config['volcengine'].get('max_tokens', 4000)
                )
                self.llm_client = VolcengineClient(volcengine_config)
                self.llm_provider = "volcengine"
                logger.info(f"âœ… ä½¿ç”¨ç«å±±å¼•æ“LLMå®¢æˆ·ç«¯ï¼š{model_id}{'(ç¼–ç æ™ºèƒ½ä½“ä¸“ç”¨)' if coder_model else '(é€šç”¨é…ç½®)'}")
            except Exception as e:
                logger.error(f"âŒ ç«å±±å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # æœ€ç»ˆæ£€æŸ¥
        if not self.llm_client:
            logger.error("âŒ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼å°†ä½¿ç”¨è§„åˆ™åŒ–è§£æ")
            self.llm_provider = "none"
        else:
            logger.info(f"âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {self.llm_provider}")
        
    def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """èŠ‚ç‚¹ä¸»å…¥å£"""
        logger.info("ğŸš€ å¼€å§‹ç®€åŒ–ç‰ˆä»»åŠ¡æ‹†åˆ†")
        
        try:
            # è·å–è®¾è®¡æ–‡æ¡£å’Œé¡¹ç›®ä¿¡æ¯
            design_doc = state.get('design_doc', '')
            project_name = state.get('project_name', 'default_project')
            project_task_id = state.get('project_task_id')  # ğŸ†• è·å–é¡¹ç›®å”¯ä¸€æ ‡è¯†
            
            logger.info(f"ğŸ“„ è®¾è®¡æ–‡æ¡£é•¿åº¦: {len(design_doc)}")
            logger.info(f"ğŸ“‹ é¡¹ç›®åç§°: {project_name}")
            logger.info(f"ğŸ·ï¸ é¡¹ç›®æ ‡è¯†: {project_task_id}")
            
            # ä½¿ç”¨ç°æœ‰çš„Jinja2æ¨¡æ¿ç”Ÿæˆä»»åŠ¡
            tasks = self._generate_tasks_with_template(design_doc, project_name, project_task_id)
            
            if not tasks:
                logger.warning("âš ï¸ æœªç”Ÿæˆä»»ä½•ä»»åŠ¡")
                return self._empty_result()
            
            logger.info(f"âœ… ä»»åŠ¡æ‹†åˆ†å®Œæˆï¼Œç”Ÿæˆ {len(tasks)} ä¸ªä»»åŠ¡")
            
            # ğŸ†• æ·»åŠ ä»»åŠ¡è¯¦æƒ…æ—¥å¿—
            for i, task in enumerate(tasks[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªä»»åŠ¡é¿å…æ—¥å¿—è¿‡é•¿
                logger.info(f"ğŸ“‹ ä»»åŠ¡ {i+1}: {task.get('task_type', 'unknown')} - {task.get('description', 'no description')[:50]}...")
            if len(tasks) > 3:
                logger.info(f"ğŸ“‹ è¿˜æœ‰ {len(tasks) - 3} ä¸ªä»»åŠ¡æœªæ˜¾ç¤º")
            
            # ä¿å­˜åˆ°SQLiteæ•°æ®åº“ç”¨äºå·¥ä½œæµ
            self._save_to_database(tasks, project_task_id)
            
            # æå–æœåŠ¡ä¿¡æ¯
            services = list(set([task.get('service_name', 'æœªçŸ¥æœåŠ¡') for task in tasks if task.get('service_name')]))
            
            return {
                'identified_services': services,
                'service_dependencies': {},  # ç®€åŒ–ç‰ˆæœ¬æš‚ä¸å¤„ç†å¤æ‚ä¾èµ–
                'task_execution_plan': {'total_tasks': len(tasks)},
                'parallel_tasks': [{'batch_id': 'batch_1', 'services': services, 'dependencies': []}],
                'generated_tasks': tasks,
                'current_phase': 'intelligent_coding'
            }
            
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡æ‹†åˆ†å¤±è´¥: {e}")
            return self._empty_result()
    
    def _empty_result(self):
        """è¿”å›ç©ºç»“æœ"""
        return {
            'identified_services': [],
            'service_dependencies': {},
            'task_execution_plan': {},
            'parallel_tasks': [],
            'generated_tasks': [],
            'current_phase': 'intelligent_coding'
        }
    
    def _generate_tasks_with_template(self, design_doc: str, project_name: str, project_task_id: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨Jinja2æ¨¡æ¿ç”Ÿæˆä»»åŠ¡"""
        try:
            from jinja2 import Environment, FileSystemLoader
            
            # è®¾ç½®æ¨¡æ¿ç›®å½•
            template_dir = os.path.join(os.path.dirname(__file__), "..", "prompts")
            env = Environment(loader=FileSystemLoader(template_dir))
            
            # åŠ è½½æ¨¡æ¿
            template = env.get_template("task_splitting/generate_sqlite_tasks_prompts.jinja2")
            
            # æ¸²æŸ“æ¨¡æ¿
            base_project_path = f"/Users/renyu/Documents/create_project/{project_name}"
            prompt = template.render(
                design_doc=design_doc,
                services_summary="åŸºäºè®¾è®¡æ–‡æ¡£çš„æœåŠ¡æ‹†åˆ†",
                base_project_path=base_project_path,
                project_task_id=project_task_id
            )
            
            logger.info("ğŸ¯ ä½¿ç”¨Jinja2æ¨¡æ¿ç”Ÿæˆä»»åŠ¡...")
            logger.info(f"ğŸ“ æ¸²æŸ“åçš„æç¤ºè¯é•¿åº¦: {len(prompt)}")
            logger.info(f"ğŸ“„ æç¤ºè¯å†…å®¹:\n{prompt[:2000]}...")  # æ˜¾ç¤ºå‰2000å­—ç¬¦ä»¥ä¾¿è°ƒè¯•
            
            # è°ƒç”¨LLM
            if not self.llm_client:
                logger.error("âŒ LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
                return []
            
            response = self.llm_client.chat(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä»»åŠ¡ç®¡ç†ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿æ ¼å¼ç”Ÿæˆä»»åŠ¡ï¼Œç¡®ä¿åŒ…å«project_task_idå­—æ®µã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1
            )
            
            logger.info(f"âœ… LLMå“åº”æ¥æ”¶å®Œæˆï¼Œé•¿åº¦: {len(response)}")
            logger.info(f"ğŸ“„ LLMå®Œæ•´å“åº”å†…å®¹:\n{response}")  # æ˜¾ç¤ºå®Œæ•´å“åº”ä»¥ä¾¿è°ƒè¯•
            
            # è§£æJSONå“åº”
            json_str = self._extract_json_from_response(response)
            if not json_str:
                logger.error("âŒ æ— æ³•ä»å“åº”ä¸­æå–JSONå†…å®¹")
                logger.debug(f"åŸå§‹å“åº”: {response[:1000]}...")
                return []
            
            # è§£æJSONå­—ç¬¦ä¸²ä¸ºå­—å…¸
            try:
                task_data = json.loads(json_str)
                logger.info(f"âœ… JSONè§£ææˆåŠŸï¼Œæ•°æ®ç±»å‹: {type(task_data)}")
            except json.JSONDecodeError as e:
                logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                logger.error(f"JSONå†…å®¹: {json_str[:500]}...")
                return []
            
            tasks = task_data.get('tasks', [])
            logger.info(f"ğŸ“‹ ä»JSONä¸­æå–çš„ä»»åŠ¡æ•°é‡: {len(tasks)}")
            
            if not tasks:
                logger.warning("âš ï¸ JSONä¸­æ²¡æœ‰taskså­—æ®µæˆ–tasksä¸ºç©º")
                logger.debug(f"task_dataå†…å®¹: {task_data}")
                return []
            
            # ğŸ†• ä¸ºæ¯ä¸ªä»»åŠ¡æ·»åŠ project_task_id
            if project_task_id:
                for task in tasks:
                    task['project_task_id'] = project_task_id
                logger.info(f"âœ… å·²ä¸º {len(tasks)} ä¸ªä»»åŠ¡æ·»åŠ é¡¹ç›®æ ‡è¯†: {project_task_id}")
            
            return tasks
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ¿ç”Ÿæˆä»»åŠ¡å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []
    
    def _llm_parse_design_document(self, design_doc: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMæ™ºèƒ½è§£æè®¾è®¡æ–‡æ¡£"""
        logger.info("ğŸ¤– ä½¿ç”¨LLMæ™ºèƒ½è§£æè®¾è®¡æ–‡æ¡£...")
        
        # ä½¿ç”¨å¢å¼ºç‰ˆJinja2é£æ ¼çš„ç»“æ„åŒ–æç¤ºè¯æ¨¡æ¿ï¼ˆåŸºäºä½ çš„ä¿®æ”¹ä¼˜åŒ–ï¼‰
        prompt = f"""ä½ æ˜¯ä»»åŠ¡ç®¡ç†ä¸“å®¶ï¼ŒåŸºäºè®¾è®¡æ–‡æ¡£ç”Ÿæˆå…·ä½“çš„å¼€å‘ä»»åŠ¡ä¿¡æ¯ã€‚

è®¾è®¡æ–‡æ¡£å†…å®¹ï¼š
{design_doc}

**é‡è¦çº¦æŸï¼šä¸¥æ ¼ä½¿ç”¨è®¾è®¡æ–‡æ¡£ä¸­çš„çœŸå®å†…å®¹ï¼Œç¦æ­¢ç¼–é€ ã€‚**

**è®¾è®¡æ–‡æ¡£å…³é”®ä¿¡æ¯å‚è€ƒï¼ˆJinja2æ¨¡æ¿é£æ ¼ï¼‰ï¼š**
- GitLabä»“åº“ï¼šç”¨æˆ·æœåŠ¡ http://localhost:30000/ls/zqyl-user-center-service.gitï¼Œç¡®æƒå¼€ç«‹æœåŠ¡ http://localhost:30000/ls/crcl-open.git
- æ¥å£è·¯å¾„ï¼š/general/multiorgManage/queryCompanyUnitListï¼Œ/crcl-open-api/lsLimit/listUnitLimitByCompanyIdï¼Œ/crcl-open-api/lsLimit/listUnitLimitByCompanyIdExport
- è¡¨åï¼št_cust_multiorg_unit
- æ•°æ®åº“ï¼šjdbc:mysql://localhost:6446/dbwebappdbï¼Œç”¨æˆ·åå¯†ç ï¼šdbwebapp/dbwebapp

è¯·ä»¥JSONæ ¼å¼è¿”å›ä»¥ä¸‹ä¿¡æ¯ï¼š

{{
    "services": [
        {{
            "name": "æœåŠ¡åç§°ï¼ˆå¦‚ï¼šç”¨æˆ·æœåŠ¡ã€ç¡®æƒå¼€ç«‹æœåŠ¡ï¼‰",
            "description": "æœåŠ¡åŠŸèƒ½æè¿°",
            "git_repository": "Gitä»“åº“å®Œæ•´åœ°å€",
            "controller_name": "ä¸»è¦æ§åˆ¶å™¨åç§°",
            "repo_name": "ä»“åº“ç›®å½•å"
        }}
    ],
    "git_repositories": [
        "http://localhost:30000/ls/zqyl-user-center-service.git",
        "http://localhost:30000/ls/crcl-open.git"
    ],
    "apis": [
        {{
            "path": "å®Œæ•´APIè·¯å¾„",
            "method": "HTTPæ–¹æ³•ï¼ˆGET/POSTï¼‰",
            "service": "æ‰€å±æœåŠ¡åç§°",
            "description": "æ¥å£åŠŸèƒ½æè¿°",
            "request_params": {{
                "å‚æ•°å1": "å‚æ•°è¯´æ˜(å¿…å¡«/å¯é€‰)",
                "å‚æ•°å2": "å‚æ•°è¯´æ˜(å¿…å¡«/å¯é€‰)"
            }},
            "response_params": {{
                "å­—æ®µå1": "å­—æ®µè¯´æ˜",
                "å­—æ®µå2": "å­—æ®µè¯´æ˜"
            }},
            "business_logic": "è¯¦ç»†çš„ä¸šåŠ¡é€»è¾‘æè¿°",
            "data_source": "æ•°æ®æ¥æºå’Œè·å–æ–¹å¼",
            "content_type": "æ•°æ®æ ¼å¼ï¼ˆapplication/jsonæˆ–å…¶ä»–ï¼‰",
            "validation_rules": {{
                "å‚æ•°å1": "æ ¡éªŒè§„åˆ™æè¿°",
                "å‚æ•°å2": "æ ¡éªŒè§„åˆ™æè¿°"
            }},
            "dependencies": ["ä¾èµ–çš„å…¶ä»–æ¥å£è·¯å¾„"]
        }}
    ],
    "databases": [
        {{
            "table_name": "t_cust_multiorg_unit",
            "database_url": "jdbc:mysql://localhost:6446/dbwebappdb",
            "username": "dbwebapp", 
            "password": "dbwebapp",
            "description": "è¡¨åŠŸèƒ½æè¿°",
            "fields": ["ä¸»è¦å­—æ®µåˆ—è¡¨"]
        }}
    ],
    "service_dependencies": {{
        "ç¡®æƒå¼€ç«‹æœåŠ¡": ["ç”¨æˆ·æœåŠ¡"]
    }},
    "technical_stack": ["Spring Boot", "MySQL", "Feign", "MyBatis"],
    "main_features": ["ä¸»è¦åŠŸèƒ½ç‰¹æ€§æè¿°"]
}}

**APIå‚æ•°è¯¦ç»†ç»“æ„ç¤ºä¾‹ï¼ˆæŒ‰ç…§ä½ çš„ä¿®æ”¹è§„èŒƒï¼‰ï¼š**
å¯¹äºå¯¼å‡ºç±»æ¥å£ï¼Œåº”åŒ…å«ï¼š
- project_path: é¡¹ç›®è·¯å¾„
- api_path: APIå®Œæ•´è·¯å¾„  
- http_method: HTTPæ–¹æ³•
- content_type: å†…å®¹ç±»å‹ï¼ˆç‰¹åˆ«æ˜¯Excelå¯¼å‡ºçš„application/vnd.openxmlformats-officedocument.spreadsheetml.sheetï¼‰
- request_params: è¯·æ±‚å‚æ•°è¯¦ç»†è¯´æ˜
- response_params: å“åº”å‚æ•°è¯¦ç»†è¯´æ˜
- business_logic: ä¸šåŠ¡é€»è¾‘æè¿°
- data_source: æ•°æ®æ¥æºè¯´æ˜
- export_format: å¯¼å‡ºæ ¼å¼ï¼ˆå¦‚xlsxï¼‰
- export_headers: å¯¼å‡ºè¡¨å¤´
- validation_rules: å‚æ•°æ ¡éªŒè§„åˆ™

**å¿…é¡»ä¸¥æ ¼æŒ‰ç…§æ–‡æ¡£å†…å®¹æå–ï¼š**
1. çœŸå®çš„GitLabä»“åº“åœ°å€ï¼ˆç«¯å£30000ï¼‰
2. å…·ä½“çš„APIæ¥å£è·¯å¾„ï¼ˆä¸‰ä¸ªæ¥å£ï¼‰
3. æ•°æ®åº“è¿æ¥ä¿¡æ¯ï¼ˆlocalhost:6446ï¼‰
4. è¡¨åt_cust_multiorg_unit
5. æœåŠ¡é—´è°ƒç”¨å…³ç³»ï¼ˆç¡®æƒå¼€ç«‹æœåŠ¡è°ƒç”¨ç”¨æˆ·æœåŠ¡ï¼‰
6. æœåŠ¡åˆ’åˆ†ä¸¥æ ¼æŒ‰ç…§æ–‡æ¡£å†…å®¹ï¼Œä¸è¦åˆå¹¶æˆ–æ‹†åˆ†æœåŠ¡
7. å…³æ³¨æ¥å£ç‰¹æ®Šè¦æ±‚ä¸­çš„ä¾èµ–å…³ç³»æè¿°

**è¾“å‡ºè¦æ±‚ï¼š**
- åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–è§£é‡Šæ–‡å­—
- ä¸¥æ ¼ä½¿ç”¨æ–‡æ¡£ä¸­çš„çœŸå®URLå’Œè·¯å¾„
- APIsæ•°ç»„ä¸­æ¯ä¸ªæ¥å£éƒ½è¦åŒ…å«å®Œæ•´çš„å‚æ•°ç»“æ„
- ä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„ä¿¡æ¯"""
        
        try:
            response = self.llm_client.chat(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ç³»ç»Ÿæ¶æ„ä¸“å®¶ï¼Œä¸“é—¨åˆ†æè½¯ä»¶è®¾è®¡æ–‡æ¡£ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§Jinja2æ¨¡æ¿é£æ ¼æå–ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=4000  # å¢åŠ tokené™åˆ¶ä»¥æ”¯æŒæ›´è¯¦ç»†çš„APIå‚æ•°ç»“æ„
            )
            
            # æå–JSONå†…å®¹
            json_str = self._extract_json_from_response(response)
            parsed_info = json.loads(json_str)
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼Œå…¼å®¹æ–°çš„è¯¦ç»†ç»“æ„åŒ–è¾“å‡º
            standardized_info = {
                # æœåŠ¡ä¿¡æ¯å¤„ç†ï¼ˆæ”¯æŒæ–°çš„è¯¦ç»†ç»“æ„ï¼‰
                'services': [service['name'] if isinstance(service, dict) else service for service in parsed_info.get('services', [])],
                'git_repositories': parsed_info.get('git_repositories', []),
                
                # APIä¿¡æ¯å¤„ç†ï¼ˆæ”¯æŒæ–°çš„è¯¦ç»†å‚æ•°ç»“æ„ï¼‰
                'apis': [api.get('path', api) if isinstance(api, dict) else api for api in parsed_info.get('apis', [])], 
                
                # æ•°æ®åº“ä¿¡æ¯å¤„ç†ï¼ˆæ”¯æŒæ–°çš„è¯¦ç»†ç»“æ„ï¼‰
                'databases': [db.get('table_name', db) if isinstance(db, dict) else db for db in parsed_info.get('databases', [])],
                
                'dependencies': parsed_info.get('service_dependencies', {}),
                'technical_stack': parsed_info.get('technical_stack', []),
                'main_features': parsed_info.get('main_features', []),
                
                # ä¿å­˜è¯¦ç»†ä¿¡æ¯ç”¨äºåç»­ä»»åŠ¡ç”Ÿæˆ
                'detailed_services': parsed_info.get('services', []),
                'detailed_apis': parsed_info.get('apis', []),
                'detailed_databases': parsed_info.get('databases', [])
            }
            
            logger.info(f"âœ… ä¼˜åŒ–ç‰ˆLLMè§£æå®Œæˆ - æœåŠ¡: {len(standardized_info.get('services', []))}, API: {len(standardized_info.get('apis', []))}")
            logger.info(f"ğŸ“‹ è¯†åˆ«çš„æœåŠ¡: {standardized_info.get('services', [])}")
            logger.info(f"ğŸ”— Gitä»“åº“: {len(standardized_info.get('git_repositories', []))}ä¸ª")
            logger.info(f"ğŸŒ APIè¯¦æƒ…: {len(standardized_info.get('detailed_apis', []))}ä¸ªæ¥å£åŒ…å«å®Œæ•´å‚æ•°")
            
            return standardized_info
            
        except Exception as e:
            logger.error(f"âŒ ä¼˜åŒ–ç‰ˆLLMè§£æå¤±è´¥: {e}")
            # å›é€€åˆ°è§„åˆ™åŒ–è§£æ
            return self._rule_parse_design_document(design_doc)
    
    def _extract_json_from_response(self, response: str) -> str:
        """ä»LLMå“åº”ä¸­æå–JSONå†…å®¹"""
        # æŸ¥æ‰¾JSONä»£ç å—
        json_patterns = [
            r'```json\s*(\{.*?\})\s*```',
            r'```\s*(\{.*?\})\s*```',
            r'(\{.*?\})'
        ]
        
        for pattern in json_patterns:
            match = re.search(pattern, response, re.DOTALL)
            if match:
                return match.group(1).strip()
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œè¿”å›æ•´ä¸ªå“åº”
        return response.strip()
    
    def _rule_parse_design_document(self, design_doc: str) -> Dict[str, Any]:
        """è§„åˆ™åŒ–è§£æè®¾è®¡æ–‡æ¡£ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        logger.info("ğŸ“‹ ä½¿ç”¨è§„åˆ™åŒ–æ–¹å¼è§£æè®¾è®¡æ–‡æ¡£...")
        
        parsed_info = {
            'services': [],
            'git_repositories': [],
            'apis': [],
            'databases': [],
            'dependencies': {}
        }
        
        # ğŸ”§ ä¼˜åŒ–æœåŠ¡åç§°æå–
        service_patterns = [
            r'(ç”¨æˆ·æœåŠ¡|ç¡®æƒå¼€ç«‹æœåŠ¡)',  # ç›´æ¥åŒ¹é…æ ¸å¿ƒæœåŠ¡
            r'æœåŠ¡[:ï¼š]\s*(zqyl-user-center-service|crcl-open)',  # æ˜ç¡®çš„æœåŠ¡å
        ]
        
        found_services = set()
        for pattern in service_patterns:
            matches = re.findall(pattern, design_doc, re.IGNORECASE)
            for match in matches:
                service_name = match if isinstance(match, str) else match[-1]
                if service_name and len(service_name) > 2:
                    found_services.add(service_name.strip())
        
        # æ ‡å‡†åŒ–æœåŠ¡åç§°
        standardized_services = []
        for service in found_services:
            if 'user' in service.lower() or 'ç”¨æˆ·' in service:
                if 'ç”¨æˆ·æœåŠ¡' not in standardized_services:
                    standardized_services.append('ç”¨æˆ·æœåŠ¡')
            elif 'crcl' in service.lower() or 'ç¡®æƒ' in service or 'å¼€ç«‹' in service:
                if 'ç¡®æƒå¼€ç«‹æœåŠ¡' not in standardized_services:
                    standardized_services.append('ç¡®æƒå¼€ç«‹æœåŠ¡')
        
        # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°æ ‡å‡†æœåŠ¡ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if not standardized_services:
            standardized_services = ['ç”¨æˆ·æœåŠ¡', 'ç¡®æƒå¼€ç«‹æœåŠ¡']
        
        parsed_info['services'] = standardized_services
        
        # æå–Gitä»“åº“åœ°å€
        git_patterns = [
            r'http[s]?://[^\s]+\.git',
            r'git@[^\s]+\.git',
        ]
        
        for pattern in git_patterns:
            matches = re.findall(pattern, design_doc)
            for match in matches:
                if match and match not in parsed_info['git_repositories']:
                    parsed_info['git_repositories'].append(match)
        
        # æå–APIæ¥å£
        api_patterns = [
            r'GET\s+(/[^\s\[\]]+)',
            r'POST\s+(/[^\s\[\]]+)',
            r'æ¥å£[:ï¼š]\s*(/[^\s\[\]]+)',
            r'(/[\w\-]+/[\w\-]+/[\w\-]+)',
        ]
        
        found_apis = set()
        for pattern in api_patterns:
            matches = re.findall(pattern, design_doc)
            for match in matches:
                if match and match.startswith('/') and len(match) > 5:
                    found_apis.add(match)
        
        parsed_info['apis'] = list(found_apis)
        
        logger.info(f"âœ… è§„åˆ™è§£æå®Œæˆ - æœåŠ¡: {len(parsed_info['services'])}, API: {len(parsed_info['apis'])}")
        return parsed_info
    
    def _generate_task_sequence(self, parsed_info: Dict[str, Any], project_name: str) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä»»åŠ¡åºåˆ—"""
        logger.info("ğŸ”„ å¼€å§‹ç”Ÿæˆä»»åŠ¡åºåˆ—...")
        
        tasks = []
        current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        base_path = f"/Users/renyu/Documents/create_project/{project_name}"
        
        services = parsed_info.get('services', ['ç”¨æˆ·æœåŠ¡', 'ç¡®æƒå¼€ç«‹æœåŠ¡'])
        if not services:
            services = ['ç”¨æˆ·æœåŠ¡', 'ç¡®æƒå¼€ç«‹æœåŠ¡']  # é»˜è®¤æœåŠ¡
        
        task_counter = 1
        
        # 1. git_extraction - ç³»ç»Ÿçº§ä»»åŠ¡
        tasks.append({
            'task_id': f'task_{task_counter:03d}',
            'service_name': 'ç³»ç»Ÿ',
            'task_type': 'git_extraction',
            'priority': task_counter,
            'status': 'pending',
            'dependencies': '[]',
            'estimated_duration': '5åˆ†é’Ÿ',
            'description': 'ä»è®¾è®¡æ–‡æ¡£æå–GitLabä»“åº“åœ°å€',
            'deliverables': json.dumps(['GitLabä»“åº“åœ°å€æ¸…å•', 'ä»“åº“è®¿é—®éªŒè¯æŠ¥å‘Š'], ensure_ascii=False),
            'implementation_details': f'è§£æè®¾è®¡æ–‡æ¡£ï¼Œæå–ä»“åº“åœ°å€: {", ".join(parsed_info.get("git_repositories", []))}',
            'completion_criteria': 'æˆåŠŸæå–å¹¶éªŒè¯GitLabä»“åº“åœ°å€å¯è®¿é—®æ€§',
            'parameters': json.dumps({
                'repositories': parsed_info.get('git_repositories', []),
                'extraction_method': 'llm_enhanced' if self.llm_client else 'regex_pattern'
            }, ensure_ascii=False),
            'created_at': current_time,
            'updated_at': current_time
        })
        task_counter += 1
        
        # 2. git_clone - ä¸ºæ¯ä¸ªæœåŠ¡ç”Ÿæˆ
        for service in services:
            service_repo = self._get_service_repo_name(service)
            tasks.append({
                'task_id': f'task_{task_counter:03d}',
                'service_name': service,
                'task_type': 'git_clone',
                'priority': task_counter,
                'status': 'pending',
                'dependencies': '["task_001"]',
                'estimated_duration': '10åˆ†é’Ÿ',
                'description': f'ä¸‹è½½{service}ä»£ç åˆ°{base_path}/{service_repo}',
                'deliverables': json.dumps([f'{service}æºç ç›®å½•'], ensure_ascii=False),
                'implementation_details': f'ä½¿ç”¨git cloneå‘½ä»¤ä¸‹è½½{service}ä»£ç åˆ°æŒ‡å®šç›®å½•ï¼Œç¡®ä¿é¡¹ç›®ç»“æ„å®Œæ•´å¯ç¼–è¯‘',
                'completion_criteria': 'ä»£ç ä¸‹è½½å®Œæˆï¼Œé¡¹ç›®ç›®å½•å­˜åœ¨ä¸”åŒ…å«pom.xmlæ–‡ä»¶',
                'parameters': json.dumps({
                    'repo_name': service_repo,
                    'target_path': f'{base_path}/{service_repo}',
                    'branch': 'master'
                }, ensure_ascii=False),
                'created_at': current_time,
                'updated_at': current_time
            })
            task_counter += 1
        
        # 3. code_analysis - ä¸ºæ¯ä¸ªæœåŠ¡ç”Ÿæˆ
        clone_dependencies = [f'task_{i:03d}' for i in range(2, 2 + len(services))]
        for i, service in enumerate(services):
            service_repo = self._get_service_repo_name(service)
            controller_name = self._get_controller_name(service)
            
            tasks.append({
                'task_id': f'task_{task_counter:03d}',
                'service_name': service,
                'task_type': 'code_analysis',
                'priority': task_counter,
                'status': 'pending',
                'dependencies': json.dumps([clone_dependencies[i]]),
                'estimated_duration': '20åˆ†é’Ÿ',
                'description': f'åˆ†æ{service}ä»£ç ç»“æ„ï¼Œç¡®å®š{controller_name}æ·»åŠ ä½ç½®',
                'deliverables': json.dumps(['ä»£ç ç»“æ„ä»Controllerå±‚å¼€å§‹åˆ°æ•°æ®è·å–çš„å®Œæ•´è·¯å¾„'], ensure_ascii=False),
                'implementation_details': f'åˆ†æé¡¹ç›®packageç»“æ„å’Œcontrollerå±‚ï¼Œæ‰¾åˆ°æˆ–åˆ›å»º{controller_name}ç±»çš„æœ€ä½³ä½ç½®',
                'completion_criteria': f'ç¡®å®š{controller_name}çš„åŒ…è·¯å¾„å’Œæ–‡ä»¶ä½ç½®ï¼Œæ˜ç¡®ä¾èµ–çš„Serviceå±‚ç»“æ„',
                'parameters': json.dumps({
                    'project_path': f'{base_path}/{service_repo}',
                    'target_controller': controller_name,
                    'analysis_scope': 'controller,service,mapper'
                }, ensure_ascii=False),
                'created_at': current_time,
                'updated_at': current_time
            })
            task_counter += 1
        
        # 4. config - ä¸ºæ¯ä¸ªæœåŠ¡ç”Ÿæˆæ•°æ®åº“é…ç½®
        analysis_dependencies = [f'task_{i:03d}' for i in range(2 + len(services), 2 + 2*len(services))]
        for i, service in enumerate(services):
            tasks.append({
                'task_id': f'task_{task_counter:03d}',
                'service_name': service,
                'task_type': 'config',
                'priority': task_counter,
                'status': 'pending',
                'dependencies': json.dumps([analysis_dependencies[i]]),
                'estimated_duration': '15åˆ†é’Ÿ',
                'description': f'é…ç½®{service}æ•°æ®åº“è¿æ¥ï¼šjdbc:mysql://localhost:6446/dbwebappdb',
                'deliverables': json.dumps(['æ•°æ®åº“é…ç½®æ–‡ä»¶', 'è¿æ¥æµ‹è¯•æŠ¥å‘Š'], ensure_ascii=False),
                'implementation_details': 'ä¿®æ”¹application.ymlæ–‡ä»¶ï¼Œé…ç½®æ•°æ®åº“è¿æ¥ä¿¡æ¯åŒ…æ‹¬URLã€ç”¨æˆ·åå¯†ç å’Œè¿æ¥æ± è®¾ç½®',
                'completion_criteria': 'application.ymlæ–‡ä»¶å·²æ›´æ–°ï¼Œæ•°æ®åº“è¿æ¥é…ç½®æ­£ç¡®ä¸”å¯è¿é€š',
                'parameters': json.dumps({
                    'database_url': 'jdbc:mysql://localhost:6446/dbwebappdb',
                    'username': 'dbwebapp',
                    'password': 'dbwebapp'
                }, ensure_ascii=False),
                'created_at': current_time,
                'updated_at': current_time
            })
            task_counter += 1
        
        # 5. database - å»ºè¡¨ä»»åŠ¡
        config_dependencies = [f'task_{i:03d}' for i in range(2 + 2*len(services), 2 + 3*len(services))]
        for i, service in enumerate(services):
            table_name = 't_cust_multiorg_unit' if 'ç”¨æˆ·' in service else 't_limit_info'
            
            tasks.append({
                'task_id': f'task_{task_counter:03d}',
                'service_name': service,
                'task_type': 'database',
                'priority': task_counter,
                'status': 'pending',
                'dependencies': json.dumps([config_dependencies[i]]),
                'estimated_duration': '15åˆ†é’Ÿ',
                'description': f'åˆ›å»º{table_name}è¡¨ï¼ŒåŒ…å«ä¸šåŠ¡å­—æ®µå’Œç´¢å¼•',
                'deliverables': json.dumps(['å»ºè¡¨SQLè„šæœ¬', 'Entityå®ä½“ç±»', 'Mapperæ¥å£æ–‡ä»¶'], ensure_ascii=False),
                'implementation_details': 'ç¼–å†™å»ºè¡¨SQLè„šæœ¬ï¼Œå®šä¹‰ä¸»é”®ã€ç´¢å¼•å’Œå­—æ®µçº¦æŸï¼Œåˆ›å»ºå¯¹åº”çš„Entityå’ŒMapperæ–‡ä»¶',
                'completion_criteria': 'è¡¨ç»“æ„åˆ›å»ºæˆåŠŸï¼ŒEntityå’ŒMapperæ–‡ä»¶å¯æ­£å¸¸ç¼–è¯‘ï¼Œæ”¯æŒåŸºæœ¬CRUDæ“ä½œ',
                'parameters': json.dumps({
                    'table_name': table_name,
                    'sql_location': f'src/main/resources/sql/{table_name}.sql'
                }, ensure_ascii=False),
                'created_at': current_time,
                'updated_at': current_time
            })
            task_counter += 1
        
        # 6. api - æ¥å£å¼€å‘ä»»åŠ¡
        database_dependencies = [f'task_{i:03d}' for i in range(2 + 3*len(services), 2 + 4*len(services))]
        apis = parsed_info.get('apis', [])
        if not apis:
            apis = ['/general/multiorgManage/queryCompanyUnitList', 
                   '/crcl-open-api/lsLimit/listUnitLimitByCompanyId',
                   '/crcl-open-api/lsLimit/listUnitLimitByCompanyIdExport']
        
        for i, api in enumerate(apis):
            service = services[i % len(services)]
            dep_task = database_dependencies[i % len(database_dependencies)]
            
            tasks.append({
                'task_id': f'task_{task_counter:03d}',
                'service_name': service,
                'task_type': 'api',
                'priority': task_counter,
                'status': 'pending',
                'dependencies': json.dumps([dep_task]),
                'estimated_duration': '60åˆ†é’Ÿ',
                'description': f'å®ç°GET {api}æ¥å£',
                'deliverables': json.dumps(['Controllerç±»', 'Serviceä¸šåŠ¡é€»è¾‘å±‚', 'æ¥å£æ–‡æ¡£'], ensure_ascii=False),
                'implementation_details': f'åœ¨Controllerä¸­å®ç°{api}æ¥å£ï¼Œæ”¯æŒæ¡ä»¶æŸ¥è¯¢å’Œåˆ†é¡µï¼Œè¿”å›è§„å®šçš„å­—æ®µæ ¼å¼',
                'completion_criteria': 'æ¥å£å¯æ­£å¸¸è®¿é—®ï¼Œè¿”å›æ•°æ®æ ¼å¼ç¬¦åˆè§„èŒƒï¼Œæ”¯æŒæ¡ä»¶æŸ¥è¯¢å’Œåˆ†é¡µåŠŸèƒ½',
                'parameters': json.dumps({
                    'api_path': api,
                    'http_method': 'GET',
                    'content_type': 'application/json'
                }, ensure_ascii=False),
                'created_at': current_time,
                'updated_at': current_time
            })
            task_counter += 1
        
        # 7. integration_test - é›†æˆæµ‹è¯•
        api_dependencies = [f'task_{i:03d}' for i in range(2 + 4*len(services), task_counter)]
        tasks.append({
            'task_id': f'task_{task_counter:03d}',
            'service_name': 'ç³»ç»Ÿ',
            'task_type': 'integration_test',
            'priority': task_counter,
            'status': 'pending',
            'dependencies': json.dumps(api_dependencies),
            'estimated_duration': '30åˆ†é’Ÿ',
            'description': 'æµ‹è¯•æœåŠ¡é—´è°ƒç”¨çš„å®Œæ•´æµç¨‹',
            'deliverables': json.dumps(['é›†æˆæµ‹è¯•ç”¨ä¾‹', 'æµ‹è¯•æ•°æ®è„šæœ¬', 'æµ‹è¯•æŠ¥å‘Š'], ensure_ascii=False),
            'implementation_details': 'ç¼–å†™é›†æˆæµ‹è¯•ç”¨ä¾‹ï¼ŒéªŒè¯æœåŠ¡é—´è°ƒç”¨æ­£ç¡®æ€§å’Œæ•°æ®å®Œæ•´æ€§',
            'completion_criteria': 'æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡ï¼Œæ¥å£è°ƒç”¨é“¾è·¯æ­£å¸¸ï¼Œæ•°æ®è¿”å›æ ¼å¼æ­£ç¡®',
            'parameters': json.dumps({
                'test_scenarios': ['ç»„ç»‡å•å…ƒé¢åº¦æŸ¥è¯¢æµ‹è¯•'],
                'test_data': {'gwCompanyId': 1, 'unitName': 'æµ‹è¯•å•å…ƒ', 'bizType': 80}
            }, ensure_ascii=False),
            'created_at': current_time,
            'updated_at': current_time
        })
        task_counter += 1
        
        # 8. deployment - ä»£ç æäº¤
        tasks.append({
            'task_id': f'task_{task_counter:03d}',
            'service_name': 'ç³»ç»Ÿ',
            'task_type': 'deployment',
            'priority': task_counter,
            'status': 'pending',
            'dependencies': json.dumps([f'task_{task_counter-1:03d}']),
            'estimated_duration': '20åˆ†é’Ÿ',
            'description': 'æäº¤ä»£ç åˆ°GitLabä»“åº“ï¼Œcommit message: feat: æ–°å¢ç»„ç»‡å•å…ƒé¢åº¦ç®¡ç†åŠŸèƒ½',
            'deliverables': json.dumps(['Gitæäº¤è®°å½•', 'ä»£ç åˆå¹¶æŠ¥å‘Š', 'éƒ¨ç½²æ–‡æ¡£'], ensure_ascii=False),
            'implementation_details': 'æ‰§è¡Œgit addã€git commitå’Œgit pushå‘½ä»¤ï¼Œæäº¤æ‰€æœ‰æ–°å¢å’Œä¿®æ”¹çš„ä»£ç æ–‡ä»¶',
            'completion_criteria': 'ä»£ç æˆåŠŸæäº¤åˆ°GitLabä»“åº“ï¼Œcommitä¿¡æ¯æ¸…æ™°ï¼Œæ— å†²çª',
            'parameters': json.dumps({
                'repositories': [{'path': f'{base_path}/{self._get_service_repo_name(s)}', 'changes': f'æ–°å¢{s}ç›¸å…³åŠŸèƒ½'} for s in services]
            }, ensure_ascii=False),
            'created_at': current_time,
            'updated_at': current_time
        })
        
        logger.info(f"âœ… ç”Ÿæˆä»»åŠ¡åºåˆ—å®Œæˆï¼Œå…± {len(tasks)} ä¸ªä»»åŠ¡")
        return tasks
    
    def _get_service_repo_name(self, service_name: str) -> str:
        """æ ¹æ®æœåŠ¡åç§°è·å–ä»“åº“å"""
        if 'ç”¨æˆ·' in service_name:
            return 'zqyl-user-center-service'
        elif 'ç¡®æƒ' in service_name or 'å¼€ç«‹' in service_name:
            return 'crcl-open'
        else:
            return service_name.replace('æœåŠ¡', '-service').lower()
    
    def _get_controller_name(self, service_name: str) -> str:
        """æ ¹æ®æœåŠ¡åç§°è·å–Controllerå"""
        if 'ç”¨æˆ·' in service_name:
            return 'MultiorgManageController'
        elif 'ç¡®æƒ' in service_name or 'å¼€ç«‹' in service_name:
            return 'LsLimitController'
        else:
            return f'{service_name.replace("æœåŠ¡", "")}Controller'
    
    def _generate_sql_content(self, tasks: List[Dict[str, Any]]) -> str:
        """ç”ŸæˆSQLæ’å…¥è¯­å¥"""
        logger.info("ğŸ”„ ç”ŸæˆSQLå†…å®¹...")
        
        sql_lines = []
        sql_lines.append("INSERT INTO execution_tasks (task_id,service_name,task_type,priority,status,dependencies,estimated_duration,description,deliverables,implementation_details,completion_criteria,parameters,created_at,updated_at) VALUES")
        
        task_values = []
        for task in tasks:
            # è½¬ä¹‰SQLå­—ç¬¦ä¸²
            def escape_sql_string(s):
                if s is None:
                    return 'NULL'
                return "'" + str(s).replace("'", "''").replace("\\", "\\\\") + "'"
            
            values = [
                escape_sql_string(task['task_id']),
                escape_sql_string(task['service_name']),
                escape_sql_string(task['task_type']),
                str(task['priority']),
                escape_sql_string(task['status']),
                escape_sql_string(task['dependencies']),
                escape_sql_string(task['estimated_duration']),
                escape_sql_string(task['description']),
                escape_sql_string(task['deliverables']),
                escape_sql_string(task['implementation_details']),
                escape_sql_string(task['completion_criteria']),
                escape_sql_string(task['parameters']),
                escape_sql_string(task['created_at']),
                escape_sql_string(task['updated_at'])
            ]
            
            task_values.append(f"\t ({','.join(values)})")
        
        sql_lines.append(',\n'.join(task_values) + ';')
        
        return '\n'.join(sql_lines)
    
    def _save_sql_file(self, sql_content: str, project_name: str) -> str:
        """ä¿å­˜SQLæ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y%m%d%H%M')
        filename = f"execution_tasks_{timestamp}.sql"
        filepath = f"/Users/renyu/{filename}"
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(sql_content)
            logger.info(f"âœ… SQLæ–‡ä»¶ä¿å­˜æˆåŠŸ: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"âŒ SQLæ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            return ""
    
    def _save_to_database(self, tasks: List[Dict[str, Any]], project_task_id: str):
        """ä¿å­˜ä»»åŠ¡åˆ°SQLiteæ•°æ®åº“"""
        try:
            from src.corder_integration.langgraph.task_manager import NodeTaskManager
            task_manager = NodeTaskManager()
            
            logger.info(f"ğŸ’¾ å¼€å§‹ä¿å­˜ {len(tasks)} ä¸ªä»»åŠ¡åˆ°æ•°æ®åº“...")
            
            # ğŸ†• åˆ é™¤ç›¸åŒproject_task_idçš„æ—§ä»»åŠ¡
            if project_task_id:
                self._delete_old_tasks(task_manager, project_task_id)
            
            for i, task in enumerate(tasks):
                # ä½¿ç”¨æ–°çš„ä»»åŠ¡ç®¡ç†å™¨ä¿å­˜æ ¼å¼
                task_data = {
                    'task_id': task.get('task_id'),
                    'project_task_id': task.get('project_task_id'),  # ğŸ†• é¡¹ç›®å”¯ä¸€æ ‡è¯†
                    'service_name': task.get('service_name', 'ç³»ç»Ÿ'),
                    'task_type': task.get('task_type', 'api'),
                    'priority': task.get('priority', 1),
                    'dependencies': task.get('dependencies', []),
                    'estimated_duration': task.get('estimated_duration', '30åˆ†é’Ÿ'),
                    'description': task.get('description', ''),
                    'deliverables': task.get('deliverables', []),
                    'implementation_details': task.get('implementation_details', ''),
                    'completion_criteria': task.get('completion_criteria', ''),
                    'parameters': task.get('parameters', {}),
                    'status': 'pending'
                }
                
                logger.debug(f"ä¿å­˜ä»»åŠ¡ {i+1}/{len(tasks)}: {task_data['task_id']} - {task_data['description'][:50]}...")
                
                # ä½¿ç”¨æ–°çš„ä¿å­˜æ–¹æ³•
                success = task_manager._save_single_task(task_data)
                if not success:
                    logger.warning(f"âš ï¸ ä»»åŠ¡ {task_data['task_id']} ä¿å­˜å¤±è´¥")
                else:
                    logger.debug(f"âœ… ä»»åŠ¡ {task_data['task_id']} ä¿å­˜æˆåŠŸ")
            
            logger.info(f"âœ… ä»»åŠ¡å·²ä¿å­˜åˆ°æ•°æ®åº“ï¼Œå…± {len(tasks)} ä¸ª")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸å·¥ä½œæµç»§ç»­
            logger.warning(f"âš ï¸ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥ï¼Œä½†å·¥ä½œæµå°†ç»§ç»­æ‰§è¡Œ")
    
    def _delete_old_tasks(self, task_manager, project_task_id: str):
        """è½¯åˆ é™¤ç›¸åŒproject_task_idçš„æ—§ä»»åŠ¡ï¼Œå°†çŠ¶æ€æ”¹ä¸ºå·²è¿‡æœŸ"""
        try:
            logger.info(f"ğŸ—‚ï¸ æ ‡è®°é¡¹ç›® {project_task_id} çš„æ—§ä»»åŠ¡ä¸ºå·²è¿‡æœŸ...")
            
            # è·å–æ•°æ®åº“è¿æ¥
            with task_manager._get_connection() as conn:
                cursor = conn.cursor()
                
                # å…ˆæŸ¥è¯¢è¦æ ‡è®°ä¸ºè¿‡æœŸçš„ä»»åŠ¡æ•°é‡
                cursor.execute("""
                    SELECT COUNT(*) FROM execution_tasks 
                    WHERE project_task_id = ? AND status != 'expired'
                """, (project_task_id,))
                
                old_count = cursor.fetchone()[0]
                
                if old_count > 0:
                    logger.info(f"ğŸ“‹ å‘ç° {old_count} ä¸ªæ—§ä»»åŠ¡ï¼Œå‡†å¤‡æ ‡è®°ä¸ºå·²è¿‡æœŸ")
                    
                    # å°†è¯¥é¡¹ç›®çš„æ‰€æœ‰éè¿‡æœŸä»»åŠ¡æ ‡è®°ä¸ºå·²è¿‡æœŸ
                    cursor.execute("""
                        UPDATE execution_tasks 
                        SET status = 'expired', 
                            updated_at = datetime('now', 'localtime')
                        WHERE project_task_id = ? AND status != 'expired'
                    """, (project_task_id,))
                    
                    expired_count = cursor.rowcount
                    logger.info(f"âœ… æˆåŠŸæ ‡è®° {expired_count} ä¸ªæ—§ä»»åŠ¡ä¸ºå·²è¿‡æœŸ")
                else:
                    logger.info("ğŸ“‹ æ²¡æœ‰å‘ç°éœ€è¦è¿‡æœŸçš„æ—§ä»»åŠ¡ï¼Œç›´æ¥åˆ›å»ºæ–°ä»»åŠ¡")
                
        except Exception as e:
            logger.error(f"âŒ æ ‡è®°æ—§ä»»åŠ¡ä¸ºè¿‡æœŸå¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸å·¥ä½œæµç»§ç»­


# åˆ›å»ºèŠ‚ç‚¹å®ä¾‹
task_simple_splitting_node_instance = TaskSimpleSplittingNode()

# å¼‚æ­¥èŠ‚ç‚¹å‡½æ•°ä¾›LangGraphä½¿ç”¨
async def task_simple_splitting_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """ç®€åŒ–ç‰ˆä»»åŠ¡æ‹†åˆ†èŠ‚ç‚¹ - LangGraphå¼‚æ­¥æ¥å£"""
    logger.info("ğŸš€ ç®€åŒ–ç‰ˆä»»åŠ¡æ‹†åˆ†èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ...")
    
    try:
        # è°ƒç”¨å®ä¾‹æ–¹æ³•
        result = task_simple_splitting_node_instance(state)
        
        # æ›´æ–°çŠ¶æ€
        state.update(result)
        
        logger.info(f"âœ… ç®€åŒ–ç‰ˆä»»åŠ¡æ‹†åˆ†å®Œæˆï¼Œè¯†åˆ« {len(result.get('identified_services', []))} ä¸ªæœåŠ¡")
        
        return state
        
    except Exception as e:
        logger.error(f"âŒ ç®€åŒ–ç‰ˆä»»åŠ¡æ‹†åˆ†å¤±è´¥: {e}")
        
        # è¿”å›å¤±è´¥çŠ¶æ€ä½†ä¸ä¸­æ–­å·¥ä½œæµ
        state.update({
            'identified_services': [],
            'service_dependencies': {},
            'task_execution_plan': {},
            'parallel_tasks': [],
            'current_phase': 'intelligent_coding'
        })
        
        return state