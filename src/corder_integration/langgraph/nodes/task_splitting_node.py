#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä»»åŠ¡æ‹†åˆ†èŠ‚ç‚¹ - é‡æ„ç‰ˆæœ¬
å®ç°æ»‘åŠ¨çª—å£æœºåˆ¶å’ŒSQLiteä»»åŠ¡å­˜å‚¨
"""

import logging
import json
import sqlite3
import uuid
from datetime import datetime
from typing import Dict, Any, List, Optional
from jinja2 import Environment, FileSystemLoader, TemplateNotFound
import os
import time

# å¯¼å…¥å®¢æˆ·ç«¯
try:
    from src.utils.volcengine_client import get_volcengine_client
    VOLCENGINE_AVAILABLE = True
except ImportError:
    logging.warning("ç«å±±å¼•æ“å®¢æˆ·ç«¯ä¸å¯ç”¨")
    VOLCENGINE_AVAILABLE = False

logger = logging.getLogger(__name__)

class TaskSplittingPrompts:
    """ä»»åŠ¡æ‹†åˆ†æç¤ºè¯ç®¡ç†å™¨ - ç®€åŒ–ç‰ˆæœ¬"""
    
    def __init__(self):
        # è®¾ç½®æ¨¡æ¿ç›®å½•
        template_dir = os.path.join(
            os.path.dirname(__file__), 
            "..", 
            "prompts"
        )
        
        self.jinja_env = Environment(
            loader=FileSystemLoader(template_dir),
            trim_blocks=True,
            lstrip_blocks=True
        )
        
        # æ¨¡æ¿æ–‡ä»¶æ˜ å°„ - å®Œæ•´æ˜ å°„
        self.template_files = {
            "design_analysis": "task_splitting/design_analysis_prompts.jinja2", 
            "service_boundary": "task_splitting/service_boundary_prompts.jinja2",
            "dependency_analysis": "task_splitting/dependency_analysis_prompts.jinja2",
            "task_scheduling": "task_splitting/task_scheduling_prompts.jinja2",
            "generate_sqlite_tasks": "task_splitting/generate_sqlite_tasks_prompts.jinja2"
        }
        
        self.templates = {}
        self._load_templates()
    
    def _load_templates(self):
        """åŠ è½½æ¨¡æ¿æ–‡ä»¶"""
        for prompt_type, template_file in self.template_files.items():
            try:
                template = self.jinja_env.get_template(template_file)
                self.templates[prompt_type] = template
                logger.info(f"âœ… æ¨¡æ¿ {template_file} åŠ è½½æˆåŠŸ")
            except TemplateNotFound:
                logger.warning(f"âš ï¸ æ¨¡æ¿æ–‡ä»¶ {template_file} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨å†…ç½®æ¨¡æ¿")
                self.templates[prompt_type] = None
            except Exception as e:
                logger.error(f"âŒ åŠ è½½æ¨¡æ¿ {template_file} å¤±è´¥: {e}")
                self.templates[prompt_type] = None
    
    def get_prompt(self, prompt_type: str, **kwargs) -> str:
        """è·å–æ¸²æŸ“åçš„æç¤ºè¯"""
        try:
            # å°è¯•ä»æ¨¡æ¿æ–‡ä»¶è·å–
            template = self.templates.get(prompt_type)
            if template:
                # è°ƒç”¨å¯¹åº”çš„å®
                macro_name = f"{prompt_type}_prompt"
                if hasattr(template.module, macro_name):
                    macro = getattr(template.module, macro_name)
                    return macro(**kwargs)
            
            # ä½¿ç”¨å†…ç½®é»˜è®¤æç¤ºè¯
            logger.warning(f"âš ï¸ ä½¿ç”¨å†…ç½®æç¤ºè¯: {prompt_type}")
            return self._get_builtin_prompt(prompt_type, **kwargs)
            
        except Exception as e:
            logger.error(f"âŒ æ¸²æŸ“æç¤ºè¯å¤±è´¥: {e}")
            return self._get_builtin_prompt(prompt_type, **kwargs)
    
    def _get_builtin_prompt(self, prompt_type: str, **kwargs) -> str:
        """å†…ç½®é»˜è®¤æç¤ºè¯"""
        builtin_templates = {
            "design_analysis": """ä½ æ˜¯ç³»ç»Ÿæ¶æ„å¸ˆã€‚åˆ†æè®¾è®¡æ–‡æ¡£ï¼Œè¾“å‡ºJSONæ ¼å¼ç»“æœã€‚  
è®¾è®¡æ–‡æ¡£: {design_doc}
é¡¹ç›®åç§°: {project_name}
è¾“å‡ºæ ¼å¼: {{"architecture_style": "", "functional_modules": [], "summary": ""}}""",
            
            "service_boundary": """ä½ æ˜¯å¾®æœåŠ¡ä¸“å®¶ã€‚è¿›è¡ŒæœåŠ¡æ‹†åˆ†ï¼Œè¾“å‡ºJSONæ ¼å¼ç»“æœã€‚
è®¾è®¡æ¦‚è¦: {design_summary}
æ¶æ„æ¦‚è¦: {architecture_summary}
è¾“å‡ºæ ¼å¼: {{"identified_services": [], "summary": ""}}""",
            
            "dependency_analysis": """ä½ æ˜¯ä¾èµ–åˆ†æä¸“å®¶ã€‚åˆ†ææœåŠ¡ä¾èµ–ï¼Œè¾“å‡ºJSONæ ¼å¼ç»“æœã€‚
æœåŠ¡æ¦‚è¦: {services_summary}
è¾“å‡ºæ ¼å¼: {{"service_dependencies": {{}}, "summary": ""}}""",
            
            "task_scheduling": """ä½ æ˜¯è°ƒåº¦ä¸“å®¶ã€‚åˆ¶å®šæ‰§è¡Œè®¡åˆ’ï¼Œè¾“å‡ºJSONæ ¼å¼ç»“æœã€‚
æœåŠ¡æ¦‚è¦: {services_summary}
ä¾èµ–æ¦‚è¦: {dependencies_summary}
è¾“å‡ºæ ¼å¼: {{"execution_phases": [], "summary": ""}}""",
            
            "generate_sqlite_tasks": """ä½ æ˜¯ä»»åŠ¡ç®¡ç†ä¸“å®¶ã€‚ç”ŸæˆSQLiteä»»åŠ¡ï¼Œè¾“å‡ºJSONæ ¼å¼ç»“æœã€‚
æ‰§è¡Œè®¡åˆ’: {execution_plan}
æœåŠ¡æ¦‚è¦: {services_summary}
åŸºç¡€é¡¹ç›®è·¯å¾„: {base_project_path}

**è·¯å¾„ä¸€è‡´æ€§è¦æ±‚ï¼š**
- git_cloneä»»åŠ¡çš„local_path: {base_project_path}/æœåŠ¡ç›®å½•å
- code_analysisä»»åŠ¡çš„project_path: {base_project_path}/æœåŠ¡ç›®å½•å
- apiä»»åŠ¡çš„project_path: {base_project_path}/æœåŠ¡ç›®å½•å
- configä»»åŠ¡çš„config_file: {base_project_path}/æœåŠ¡ç›®å½•å/é…ç½®æ–‡ä»¶è·¯å¾„
- deploymentä»»åŠ¡çš„path: {base_project_path}/æœåŠ¡ç›®å½•å
- **æ‰€æœ‰è·¯å¾„å­—æ®µå¿…é¡»ä½¿ç”¨å®Œå…¨ç›¸åŒçš„åŸºç¡€è·¯å¾„æ ¼å¼**

è¯·è¾“å‡ºJSONæ ¼å¼çš„ä»»åŠ¡åˆ—è¡¨ï¼š
{{
  "tasks": [
    {{
      "task_id": "task_001",
      "service_name": "æœåŠ¡åç§°",
      "task_type": "git_clone",
      "priority": 1,
      "dependencies": [],
      "estimated_duration": "10åˆ†é’Ÿ",
      "description": "ä»»åŠ¡æè¿°",
      "parameters": {{
        "git_url": "ä»“åº“åœ°å€",
        "local_path": "{base_project_path}/æœåŠ¡ç›®å½•å"
      }}
    }},
    {{
      "task_id": "task_002",
      "service_name": "æœåŠ¡åç§°",
      "task_type": "code_analysis",
      "priority": 2,
      "dependencies": ["task_001"],
      "estimated_duration": "20åˆ†é’Ÿ",
      "description": "ä»£ç åˆ†æä»»åŠ¡æè¿°",
      "parameters": {{
        "project_path": "{base_project_path}/æœåŠ¡ç›®å½•å",
        "target_controller": "ControllerName",
        "target_api": "/api/path"
      }}
    }},
    {{
      "task_id": "task_003",
      "service_name": "æœåŠ¡åç§°",
      "task_type": "api",
      "priority": 3,
      "dependencies": ["task_002"],
      "estimated_duration": "30åˆ†é’Ÿ",
      "description": "APIä»»åŠ¡æè¿°",
      "parameters": {{
        "project_path": "{base_project_path}/æœåŠ¡ç›®å½•å",
        "api_path": "/api/path",
        "http_method": "GET"
      }}
    }}
  ],
  "summary": "ä»»åŠ¡ç”Ÿæˆæ¦‚è¦"
}}"""
        }
        
        template_str = builtin_templates.get(prompt_type, "")
        if template_str:
            return template_str.format(**kwargs)
        return ""

class SlidingWindowManager:
    """æ»‘åŠ¨çª—å£ç®¡ç†å™¨ - æ»‘åŠ¨çª—å£å®ç°"""
    
    def __init__(self, max_window_size: int = 2500, overlap_size: int = 300):
        self.max_window_size = max_window_size
        self.overlap_size = overlap_size  # çª—å£é‡å å¤§å°
        self.context_history = []
        
    def add_context(self, context: str, context_type: str):
        """æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        self.context_history.append({
            "type": context_type,
            "content": context,
            "timestamp": datetime.now()
        })
        
        # ä¿æŒæœ€è¿‘çš„5ä¸ªä¸Šä¸‹æ–‡
        if len(self.context_history) > 5:
            self.context_history = self.context_history[-5:]
    
    def get_context_window(self) -> Optional[str]:
        """è·å–å½“å‰ä¸Šä¸‹æ–‡çª—å£"""
        if not self.context_history:
            return None
        
        # ç»„åˆæœ€è¿‘çš„ä¸Šä¸‹æ–‡
        contexts = []
        for ctx in self.context_history[-3:]:  # æœ€è¿‘3ä¸ªä¸Šä¸‹æ–‡
            contexts.append(f"[{ctx['type']}]: {ctx['content'][:300]}...")
        
        return "\n".join(contexts)
    
    def split_text_into_windows(self, text: str) -> List[Dict[str, Any]]:
        """å°†æ–‡æœ¬åˆ†å‰²æˆæ»‘åŠ¨çª—å£ç‰‡æ®µ"""
        if len(text) <= self.max_window_size:
            return [{
                "content": text,
                "window_id": 1,
                "is_complete": True,
                "start_pos": 0,
                "end_pos": len(text)
            }]
        
        windows = []
        start_pos = 0
        window_id = 1
        
        while start_pos < len(text):
            # è®¡ç®—å½“å‰çª—å£çš„ç»“æŸä½ç½®
            end_pos = min(start_pos + self.max_window_size, len(text))
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªçª—å£ï¼Œå°è¯•åœ¨å¥å·æˆ–æ¢è¡Œç¬¦å¤„åˆ‡åˆ†
            if end_pos < len(text):
                # å‘åæŸ¥æ‰¾åˆé€‚çš„åˆ‡åˆ†ç‚¹ï¼ˆå¥å·ã€æ¢è¡Œç¬¦ã€ä¸­æ–‡å¥å·ï¼‰
                for i in range(end_pos - 100, end_pos):
                    if i > 0 and text[i] in '.ã€‚\n':
                        end_pos = i + 1
                        break
            
            window_content = text[start_pos:end_pos]
            windows.append({
                "content": window_content,
                "window_id": window_id,
                "is_complete": end_pos >= len(text),
                "start_pos": start_pos,
                "end_pos": end_pos,
                "total_windows": None  # ç¨åå¡«å……
            })
            
            # ä¸‹ä¸€ä¸ªçª—å£çš„èµ·å§‹ä½ç½®ï¼ˆæœ‰é‡å ï¼‰
            if end_pos >= len(text):
                break
            start_pos = end_pos - self.overlap_size
            window_id += 1
        
        # å¡«å……æ€»çª—å£æ•°
        for window in windows:
            window["total_windows"] = len(windows)
            
        logger.info(f"ğŸ“„ æ–‡æ¡£åˆ†å‰²å®Œæˆï¼š{len(text)} å­—ç¬¦ -> {len(windows)} ä¸ªçª—å£")
        return windows
    
    def merge_analysis_results(self, results: List[Dict[str, Any]], analysis_type: str) -> Dict[str, Any]:
        """åˆå¹¶å¤šä¸ªçª—å£çš„åˆ†æç»“æœ"""
        if not results:
            return {"summary": f"{analysis_type}åˆ†ææ— ç»“æœ", "merged": True}
        
        if len(results) == 1:
            return results[0]
        
        logger.info(f"ğŸ”„ åˆå¹¶ {len(results)} ä¸ª{analysis_type}åˆ†æç»“æœ...")
        
        # æ ¹æ®åˆ†æç±»å‹è¿›è¡Œä¸åŒçš„åˆå¹¶ç­–ç•¥
        if analysis_type == "è®¾è®¡åˆ†æ":
            return self._merge_design_analysis(results)
        elif analysis_type == "æœåŠ¡æ‹†åˆ†":
            return self._merge_service_boundary(results)
        elif analysis_type == "ä¾èµ–åˆ†æ":
            return self._merge_dependency_analysis(results)
        elif analysis_type == "ä»»åŠ¡è°ƒåº¦":
            return self._merge_task_scheduling(results)
        elif analysis_type == "ä»»åŠ¡ç”Ÿæˆ":
            return self._merge_task_generation(results)
        else:
            return self._merge_generic(results, analysis_type)
    
    def _merge_design_analysis(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶è®¾è®¡åˆ†æç»“æœ"""
        merged = {
            "architecture_style": results[0].get("architecture_style", "å¾®æœåŠ¡"),
            "technology_stack": [],
            "functional_modules": [],
            "system_components": [],
            "data_flow": [],
            "integration_points": [],
            "summary": ""
        }
        
        summaries = []
        for result in results:
            # åˆå¹¶æŠ€æœ¯æ ˆï¼ˆå»é‡ï¼‰
            if result.get("technology_stack"):
                merged["technology_stack"].extend(result["technology_stack"])
            
            # åˆå¹¶åŠŸèƒ½æ¨¡å—
            if result.get("functional_modules"):
                merged["functional_modules"].extend(result["functional_modules"])
            
            # åˆå¹¶ç³»ç»Ÿç»„ä»¶
            if result.get("system_components"):
                merged["system_components"].extend(result["system_components"])
                
            # åˆå¹¶æ•°æ®æµ
            if result.get("data_flow"):
                merged["data_flow"].extend(result["data_flow"])
                
            # åˆå¹¶é›†æˆç‚¹
            if result.get("integration_points"):
                merged["integration_points"].extend(result["integration_points"])
            
            # æ”¶é›†æ¦‚è¦
            if result.get("summary"):
                summaries.append(result["summary"])
        
        # å»é‡
        merged["technology_stack"] = list(set(merged["technology_stack"]))
        merged["system_components"] = list(set(merged["system_components"]))
        merged["data_flow"] = list(set(merged["data_flow"]))
        merged["integration_points"] = list(set(merged["integration_points"]))
        
        # åˆå¹¶æ¦‚è¦
        merged["summary"] = " ".join(summaries)[:200] if summaries else "è®¾è®¡åˆ†æå®Œæˆ"
        
        return merged
    
    def _merge_service_boundary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶æœåŠ¡è¾¹ç•Œåˆ†æç»“æœ"""
        all_services = []
        summaries = []
        
        for result in results:
            if result.get("identified_services"):
                all_services.extend(result["identified_services"])
            if result.get("summary"):
                summaries.append(result["summary"])
        
        # å»é‡æœåŠ¡ï¼ˆåŸºäºnameå­—æ®µï¼‰
        unique_services = []
        seen_names = set()
        for service in all_services:
            name = service.get("name", "")
            if name and name not in seen_names:
                unique_services.append(service)
                seen_names.add(name)
        
        return {
            "identified_services": unique_services,
            "summary": " ".join(summaries)[:200] if summaries else "æœåŠ¡æ‹†åˆ†å®Œæˆ"
        }
    
    def _merge_dependency_analysis(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶ä¾èµ–åˆ†æç»“æœ"""
        merged_dependencies = {}
        summaries = []
        critical_paths = []
        
        for result in results:
            if result.get("service_dependencies"):
                merged_dependencies.update(result["service_dependencies"])
            if result.get("summary"):
                summaries.append(result["summary"])
            if result.get("critical_path"):
                critical_paths.extend(result["critical_path"])
        
        return {
            "service_dependencies": merged_dependencies,
            "critical_path": list(set(critical_paths)),
            "summary": " ".join(summaries)[:200] if summaries else "ä¾èµ–åˆ†æå®Œæˆ"
        }
    
    def _merge_task_scheduling(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶ä»»åŠ¡è°ƒåº¦ç»“æœ"""
        all_phases = []
        all_parallel_groups = []
        execution_orders = []
        summaries = []
        
        for result in results:
            if result.get("execution_phases"):
                all_phases.extend(result["execution_phases"])
            if result.get("parallel_groups"):
                all_parallel_groups.extend(result["parallel_groups"])
            if result.get("execution_order"):
                execution_orders.extend(result["execution_order"])
            if result.get("summary"):
                summaries.append(result["summary"])
        
        return {
            "execution_phases": all_phases,
            "parallel_groups": all_parallel_groups,
            "execution_order": list(set(execution_orders)),
            "summary": " ".join(summaries)[:200] if summaries else "ä»»åŠ¡è°ƒåº¦å®Œæˆ"
        }
    
    def _merge_task_generation(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶ä»»åŠ¡ç”Ÿæˆç»“æœ"""
        all_tasks = []
        summaries = []
        
        for result in results:
            if result.get("tasks"):
                all_tasks.extend(result["tasks"])
            if result.get("summary"):
                summaries.append(result["summary"])
        
        # å»é‡ä»»åŠ¡ï¼ˆåŸºäºtask_idå­—æ®µï¼‰
        unique_tasks = []
        seen_ids = set()
        for task in all_tasks:
            task_id = task.get("task_id", "")
            if task_id and task_id not in seen_ids:
                unique_tasks.append(task)
                seen_ids.add(task_id)
            elif not task_id:
                # ä¸ºæ²¡æœ‰IDçš„ä»»åŠ¡ç”Ÿæˆå”¯ä¸€ID
                import uuid
                task["task_id"] = f"task_{str(uuid.uuid4())[:8]}"
                unique_tasks.append(task)
        
        return {
            "tasks": unique_tasks,
            "total_tasks": len(unique_tasks),
            "summary": " ".join(summaries)[:200] if summaries else "ä»»åŠ¡ç”Ÿæˆå®Œæˆ"
        }
    
    def _merge_generic(self, results: List[Dict[str, Any]], analysis_type: str) -> Dict[str, Any]:
        """é€šç”¨åˆå¹¶æ–¹æ³•"""
        merged = {}
        summaries = []
        
        for result in results:
            for key, value in result.items():
                if key == "summary":
                    if value:
                        summaries.append(value)
                elif key not in merged:
                    merged[key] = value
                elif isinstance(value, list):
                    if isinstance(merged[key], list):
                        merged[key].extend(value)
                elif isinstance(value, dict):
                    if isinstance(merged[key], dict):
                        merged[key].update(value)
        
        merged["summary"] = " ".join(summaries)[:200] if summaries else f"{analysis_type}å®Œæˆ"
        return merged

class TaskStorageManager:
    """ä»»åŠ¡å­˜å‚¨ç®¡ç†å™¨ - å¢å¼ºçš„SQLiteæ•°æ®åº“æ“ä½œï¼Œæ”¯æŒWindowsç¯å¢ƒ
    
    ğŸ“ æ³¨æ„ï¼šæ­¤æ•°æ®åº“ä¸“é—¨ç”¨äºå­˜å‚¨æ‰§è¡Œä»»åŠ¡ï¼Œä¸LangGraphæ£€æŸ¥ç‚¹æ•°æ®åº“(workflow_checkpoints.db)åˆ†ç¦»
    """
    
    def __init__(self, db_path: str = "coding_agent_workflow.db"):
        """åˆå§‹åŒ–ä»»åŠ¡å­˜å‚¨ç®¡ç†å™¨
        
        Args:
            db_path: ä»»åŠ¡æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (é»˜è®¤: coding_agent_workflow.db)
                    æ³¨æ„ï¼šä¸LangGraphæ£€æŸ¥ç‚¹æ•°æ®åº“åˆ†ç¦»ï¼Œé¿å…å†²çª
        """
        self.db_path = db_path
        self.max_retries = 3
        self.retry_delay = 1.0
        self._init_database()
    
    def _get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥ï¼Œè®¾ç½®WALæ¨¡å¼å’Œè¶…æ—¶"""
        conn = sqlite3.connect(
            self.db_path, 
            timeout=30.0,  # 30ç§’è¶…æ—¶
            isolation_level=None  # è‡ªåŠ¨æäº¤æ¨¡å¼
        )
        
        # è®¾ç½®WALæ¨¡å¼å’Œä¼˜åŒ–å‚æ•°
        conn.execute("PRAGMA journal_mode=WAL")
        conn.execute("PRAGMA synchronous=NORMAL") 
        conn.execute("PRAGMA cache_size=10000")
        conn.execute("PRAGMA temp_store=memory")
        conn.execute("PRAGMA mmap_size=268435456")  # 256MB
        
        return conn
    
    def _execute_with_retry(self, operation_func, *args, **kwargs):
        """å¸¦é‡è¯•æœºåˆ¶çš„æ•°æ®åº“æ“ä½œ"""
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                return operation_func(*args, **kwargs)
            except sqlite3.OperationalError as e:
                last_error = e
                if "database is locked" in str(e):
                    logger.warning(f"ğŸ”„ æ•°æ®åº“é”å®šï¼Œç¬¬{attempt + 1}æ¬¡é‡è¯•...")
                    time.sleep(self.retry_delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    raise e
            except Exception as e:
                logger.error(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
                raise e
        
        logger.error(f"âŒ æ•°æ®åº“æ“ä½œé‡è¯•{self.max_retries}æ¬¡åä»ç„¶å¤±è´¥")
        if last_error is not None:
            raise last_error
        else:
            raise RuntimeError(f"æ•°æ®åº“æ“ä½œé‡è¯•{self.max_retries}æ¬¡åå¤±è´¥")
    
    def force_unlock_database(self):
        """å¼ºåˆ¶è§£é”æ•°æ®åº“ï¼ˆWindowsç¯å¢ƒç‰¹æ®Šå¤„ç†ï¼‰"""
        try:
            import os
            import time
            
            # å¼ºåˆ¶å…³é—­æ‰€æœ‰WALç›¸å…³æ–‡ä»¶
            wal_file = f"{self.db_path}-wal"
            shm_file = f"{self.db_path}-shm"
            
            # å°è¯•åˆ é™¤WALæ–‡ä»¶
            for file_path in [wal_file, shm_file]:
                if os.path.exists(file_path):
                    try:
                        os.remove(file_path)
                        logger.info(f"âœ… åˆ é™¤WALæ–‡ä»¶: {file_path}")
                    except PermissionError:
                        logger.warning(f"âš ï¸ æ— æ³•åˆ é™¤æ–‡ä»¶ {file_path}ï¼Œå¯èƒ½è¢«å…¶ä»–è¿›ç¨‹å ç”¨")
            
            # æ‰§è¡ŒVACUUMå’Œä¼˜åŒ–
            try:
                with self._get_connection() as conn:
                    conn.execute("PRAGMA wal_checkpoint(TRUNCATE)")
                    conn.execute("VACUUM")
                logger.info("âœ… æ•°æ®åº“WALæ£€æŸ¥ç‚¹å’Œä¼˜åŒ–å®Œæˆ")
            except Exception as e:
                logger.warning(f"âš ï¸ WALæ£€æŸ¥ç‚¹æ“ä½œå¤±è´¥: {e}")
                
        except Exception as e:
            logger.error(f"âŒ å¼ºåˆ¶è§£é”å¤±è´¥: {e}")
    
    def reset_database(self):
        """é‡ç½®æ•°æ®åº“è¡¨ç»“æ„"""
        def _reset_operation():
            # å…ˆå°è¯•å¼ºåˆ¶è§£é”
            self.force_unlock_database()
            
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                # åˆ é™¤æ—§è¡¨
                cursor.execute("DROP TABLE IF EXISTS execution_tasks")
                
                # é‡æ–°åˆ›å»ºè¡¨
                cursor.execute("""
                    CREATE TABLE execution_tasks (
                        task_id TEXT PRIMARY KEY,
                        service_name TEXT NOT NULL,
                        task_type TEXT NOT NULL,  -- database|api|service|config|test|deployment
                        priority INTEGER DEFAULT 1,
                        status TEXT DEFAULT 'pending',
                        dependencies TEXT,  -- JSON array
                        estimated_duration TEXT,
                        description TEXT,
                        deliverables TEXT,  -- JSON array - å…·ä½“äº¤ä»˜ç‰©
                        implementation_details TEXT,  -- è¯¦ç»†å®ç°è¯´æ˜
                        completion_criteria TEXT,  -- å®Œæˆæ ‡å‡†
                        parameters TEXT,     -- JSON object - å¢å¼ºçš„å‚æ•°
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                logger.info("âœ… æ•°æ®åº“è¡¨ç»“æ„é‡ç½®å®Œæˆ")
        
        try:
            self._execute_with_retry(_reset_operation)
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“é‡ç½®å¤±è´¥: {e}")
            # å°è¯•æç«¯æƒ…å†µä¸‹çš„ä¿®å¤
            try:
                self.force_unlock_database()
                time.sleep(2)  # ç­‰å¾…2ç§’
                self._execute_with_retry(_reset_operation)
            except Exception as final_error:
                logger.error(f"âŒ æœ€ç»ˆæ•°æ®åº“é‡ç½®å¤±è´¥: {final_error}")
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        def _init_operation():
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                # åˆ›å»ºä»»åŠ¡è¡¨ - æ”¯æŒç»†ç²’åº¦ä»»åŠ¡
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS execution_tasks (
                        task_id TEXT PRIMARY KEY,
                        service_name TEXT NOT NULL,
                        task_type TEXT NOT NULL,  -- database|api|service|config|test|deployment
                        priority INTEGER DEFAULT 1,
                        status TEXT DEFAULT 'pending',
                        dependencies TEXT,  -- JSON array
                        estimated_duration TEXT,
                        description TEXT,
                        deliverables TEXT,  -- JSON array - å…·ä½“äº¤ä»˜ç‰©
                        implementation_details TEXT,  -- è¯¦ç»†å®ç°è¯´æ˜
                        completion_criteria TEXT,  -- å®Œæˆæ ‡å‡†
                        parameters TEXT,     -- JSON object - å¢å¼ºçš„å‚æ•°
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                logger.info("âœ… ä»»åŠ¡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        
        try:
            self._execute_with_retry(_init_operation)
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            # å°è¯•å¼ºåˆ¶è§£é”åé‡è¯•
            self.force_unlock_database()
            time.sleep(1)
            try:
                self._execute_with_retry(_init_operation)
            except Exception as final_error:
                logger.error(f"âŒ æœ€ç»ˆæ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {final_error}")
    
    def save_tasks(self, tasks: List[Dict[str, Any]]) -> bool:
        """ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“"""
        def _save_operation():
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                for task in tasks:
                    cursor.execute("""
                        INSERT OR REPLACE INTO execution_tasks 
                        (task_id, service_name, task_type, priority, dependencies, 
                         estimated_duration, description, deliverables, 
                         implementation_details, completion_criteria, parameters)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        task.get('task_id', str(uuid.uuid4())),
                        task.get('service_name', ''),
                        task.get('task_type', 'code_generation'),
                        task.get('priority', 1),
                        json.dumps(task.get('dependencies', [])),
                        task.get('estimated_duration', '30åˆ†é’Ÿ'),
                        task.get('description', ''),
                        json.dumps(task.get('deliverables', [])),
                        task.get('implementation_details', ''),
                        task.get('completion_criteria', ''),
                        json.dumps(task.get('parameters', {}))
                    ))
                
                logger.info(f"âœ… å·²ä¿å­˜ {len(tasks)} ä¸ªä»»åŠ¡åˆ°æ•°æ®åº“")
                return True
        
        try:
            return self._execute_with_retry(_save_operation)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ä»»åŠ¡å¤±è´¥: {e}")
            return False
    
    def get_pending_tasks(self) -> List[Dict[str, Any]]:
        """è·å–å¾…æ‰§è¡Œçš„ä»»åŠ¡"""
        def _get_operation():
            with self._get_connection() as conn:
                cursor = conn.cursor()
                
                cursor.execute("""
                    SELECT task_id, service_name, task_type, priority, dependencies,
                           estimated_duration, description, deliverables,
                           implementation_details, completion_criteria, parameters
                    FROM execution_tasks 
                    WHERE status = 'pending'
                    ORDER BY priority ASC, created_at ASC
                """)
                
                tasks = []
                for row in cursor.fetchall():
                    tasks.append({
                        'task_id': row[0],
                        'service_name': row[1],
                        'task_type': row[2],
                        'priority': row[3],
                        'dependencies': json.loads(row[4] or '[]'),
                        'estimated_duration': row[5],
                        'description': row[6],
                        'deliverables': json.loads(row[7] or '[]'),
                        'implementation_details': row[8],
                        'completion_criteria': row[9],
                        'parameters': json.loads(row[10] or '{}')
                    })
                
                return tasks
        
        try:
            return self._execute_with_retry(_get_operation)
        except Exception as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡å¤±è´¥: {e}")
            return []

async def task_splitting_node(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»»åŠ¡æ‹†åˆ†èŠ‚ç‚¹ - é‡æ„ç‰ˆæœ¬
    å®ç°æ»‘åŠ¨çª—å£æœºåˆ¶å’ŒSQLiteä»»åŠ¡å­˜å‚¨
    """
    
    logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œä»»åŠ¡æ‹†åˆ†: {state['project_name']}")
    logger.info(f"ğŸ“¥ è¾“å…¥çŠ¶æ€é”®: {list(state.keys())}")
    logger.info(f"ğŸ“„ è®¾è®¡æ–‡æ¡£é•¿åº¦: {len(state.get('design_doc', ''))}")
    logger.info(f"ğŸ”„ å½“å‰é˜¶æ®µ: {state.get('current_phase', 'unknown')}")
    
    # ğŸ”§ è®¡ç®—é¡¹ç›®è·¯å¾„ï¼Œä¸git_management_nodeä¿æŒä¸€è‡´
    output_path = state.get('output_path', '/Users/renyu/Documents/create_project')
    project_name = state.get('project_name', 'unknown_project')
    base_project_path = f"{output_path}/{project_name}"
    logger.info(f"ğŸ“ è®¡ç®—çš„åŸºç¡€é¡¹ç›®è·¯å¾„: {base_project_path}")
    
    # åˆå§‹åŒ–ç»„ä»¶
    if not VOLCENGINE_AVAILABLE:
        logger.error("âŒ ç«å±±å¼•æ“å®¢æˆ·ç«¯ä¸å¯ç”¨")
        state["execution_errors"].append("ç«å±±å¼•æ“å®¢æˆ·ç«¯ä¸å¯ç”¨")
        return state
    
    client = get_volcengine_client()
    prompts = TaskSplittingPrompts()
    window_manager = SlidingWindowManager(max_window_size=2000)
    task_storage = TaskStorageManager()
    
    # ğŸ”§ é‡ç½®æ•°æ®åº“è¡¨ç»“æ„ä»¥æ”¯æŒæ–°çš„ä»»åŠ¡å­—æ®µ
    task_storage.reset_database()
    
    try:
        # ğŸ§  æ­¥éª¤1ï¼šè®¾è®¡æ–‡æ¡£åˆ†æï¼ˆä½¿ç”¨çœŸæ­£çš„æ»‘åŠ¨çª—å£ï¼‰
        logger.info("ğŸ“‹ æ­¥éª¤1ï¼šå¼€å§‹è®¾è®¡æ–‡æ¡£åˆ†æ...")
        
        # å°†è®¾è®¡æ–‡æ¡£åˆ†å‰²æˆæ»‘åŠ¨çª—å£
        design_windows = window_manager.split_text_into_windows(state["design_doc"])
        logger.info(f"ğŸ“„ è®¾è®¡æ–‡æ¡£åˆ†å‰²æˆ {len(design_windows)} ä¸ªçª—å£")
        
        # å¯¹æ¯ä¸ªçª—å£è¿›è¡Œåˆ†æ
        window_results = []
        for window in design_windows:
            total_windows = window.get('total_windows', len(design_windows))
            logger.info(f"ğŸ“ åˆ†æçª—å£ {window['window_id']}/{total_windows} (å­—ç¬¦: {window['start_pos']}-{window['end_pos']})")
            
            design_analysis_prompt = prompts.get_prompt(
                "design_analysis", 
                design_doc=window["content"],
                project_name=state["project_name"],
                context_window=window_manager.get_context_window()
            )
            
            design_analysis_result = client.chat(
                messages=[
                    {"role": "system", "content": f"ä½ æ˜¯ä¸“ä¸šçš„ç³»ç»Ÿæ¶æ„å¸ˆã€‚æ­£åœ¨åˆ†æç¬¬ {window['window_id']}/{total_windows} ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚"},
                    {"role": "user", "content": design_analysis_prompt}
                ],
                temperature=0.3
            )
            
            # è§£æå•ä¸ªçª—å£çš„ç»“æœ
            window_data = _extract_json_from_response(design_analysis_result)
            window_results.append(window_data)
            logger.info(f"âœ… çª—å£ {window['window_id']} åˆ†æå®Œæˆ")
        
        # åˆå¹¶æ‰€æœ‰çª—å£çš„åˆ†æç»“æœ
        design_analysis_data = window_manager.merge_analysis_results(window_results, "è®¾è®¡åˆ†æ")
        logger.info(f"ğŸ“Š åˆå¹¶åçš„è®¾è®¡åˆ†æç»“æœ: {design_analysis_data}")
        design_summary = design_analysis_data.get('summary', 'è®¾è®¡åˆ†æå®Œæˆ')
        logger.info(f"ğŸ“ è®¾è®¡æ¦‚è¦: {design_summary}")
        window_manager.add_context(design_summary, "è®¾è®¡åˆ†æ")
        
        # ğŸ—ï¸ æ­¥éª¤2ï¼šæŠ€æœ¯æ¶æ„åˆ†æ
        logger.info("ğŸ—ï¸ æ­¥éª¤2ï¼šå¼€å§‹æŠ€æœ¯æ¶æ„åˆ†æ...")
        
        # åŸºäºè®¾è®¡æ¦‚è¦è¿›è¡Œæ›´æ·±å…¥çš„æŠ€æœ¯æ¶æ„åˆ†æ
        architecture_summary = design_summary  # ç›´æ¥ä½¿ç”¨æ­¥éª¤1çš„è®¾è®¡æ¦‚è¦
        logger.info(f"ğŸ“ æŠ€æœ¯æ¶æ„æ¦‚è¦: {architecture_summary}")
        window_manager.add_context(architecture_summary, "æŠ€æœ¯æ¶æ„")
        
        # ğŸ” æ­¥éª¤3ï¼šæœåŠ¡è¾¹ç•Œè¯†åˆ«
        logger.info("ğŸ” æ­¥éª¤3ï¼šå¼€å§‹æœåŠ¡è¾¹ç•Œè¯†åˆ«...")
        logger.info(f"ğŸ“ è¾“å…¥ - è®¾è®¡æ¦‚è¦: {design_summary}")
        logger.info(f"ğŸ“ è¾“å…¥ - æ¶æ„æ¦‚è¦: {architecture_summary}")
        
        service_prompt = prompts.get_prompt(
            "service_boundary",
            design_summary=design_summary,
            architecture_summary=architecture_summary,
            context_window=window_manager.get_context_window()
        )
        logger.info(f"ğŸ“ æœåŠ¡æ‹†åˆ†æç¤ºè¯é•¿åº¦: {len(service_prompt)}")
        logger.debug(f"ğŸ“ æœåŠ¡æ‹†åˆ†æç¤ºè¯å†…å®¹: {service_prompt[:500]}...")
        
        service_result = client.chat(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å¾®æœåŠ¡æ¶æ„ä¸“å®¶ã€‚"},
                {"role": "user", "content": service_prompt}
            ],
            temperature=0.2
        )
        logger.info(f"âœ… æœåŠ¡æ‹†åˆ†LLMå“åº”é•¿åº¦: {len(service_result)}")
        logger.debug(f"ğŸ” æœåŠ¡æ‹†åˆ†åŸå§‹å“åº”: {service_result[:500]}...")
        
        # è§£ææœåŠ¡è¯†åˆ«ç»“æœ
        try:
            service_data = _extract_json_from_response(service_result)
            logger.info(f"ğŸ“Š æœåŠ¡æ‹†åˆ†è§£æç»“æœ: {service_data}")
        except Exception as e:
            logger.error(f"âŒ æœåŠ¡æ‹†åˆ†ç»“æœè§£æå¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤æœåŠ¡åˆ—è¡¨
            service_data = {
                'identified_services': [
                    {'name': 'ç”¨æˆ·æœåŠ¡', 'description': 'è´Ÿè´£ç”¨æˆ·ç®¡ç†'},
                    {'name': 'ä¸šåŠ¡æœåŠ¡', 'description': 'è´Ÿè´£æ ¸å¿ƒä¸šåŠ¡é€»è¾‘'}
                ],
                'summary': 'æœåŠ¡æ‹†åˆ†åŸºäºé»˜è®¤é…ç½®å®Œæˆ'
            }
            logger.info(f"ğŸ”§ ä½¿ç”¨é»˜è®¤æœåŠ¡é…ç½®: {service_data}")
        
        identified_services = service_data.get('identified_services', [])
        
        # å¦‚æœæœåŠ¡åˆ—è¡¨ä¸ºç©ºï¼Œåˆ›å»ºé»˜è®¤æœåŠ¡
        if not identified_services:
            logger.warning("âš ï¸ æœªè¯†åˆ«åˆ°ä»»ä½•æœåŠ¡ï¼Œåˆ›å»ºé»˜è®¤æœåŠ¡")
            identified_services = [
                {'name': 'ç”¨æˆ·æœåŠ¡', 'description': 'è´Ÿè´£ç”¨æˆ·ç®¡ç†'},
                {'name': 'ä¸šåŠ¡æœåŠ¡', 'description': 'è´Ÿè´£æ ¸å¿ƒä¸šåŠ¡é€»è¾‘'}
            ]
            
        logger.info(f"ğŸ¯ è¯†åˆ«çš„æœåŠ¡åˆ—è¡¨: {identified_services}")
        logger.info(f"ğŸ”¢ è¯†åˆ«çš„æœåŠ¡æ•°é‡: {len(identified_services)}")
        services_summary = service_data.get('summary', 'æœåŠ¡æ‹†åˆ†å®Œæˆ')
        logger.info(f"ğŸ“ æœåŠ¡æ‹†åˆ†æ¦‚è¦: {services_summary}")
        window_manager.add_context(services_summary, "æœåŠ¡æ‹†åˆ†")
        
        # ğŸŒ æ­¥éª¤4ï¼šä¾èµ–åˆ†æ
        logger.info("ğŸŒ æ­¥éª¤4ï¼šå¼€å§‹ä¾èµ–åˆ†æ...")
        dependency_prompt = prompts.get_prompt(
            "dependency_analysis",
            services_summary=services_summary,
            architecture_summary=architecture_summary,
            context_window=window_manager.get_context_window()
        )
        logger.info(f"ğŸ“ ä¾èµ–åˆ†ææç¤ºè¯é•¿åº¦: {len(dependency_prompt)}")
        
        dependency_result = client.chat(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯å¾®æœåŠ¡ä¾èµ–åˆ†æä¸“å®¶ã€‚"},
                {"role": "user", "content": dependency_prompt}
            ],
            temperature=0.2
        )
        logger.info(f"âœ… ä¾èµ–åˆ†æLLMå“åº”é•¿åº¦: {len(dependency_result)}")
        
        # è§£æä¾èµ–åˆ†æç»“æœ
        dependency_data = _extract_json_from_response(dependency_result)
        logger.info(f"ğŸ“Š ä¾èµ–åˆ†æè§£æç»“æœ: {dependency_data}")
        dependencies_summary = dependency_data.get('summary', 'ä¾èµ–åˆ†æå®Œæˆ')
        logger.info(f"ğŸ“ ä¾èµ–åˆ†ææ¦‚è¦: {dependencies_summary}")
        
        # ğŸ“… æ­¥éª¤5ï¼šä»»åŠ¡è°ƒåº¦
        logger.info("ğŸ“… æ­¥éª¤5ï¼šå¼€å§‹åˆ¶å®šæ‰§è¡Œè®¡åˆ’...")
        scheduling_prompt = prompts.get_prompt(
            "task_scheduling",
            services_summary=services_summary,
            dependencies_summary=dependencies_summary,
            context_window=window_manager.get_context_window()
        )
        logger.info(f"ğŸ“ ä»»åŠ¡è°ƒåº¦æç¤ºè¯é•¿åº¦: {len(scheduling_prompt)}")
        
        scheduling_result = client.chat(
            messages=[
                {"role": "system", "content": "ä½ æ˜¯é¡¹ç›®è°ƒåº¦ä¸“å®¶ã€‚"},
                {"role": "user", "content": scheduling_prompt}
            ],
            temperature=0.2
        )
        logger.info(f"âœ… ä»»åŠ¡è°ƒåº¦LLMå“åº”é•¿åº¦: {len(scheduling_result)}")
        
        # è§£ææ‰§è¡Œè®¡åˆ’
        execution_data = _extract_json_from_response(scheduling_result)
        logger.info(f"ğŸ“Š æ‰§è¡Œè®¡åˆ’è§£æç»“æœ: {execution_data}")
        
        # ğŸ’¾ æ­¥éª¤6ï¼šç”ŸæˆSQLiteä»»åŠ¡ï¼ˆä½¿ç”¨æ»‘åŠ¨çª—å£å¤„ç†é•¿æ‰§è¡Œè®¡åˆ’ï¼‰
        logger.info("ğŸ’¾ æ­¥éª¤6ï¼šå¼€å§‹ç”ŸæˆSQLiteä»»åŠ¡...")
        
        # å‡†å¤‡æ‰§è¡Œè®¡åˆ’æ–‡æœ¬
        execution_plan_text = json.dumps(execution_data, ensure_ascii=False, indent=2)
        
        # å¦‚æœæ‰§è¡Œè®¡åˆ’å¾ˆé•¿ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£å¤„ç†
        if len(execution_plan_text) > 1800:
            logger.info(f"ğŸ“„ æ‰§è¡Œè®¡åˆ’è¾ƒé•¿({len(execution_plan_text)}å­—ç¬¦)ï¼Œä½¿ç”¨æ»‘åŠ¨çª—å£å¤„ç†")
            plan_windows = window_manager.split_text_into_windows(execution_plan_text)
            
            # å¯¹æ¯ä¸ªçª—å£ç”Ÿæˆä»»åŠ¡
            all_task_results = []
            for window in plan_windows:
                total_windows = window.get('total_windows', len(plan_windows))
                logger.info(f"ğŸ“ å¤„ç†è®¡åˆ’çª—å£ {window['window_id']}/{total_windows}")
                
                task_generation_prompt = prompts.get_prompt(
                    "generate_sqlite_tasks",
                    execution_plan=window["content"],
                    services_summary=services_summary,
                    context_window=window_manager.get_context_window(),
                    base_project_path=base_project_path
                )
                
                task_result = client.chat(
                    messages=[
                        {"role": "system", "content": f"ä½ æ˜¯ä»»åŠ¡ç®¡ç†ä¸“å®¶ã€‚æ­£åœ¨å¤„ç†ç¬¬ {window['window_id']}/{total_windows} ä¸ªæ‰§è¡Œè®¡åˆ’ç‰‡æ®µã€‚è¯·ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿æ ¼å¼ç”Ÿæˆä»»åŠ¡ï¼Œç‰¹åˆ«æ³¨æ„git_cloneå’Œapiä»»åŠ¡çš„è·¯å¾„ä¸€è‡´æ€§ã€‚"},
                        {"role": "user", "content": task_generation_prompt}
                    ],
                    temperature=0.1
                )
                
                window_task_data = _extract_json_from_response(task_result)
                all_task_results.append(window_task_data)
                logger.info(f"âœ… çª—å£ {window['window_id']} ä»»åŠ¡ç”Ÿæˆå®Œæˆ")
            
            # åˆå¹¶æ‰€æœ‰çª—å£çš„ä»»åŠ¡
            task_data = window_manager.merge_analysis_results(all_task_results, "ä»»åŠ¡ç”Ÿæˆ")
        else:
            # æ‰§è¡Œè®¡åˆ’ä¸é•¿ï¼Œç›´æ¥å¤„ç†
            logger.info(f"ğŸ“„ æ‰§è¡Œè®¡åˆ’é€‚ä¸­({len(execution_plan_text)}å­—ç¬¦)ï¼Œç›´æ¥å¤„ç†")
            task_generation_prompt = prompts.get_prompt(
                "generate_sqlite_tasks",
                execution_plan=execution_plan_text,
                services_summary=services_summary,
                context_window=window_manager.get_context_window(),
                base_project_path=base_project_path
            )
            
            task_result = client.chat(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä»»åŠ¡ç®¡ç†ä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§æ¨¡æ¿æ ¼å¼ç”Ÿæˆä»»åŠ¡ï¼Œç‰¹åˆ«æ³¨æ„git_cloneå’Œapiä»»åŠ¡çš„è·¯å¾„ä¸€è‡´æ€§ã€‚"},
                    {"role": "user", "content": task_generation_prompt}
                ],
                temperature=0.1
            )
            
            task_data = _extract_json_from_response(task_result)
        
        logger.info(f"ğŸ“Š ä»»åŠ¡ç”Ÿæˆè§£æç»“æœ: {task_data}")
        tasks = task_data.get('tasks', [])
        logger.info(f"ğŸ¯ ç”Ÿæˆçš„ä»»åŠ¡åˆ—è¡¨: {tasks}")
        logger.info(f"ğŸ”¢ ç”Ÿæˆçš„ä»»åŠ¡æ•°é‡: {len(tasks)}")
        
        # ä¿å­˜ä»»åŠ¡åˆ°SQLiteæ•°æ®åº“
        if tasks:
            success = task_storage.save_tasks(tasks)
            if success:
                logger.info(f"âœ… å·²ç”Ÿæˆå¹¶ä¿å­˜ {len(tasks)} ä¸ªä»»åŠ¡åˆ°æ•°æ®åº“")
            else:
                logger.error("âŒ ä»»åŠ¡ä¿å­˜å¤±è´¥")
        else:
            logger.warning("âš ï¸ æ²¡æœ‰ç”Ÿæˆä»»ä½•ä»»åŠ¡")
        
        # æ›´æ–°çŠ¶æ€
        final_services = [service.get('name', f'æœåŠ¡{i+1}') for i, service in enumerate(identified_services)]
        logger.info(f"ğŸ“¤ æœ€ç»ˆæœåŠ¡åˆ—è¡¨: {final_services}")
        
        # ğŸ”§ ç”Ÿæˆå¹¶è¡Œä»»åŠ¡æ‰¹æ¬¡ï¼ˆä¸ºæ™ºèƒ½ç¼–ç èŠ‚ç‚¹å‡†å¤‡ï¼‰
        parallel_tasks = [
            {
                "batch_id": "batch_1",
                "services": final_services,  # ç®€å•èµ·è§ï¼ŒæŠŠæ‰€æœ‰æœåŠ¡æ”¾åœ¨ä¸€ä¸ªæ‰¹æ¬¡
                "dependencies": []
            }
        ]
        logger.info(f"ğŸ“¤ ç”Ÿæˆå¹¶è¡Œä»»åŠ¡æ‰¹æ¬¡: {parallel_tasks}")
        
        state.update({
            "identified_services": final_services,
            "service_dependencies": dependency_data.get('service_dependencies', {}),
            "task_execution_plan": execution_data,
            "parallel_tasks": parallel_tasks,  # ğŸ”§ æ–°å¢ï¼šä¸ºæ™ºèƒ½ç¼–ç èŠ‚ç‚¹å‡†å¤‡
            "generated_tasks": tasks,
            "current_phase": "intelligent_coding"  # ğŸ”§ ä¿®æ”¹ï¼šè®¾ç½®ä¸ºä¸‹ä¸€ä¸ªé˜¶æ®µ
        })
        
        logger.info(f"ğŸ“¤ è¾“å‡ºçŠ¶æ€é”®: {list(state.keys())}")
        logger.info(f"ğŸ“¤ è¾“å‡º - è¯†åˆ«æœåŠ¡: {state['identified_services']}")
        logger.info(f"ğŸ“¤ è¾“å‡º - å½“å‰é˜¶æ®µ: {state['current_phase']}")
        logger.info(f"âœ… ä»»åŠ¡æ‹†åˆ†å®Œæˆï¼Œè¯†åˆ« {len(identified_services)} ä¸ªæœåŠ¡ï¼Œç”Ÿæˆ {len(tasks)} ä¸ªä»»åŠ¡")
        return state
        
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‹†åˆ†å¤±è´¥: {e}")
        state["execution_errors"].append(f"ä»»åŠ¡æ‹†åˆ†å¤±è´¥: {str(e)}")
        return state

def _extract_json_from_response(response_text: str) -> Dict[str, Any]:
    """ä»LLMå“åº”ä¸­æå–JSONæ•°æ®"""
    try:
        # å°è¯•ç›´æ¥è§£æ
        if response_text.strip().startswith('{'):
            return json.loads(response_text.strip())
        
        # æŸ¥æ‰¾JSONä»£ç å—
        import re
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(1))
        
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªJSONå¯¹è±¡
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(0))
        
        logger.warning("âš ï¸ æ— æ³•ä»å“åº”ä¸­æå–JSONï¼Œè¿”å›é»˜è®¤ç»“æ„")
        return {"summary": "è§£æå¤±è´¥", "error": "JSONæå–å¤±è´¥"}
        
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
        return {"summary": "JSONè§£æé”™è¯¯", "error": str(e)}
    except Exception as e:
        logger.error(f"âŒ å“åº”è§£æå¤±è´¥: {e}")
        return {"summary": "å“åº”è§£æå¤±è´¥", "error": str(e)} 