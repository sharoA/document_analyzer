#!/usr/bin/env python3
"""
ç«å±±å¼•æ“APIå®¢æˆ·ç«¯
ä¸“é—¨ç”¨äºç«å±±å¼•æ“APIçš„å°è£…å’Œè°ƒç”¨
"""

import os
import logging
import time
from typing import List, Dict, Optional, Any, Generator
from dataclasses import dataclass

try:
    from openai import OpenAI
except ImportError:
    raise ImportError("è¯·å®‰è£…OpenAIåŒ…: pip install openai")

try:
    from .simple_config import settings
    from .llm_logger import log_llm_request, log_llm_response, log_llm_stream_chunk
except ImportError:
    from dotenv import load_dotenv
    load_dotenv()
    
    class MockSettings:
        VOLCENGINE_API_KEY = os.getenv("VOLCENGINE_API_KEY", "")
        VOLCENGINE_MODEL_ID = os.getenv("VOLCENGINE_MODEL_ID", "ep-20250528194304-wbvcf")
        VOLCENGINE_BASE_URL = os.getenv("VOLCENGINE_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3")
        DEFAULT_TEMPERATURE = 0.7
        DEFAULT_MAX_TOKENS = 2000
        DEFAULT_TIMEOUT = 120  # è°ƒæ•´ä¸º2åˆ†é’Ÿï¼Œé€‚åº”å¤§æ¨¡å‹å“åº”æ—¶é—´
    
    settings = MockSettings()
    
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºç©ºçš„æ—¥å¿—å‡½æ•°
    def log_llm_request(*args, **kwargs):
        return f"volcengine_{int(time.time() * 1000)}"
    
    def log_llm_response(*args, **kwargs):
        pass
    
    def log_llm_stream_chunk(*args, **kwargs):
        pass

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

@dataclass
class VolcengineConfig:
    """ç«å±±å¼•æ“é…ç½®"""
    api_key: str
    model_id: str
    base_url: str = "https://ark.cn-beijing.volces.com/api/v3"
    temperature: float = 0.7
    max_tokens: int = 2000
    timeout: int = 120  # è°ƒæ•´ä¸º2åˆ†é’Ÿï¼Œé€‚åº”å¤§æ¨¡å‹å“åº”æ—¶é—´

class VolcengineClient:
    """ç«å±±å¼•æ“å®¢æˆ·ç«¯"""
    
    def __init__(self, config: Optional[VolcengineConfig] = None):
        """
        åˆå§‹åŒ–ç«å±±å¼•æ“å®¢æˆ·ç«¯
        
        Args:
            config: ç«å±±å¼•æ“é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
        """
        if config is None:
            config = VolcengineConfig(
                api_key=settings.VOLCENGINE_API_KEY,
                model_id=settings.VOLCENGINE_MODEL_ID,
                base_url=settings.VOLCENGINE_BASE_URL,
                temperature=settings.DEFAULT_TEMPERATURE,
                max_tokens=settings.DEFAULT_MAX_TOKENS,
                timeout=settings.DEFAULT_TIMEOUT
            )
        
        self.config = config
        
        if not self.config.api_key:
            raise ValueError("ç«å±±å¼•æ“APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®VOLCENGINE_API_KEY")
        
        self.client = OpenAI(
            api_key=self.config.api_key,
            base_url=self.config.base_url,
            timeout=self.config.timeout
        )
        
        logger.info("ç«å±±å¼•æ“å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        stream: bool = False,
        **kwargs
    ) -> str:
        """
        èŠå¤©å¯¹è¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            AIå›å¤å†…å®¹
        """
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        request_params = {
            "temperature": temperature or self.config.temperature,
            "max_tokens": max_tokens or self.config.max_tokens,
            "stream": stream,
            **kwargs
        }
        
        # è®°å½•è¯·æ±‚æ—¥å¿—
        request_id = log_llm_request(
            provider="volcengine",
            model=self.config.model_id,
            messages=messages,
            parameters=request_params
        )
        
        start_time = time.time()
        
        try:
            response = self.client.chat.completions.create(
                model=self.config.model_id,
                messages=messages,
                **request_params
            )
            
            response_time = time.time() - start_time
            
            if stream:
                # è®°å½•æµå¼å“åº”å¼€å§‹
                log_llm_response(
                    request_id=request_id,
                    response_content="[STREAM_START]",
                    response_time=response_time,
                    token_usage=None
                )
                return response
            else:
                response_content = response.choices[0].message.content
                
                # æå–tokenä½¿ç”¨æƒ…å†µ
                token_usage = None
                if hasattr(response, 'usage') and response.usage:
                    token_usage = {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    }
                
                # è®°å½•å“åº”æ—¥å¿—
                log_llm_response(
                    request_id=request_id,
                    response_content=response_content,
                    response_time=response_time,
                    token_usage=token_usage
                )
                
                return response_content
                
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = str(e)
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            log_llm_response(
                request_id=request_id,
                response_content="",
                response_time=response_time,
                error=error_msg
            )
            
            logger.error(f"ç«å±±å¼•æ“APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def stream_chat(
        self,
        messages: List[Dict[str, str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Generator[str, None, None]:
        """
        æµå¼èŠå¤©
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            æµå¼å“åº”å†…å®¹
        """
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        request_params = {
            "temperature": temperature or self.config.temperature,
            "max_tokens": max_tokens or self.config.max_tokens,
            "stream": True,
            **kwargs
        }
        
        # è®°å½•è¯·æ±‚æ—¥å¿—
        request_id = log_llm_request(
            provider="volcengine",
            model=self.config.model_id,
            messages=messages,
            parameters=request_params
        )
        
        start_time = time.time()
        chunk_index = 0
        full_response = ""
        
        try:
            stream = self.client.chat.completions.create(
                model=self.config.model_id,
                messages=messages,
                **request_params
            )
            
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    chunk_content = chunk.choices[0].delta.content
                    full_response += chunk_content
                    
                    # è®°å½•æµå¼å“åº”å—
                    log_llm_stream_chunk(
                        request_id=request_id,
                        chunk_content=chunk_content,
                        chunk_index=chunk_index
                    )
                    
                    chunk_index += 1
                    yield chunk_content
            
            # è®°å½•å®Œæ•´å“åº”
            response_time = time.time() - start_time
            log_llm_response(
                request_id=request_id,
                response_content=full_response,
                response_time=response_time,
                token_usage=None  # æµå¼å“åº”é€šå¸¸ä¸è¿”å›tokenä½¿ç”¨æƒ…å†µ
            )
                    
        except Exception as e:
            response_time = time.time() - start_time
            error_msg = str(e)
            
            # è®°å½•é”™è¯¯æ—¥å¿—
            log_llm_response(
                request_id=request_id,
                response_content=full_response,
                response_time=response_time,
                error=error_msg
            )
            
            logger.error(f"ç«å±±å¼•æ“æµå¼APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def analyze_requirement(self, content: str, **kwargs) -> str:
        """
        åˆ†æéœ€æ±‚æ–‡æ¡£
        
        Args:
            content: éœ€æ±‚å†…å®¹
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            åˆ†æç»“æœ
        """
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹éœ€æ±‚æ–‡æ¡£ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

## åˆ†æè¦æ±‚ï¼š
1. **æ ¸å¿ƒåŠŸèƒ½ç‚¹**ï¼šåˆ—å‡ºä¸»è¦åŠŸèƒ½æ¨¡å—
2. **ä¸šåŠ¡å…³é”®è¯**ï¼šæå–ä¸šåŠ¡ç›¸å…³çš„é‡è¦æœ¯è¯­
3. **æŠ€æœ¯å…³é”®è¯**ï¼šè¯†åˆ«æŠ€æœ¯æ ˆå’ŒæŠ€æœ¯è¦æ±‚
4. **æ•°æ®å­—æ®µéœ€æ±‚**ï¼šåˆ†æéœ€è¦çš„æ•°æ®ç»“æ„
5. **æ¥å£éœ€æ±‚**ï¼šè¯†åˆ«éœ€è¦çš„APIæ¥å£
6. **æ½œåœ¨é—®é¢˜**ï¼šæŒ‡å‡ºå¯èƒ½çš„é£é™©å’ŒæŒ‘æˆ˜
7. **å®ç°å»ºè®®**ï¼šæä¾›æŠ€æœ¯å®ç°å»ºè®®

è¯·ç”¨ç»“æ„åŒ–çš„Markdownæ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚"""
            },
            {
                "role": "user",
                "content": f"éœ€æ±‚æ–‡æ¡£ï¼š\n{content}"
            }
        ]
        
        return self.chat(messages, temperature=0.3, **kwargs)
    
    def generate_api_design(self, requirement: str, **kwargs) -> str:
        """
        ç”ŸæˆAPIè®¾è®¡
        
        Args:
            requirement: éœ€æ±‚å†…å®¹
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            APIè®¾è®¡æ–‡æ¡£
        """
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„APIè®¾è®¡å¸ˆã€‚è¯·æ ¹æ®éœ€æ±‚è®¾è®¡RESTful APIï¼š

## è®¾è®¡è¦æ±‚ï¼š
1. **æ¥å£åˆ—è¡¨**ï¼šå®Œæ•´çš„APIç«¯ç‚¹åˆ—è¡¨
2. **è¯·æ±‚æ ¼å¼**ï¼šHTTPæ–¹æ³•ã€URLã€è¯·æ±‚å‚æ•°
3. **å“åº”æ ¼å¼**ï¼šè¿”å›æ•°æ®ç»“æ„å’ŒçŠ¶æ€ç 
4. **æ•°æ®æ¨¡å‹**ï¼šå®ä½“ç±»å’Œæ•°æ®åº“è¡¨ç»“æ„
5. **é”™è¯¯å¤„ç†**ï¼šé”™è¯¯ç å’Œé”™è¯¯ä¿¡æ¯
6. **è®¤è¯æˆæƒ**ï¼šå®‰å…¨æœºåˆ¶è®¾è®¡
7. **æ¥å£æ–‡æ¡£**ï¼šè¯¦ç»†çš„APIæ–‡æ¡£

è¯·ç”¨JSONå’ŒMarkdownæ··åˆæ ¼å¼è¾“å‡ºè®¾è®¡ç»“æœã€‚"""
            },
            {
                "role": "user",
                "content": f"éœ€æ±‚ï¼š\n{requirement}"
            }
        ]
        
        return self.chat(messages, temperature=0.4, **kwargs)
    
    def generate_design_document(self, requirement: str, doc_type: str = "backend", **kwargs) -> str:
        """
        ç”Ÿæˆè®¾è®¡æ–‡æ¡£
        
        Args:
            requirement: éœ€æ±‚å†…å®¹
            doc_type: æ–‡æ¡£ç±»å‹ (backend/frontend/database)
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            è®¾è®¡æ–‡æ¡£
        """
        if doc_type == "backend":
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åç«¯æ¶æ„å¸ˆã€‚è¯·æ ¹æ®éœ€æ±‚ç”Ÿæˆåç«¯è®¾è®¡æ–‡æ¡£ï¼š

## è®¾è®¡å†…å®¹ï¼š
1. **ç³»ç»Ÿæ¶æ„**ï¼šæ•´ä½“æ¶æ„è®¾è®¡
2. **æŠ€æœ¯æ ˆ**ï¼šåç«¯æŠ€æœ¯é€‰å‹
3. **æ•°æ®åº“è®¾è®¡**ï¼šè¡¨ç»“æ„å’Œå…³ç³»
4. **APIè®¾è®¡**ï¼šæ¥å£è§„èŒƒ
5. **ä¸šåŠ¡é€»è¾‘**ï¼šæ ¸å¿ƒä¸šåŠ¡æµç¨‹
6. **å®‰å…¨è®¾è®¡**ï¼šè®¤è¯æˆæƒæœºåˆ¶
7. **æ€§èƒ½ä¼˜åŒ–**ï¼šç¼“å­˜å’Œä¼˜åŒ–ç­–ç•¥
8. **éƒ¨ç½²æ–¹æ¡ˆ**ï¼šéƒ¨ç½²æ¶æ„

è¯·ç”¨è¯¦ç»†çš„Markdownæ ¼å¼è¾“å‡ºè®¾è®¡æ–‡æ¡£ã€‚"""
        
        elif doc_type == "frontend":
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å‰ç«¯æ¶æ„å¸ˆã€‚è¯·æ ¹æ®éœ€æ±‚ç”Ÿæˆå‰ç«¯è®¾è®¡æ–‡æ¡£ï¼š

## è®¾è®¡å†…å®¹ï¼š
1. **é¡µé¢ç»“æ„**ï¼šé¡µé¢å±‚æ¬¡å’Œå¯¼èˆª
2. **ç»„ä»¶è®¾è®¡**ï¼šå¯å¤ç”¨ç»„ä»¶
3. **çŠ¶æ€ç®¡ç†**ï¼šæ•°æ®æµè®¾è®¡
4. **UI/UXè®¾è®¡**ï¼šç•Œé¢å’Œäº¤äº’è®¾è®¡
5. **æŠ€æœ¯æ ˆ**ï¼šå‰ç«¯æŠ€æœ¯é€‰å‹
6. **è·¯ç”±è®¾è®¡**ï¼šé¡µé¢è·¯ç”±è§„åˆ’
7. **APIé›†æˆ**ï¼šå‰åç«¯æ¥å£å¯¹æ¥
8. **æ€§èƒ½ä¼˜åŒ–**ï¼šå‰ç«¯ä¼˜åŒ–ç­–ç•¥

è¯·ç”¨è¯¦ç»†çš„Markdownæ ¼å¼è¾“å‡ºè®¾è®¡æ–‡æ¡£ã€‚"""
        
        else:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç³»ç»Ÿè®¾è®¡å¸ˆã€‚è¯·æ ¹æ®éœ€æ±‚ç”Ÿæˆç³»ç»Ÿè®¾è®¡æ–‡æ¡£ï¼š

## è®¾è®¡å†…å®¹ï¼š
1. **éœ€æ±‚åˆ†æ**ï¼šåŠŸèƒ½å’ŒéåŠŸèƒ½éœ€æ±‚
2. **ç³»ç»Ÿæ¶æ„**ï¼šæ•´ä½“æ¶æ„è®¾è®¡
3. **æ¨¡å—è®¾è®¡**ï¼šå„æ¨¡å—èŒè´£
4. **æ•°æ®è®¾è®¡**ï¼šæ•°æ®æ¨¡å‹å’Œæµç¨‹
5. **æ¥å£è®¾è®¡**ï¼šå†…å¤–éƒ¨æ¥å£
6. **å®‰å…¨è®¾è®¡**ï¼šå®‰å…¨ç­–ç•¥
7. **æ€§èƒ½è®¾è®¡**ï¼šæ€§èƒ½æŒ‡æ ‡å’Œä¼˜åŒ–
8. **è¿ç»´è®¾è®¡**ï¼šç›‘æ§å’Œéƒ¨ç½²

è¯·ç”¨è¯¦ç»†çš„Markdownæ ¼å¼è¾“å‡ºè®¾è®¡æ–‡æ¡£ã€‚"""
        
        messages = [
            {
                "role": "system",
                "content": system_prompt
            },
            {
                "role": "user",
                "content": f"éœ€æ±‚ï¼š\n{requirement}"
            }
        ]
        
        return self.chat(messages, temperature=0.5, **kwargs)
    
    def generate_code(self, description: str, language: str = "python", **kwargs) -> str:
        """
        ç”Ÿæˆä»£ç 
        
        Args:
            description: ä»£ç æè¿°
            language: ç¼–ç¨‹è¯­è¨€
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç”Ÿæˆçš„ä»£ç 
        """
        messages = [
            {
                "role": "system",
                "content": f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{language}ç¨‹åºå‘˜ã€‚è¯·æ ¹æ®æè¿°ç”Ÿæˆé«˜è´¨é‡çš„ä»£ç ï¼š

## ä»£ç è¦æ±‚ï¼š
1. **å®Œæ•´å®ç°**ï¼šåŠŸèƒ½å®Œæ•´å¯è¿è¡Œ
2. **ä»£ç è§„èŒƒ**ï¼šéµå¾ª{language}ç¼–ç è§„èŒƒ
3. **æ³¨é‡Šæ–‡æ¡£**ï¼šè¯¦ç»†çš„æ³¨é‡Šå’Œæ–‡æ¡£
4. **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸å¤„ç†
5. **æµ‹è¯•ç”¨ä¾‹**ï¼šåŸºæœ¬çš„æµ‹è¯•ä»£ç 
6. **ä½¿ç”¨ç¤ºä¾‹**ï¼šä»£ç ä½¿ç”¨ç¤ºä¾‹

è¯·ç¡®ä¿ä»£ç è´¨é‡é«˜ã€å¯è¯»æ€§å¼ºã€å¯ç»´æŠ¤æ€§å¥½ã€‚"""
            },
            {
                "role": "user",
                "content": f"è¯·ç”¨{language}å®ç°ï¼š\n{description}"
            }
        ]
        
        return self.chat(messages, temperature=0.2, **kwargs)
    
    def test_connection(self) -> bool:
        """
        æµ‹è¯•è¿æ¥
        
        Returns:
            è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            response = self.chat([
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'"}
            ])
            logger.info("ç«å±±å¼•æ“è¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"ç«å±±å¼•æ“è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

# å…¨å±€ç«å±±å¼•æ“å®¢æˆ·ç«¯å®ä¾‹
_volcengine_client = None

def get_volcengine_client() -> VolcengineClient:
    """
    è·å–ç«å±±å¼•æ“å®¢æˆ·ç«¯å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    
    Returns:
        ç«å±±å¼•æ“å®¢æˆ·ç«¯
    """
    global _volcengine_client
    if _volcengine_client is None:
        _volcengine_client = VolcengineClient()
    return _volcengine_client

# ä¾¿æ·å‡½æ•°
def volcengine_chat(messages: List[Dict[str, str]], **kwargs) -> str:
    """ç«å±±å¼•æ“èŠå¤©ä¾¿æ·å‡½æ•°"""
    client = get_volcengine_client()
    return client.chat(messages, **kwargs)

def volcengine_stream_chat(messages: List[Dict[str, str]], **kwargs) -> Generator[str, None, None]:
    """ç«å±±å¼•æ“æµå¼èŠå¤©ä¾¿æ·å‡½æ•°"""
    client = get_volcengine_client()
    yield from client.stream_chat(messages, **kwargs)

def volcengine_analyze_requirement(content: str, **kwargs) -> str:
    """ç«å±±å¼•æ“éœ€æ±‚åˆ†æä¾¿æ·å‡½æ•°"""
    client = get_volcengine_client()
    return client.analyze_requirement(content, **kwargs)

def volcengine_generate_api_design(requirement: str, **kwargs) -> str:
    """ç«å±±å¼•æ“APIè®¾è®¡ä¾¿æ·å‡½æ•°"""
    client = get_volcengine_client()
    return client.generate_api_design(requirement, **kwargs)

def volcengine_generate_design_document(requirement: str, doc_type: str = "backend", **kwargs) -> str:
    """ç«å±±å¼•æ“è®¾è®¡æ–‡æ¡£ç”Ÿæˆä¾¿æ·å‡½æ•°"""
    client = get_volcengine_client()
    return client.generate_design_document(requirement, doc_type, **kwargs)

def volcengine_generate_code(description: str, language: str = "python", **kwargs) -> str:
    """ç«å±±å¼•æ“ä»£ç ç”Ÿæˆä¾¿æ·å‡½æ•°"""
    client = get_volcengine_client()
    return client.generate_code(description, language, **kwargs)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸŒ‹ ç«å±±å¼•æ“å®¢æˆ·ç«¯æµ‹è¯•")
    print("=" * 50)
    
    try:
        client = get_volcengine_client()
        
        # æµ‹è¯•è¿æ¥
        if client.test_connection():
            print("âœ… è¿æ¥æµ‹è¯•æˆåŠŸ")
            
            # æµ‹è¯•èŠå¤©
            response = client.chat([
                {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
            ])
            print(f"âœ… èŠå¤©æµ‹è¯•: {response[:100]}...")
            
            # æµ‹è¯•éœ€æ±‚åˆ†æ
            analysis = client.analyze_requirement("å¼€å‘ä¸€ä¸ªç”¨æˆ·ç®¡ç†ç³»ç»Ÿ")
            print(f"âœ… éœ€æ±‚åˆ†ææµ‹è¯•: {analysis[:100]}...")
            
        else:
            print("âŒ è¿æ¥æµ‹è¯•å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}") 