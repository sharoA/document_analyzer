"""
å†…å®¹åˆ†ææœåŠ¡
ä½¿ç”¨å¤§æ¨¡å‹å’Œå‘é‡æ•°æ®åº“è¿›è¡ŒCRUDæ“ä½œåˆ†æå’Œä¸šåŠ¡éœ€æ±‚è¯†åˆ«
"""

import json
import time
import re
from typing import Dict, Any, List
from .base_service import BaseAnalysisService
from sentence_transformers import SentenceTransformer
from ..utils.weaviate_helper import get_weaviate_client

class ContentAnalyzerService(BaseAnalysisService):
    """å†…å®¹åˆ†ææœåŠ¡ç±»"""
    
    def __init__(self, llm_client=None, vector_db=None):
        super().__init__(llm_client, vector_db)
        # åˆå§‹åŒ–å‘é‡æ¨¡å‹ - ä½¿ç”¨bge-large-zhï¼ˆ1024ç»´ï¼Œä¸­æ–‡ä¼˜åŒ–ï¼‰
        self.embedding_model = SentenceTransformer('BAAI/bge-large-zh')
        # åˆå§‹åŒ–Weaviateå®¢æˆ·ç«¯
        self.weaviate_client = get_weaviate_client()
        # æ·»åŠ å‘é‡ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤è®¡ç®—
        self._embedding_cache = {}
        self._cache_max_size = 1000  # æœ€å¤šç¼“å­˜1000ä¸ªå‘é‡
    
    async def analyze(self, task_id: str, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œå†…å®¹åˆ†æ
        
        Args:
            task_id: ä»»åŠ¡ID
            input_data: éœ€æ±‚æ–‡æ¡£ï¼Œè½¬æˆçš„markdownæ¨¡å¼
            
        Returns:
            å†…å®¹åˆ†æç»“æœå­—å…¸
        """
        start_time = time.time()
        
        try:
            # æå–æ–‡ä»¶ç»“æœ
            document_content = input_data.get("document_content", "")
            self.logger.info(f"å†…å®¹åˆ†æå¼€å§‹document_content: {document_content}")
            self._log_analysis_start(task_id, "å†…å®¹åˆ†æ", len(document_content))
            
            # Step 1: æ–‡æ¡£é¢„å¤„ç†
            structured_chunks = await self._preprocess_document(document_content)
            
            self.logger.info(f"æ–‡æ¡£é¢„å¤„ç†: {structured_chunks}")
            # Step 2: ä¸å†å²çŸ¥è¯†åº“å†…å®¹å¯¹æ¯”
            history_comparison = await self._compare_with_history(structured_chunks)
            
            self.logger.info(f"å†å²æ¯”å¯¹ç»“æœ: {history_comparison}")
            # Step 3: å¤§æ¨¡å‹å˜æ›´åˆ¤æ–­ä¸ç»“æ„åŒ–è¾“å‡º
            change_analysis = await self._analyze_changes(structured_chunks, history_comparison, document_content)
            
            self.logger.info(f"å˜æ›´åˆ†æå®Œæˆ: {change_analysis}")
            
            # Step 4: æ™ºèƒ½åˆå¹¶ç›¸ä¼¼çš„å˜æ›´é¡¹
            merged_change_analysis = await self._merge_similar_changes(change_analysis)
            
            self.logger.info(f"ç›¸ä¼¼å˜æ›´é¡¹åˆå¹¶å®Œæˆ: {merged_change_analysis}")
            
            # Step 5: å¯¹å˜æ›´é¡¹ä»åŸæ–‡æ¡£æå–è¯¦ç»†å˜æ›´ç‚¹
            enhanced_change_analysis = await self._extract_detailed_changes(merged_change_analysis, document_content)
            
            # Step 6: å¯¹ç”Ÿæˆçš„ç»“æœè¿›è¡Œå»é‡å¤„ç†ï¼Œè‹¥å­˜åœ¨å˜æ›´è¯¦æƒ…è¶…è¿‡80%çš„ï¼Œåˆ™è¿›è¡Œåˆå¹¶ï¼Œå˜æ›´ç‚¹å®Œå…¨ä¸€æ ·çš„å»å¤„é‡å¤
            final_change_analysis = await self._deduplicate_and_sort_changes(enhanced_change_analysis)

            self.logger.info(f"å»é‡æ’åºå®Œæˆ: {final_change_analysis}")
            
            # æ¸…ç†ç»“æ„åŒ–å†…å®¹å—ï¼Œç§»é™¤å‘é‡åµŒå…¥ä»¥å‡å°‘è¾“å‡ºå¤§å°
            cleaned_chunks = []
            for chunk in structured_chunks:
                cleaned_chunk = {
                    "section": chunk["section"],
                    "content": chunk["content"],
                    "level": chunk["level"],
                    "image_refs": chunk.get("image_refs", [])
                    # ä¸åŒ…å«embeddingå‘é‡
                }
                cleaned_chunks.append(cleaned_chunk)
            
            # åˆå¹¶åˆ†æç»“æœ
            content_result = {
                "change_analysis": final_change_analysis,
                "metadata": {
                    "analysis_method": "LLM+å‘é‡æ•°æ®åº“åˆ†æ+è¯¦ç»†å†…å®¹æå–+æ™ºèƒ½åˆå¹¶",
                    "analysis_time": time.time() - start_time,
                    "content_length": len(document_content),
                    "chunks_count": len(structured_chunks)
                }
            }
            self.logger.info(f"å†…å®¹åˆ†æç»“æœ: {content_result}")

            duration = time.time() - start_time
            self._log_analysis_complete(task_id, "å†…å®¹åˆ†æ", duration, len(str(content_result)))
            
            return self._create_response(
                success=True,
                data=content_result,
                metadata={"analysis_duration": duration}
            )
            
        except Exception as e:
            self._log_error(task_id, "å†…å®¹åˆ†æ", e)
            return self._create_response(
                success=False,
                error=f"å†…å®¹åˆ†æå¤±è´¥: {str(e)}"
            )
    
    async def _preprocess_document(self, document_content: str) -> List[Dict[str, Any]]:
        """
        Step 1: æ–‡æ¡£é¢„å¤„ç† - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒæ‰¹é‡å‘é‡åŒ–
        å°†markdownæ–‡æ¡£æ‹†è§£ä¸ºç»“æ„åŒ–å†…å®¹å—
        
        Args:
            document_content: markdownæ ¼å¼çš„æ–‡æ¡£å†…å®¹
            
        Returns:
            ç»“æ„åŒ–å†…å®¹å—åˆ—è¡¨
        """
        structured_chunks = []
        
        # æŒ‰è¡Œåˆ†å‰²æ–‡æ¡£
        lines = document_content.split('\n')
        current_section = ""
        current_content = []
        current_level = 0
        
        # ç¬¬ä¸€æ­¥ï¼šåˆ†å‰²æ–‡æ¡£ä½†ä¸ç”Ÿæˆå‘é‡
        temp_chunks = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æµ‹æ ‡é¢˜çº§åˆ«
            if line.startswith('#'):
                # ä¿å­˜ä¹‹å‰çš„å†…å®¹å—
                if current_section and current_content:
                    content_text = '\n'.join(current_content).strip()
                    if content_text:
                        temp_chunk = {
                            "section": current_section,
                            "content": content_text,
                            "level": current_level,
                            "text_for_embedding": f"{current_section}\n{content_text}",
                            "image_refs": self._extract_image_refs(content_text)
                        }
                        temp_chunks.append(temp_chunk)
                
                # å¼€å§‹æ–°çš„æ®µè½
                current_level = len(line) - len(line.lstrip('#'))
                current_section = line.lstrip('# ').strip()
                current_content = []
            else:
                # æ™®é€šå†…å®¹è¡Œ
                current_content.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
        if current_section and current_content:
            content_text = '\n'.join(current_content).strip()
            if content_text:
                temp_chunk = {
                    "section": current_section,
                    "content": content_text,
                    "level": current_level,
                    "text_for_embedding": f"{current_section}\n{content_text}",
                    "image_refs": self._extract_image_refs(content_text)
                }
                temp_chunks.append(temp_chunk)
        
        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡ç”Ÿæˆå‘é‡åµŒå…¥ï¼ˆæ€§èƒ½ä¼˜åŒ–å…³é”®ï¼‰
        if temp_chunks:
            try:
                self.logger.info(f"å¼€å§‹æ‰¹é‡ç”Ÿæˆå‘é‡åµŒå…¥ï¼Œå…± {len(temp_chunks)} ä¸ªæ®µè½")
                start_time = time.time()
                
                # æå–æ‰€æœ‰éœ€è¦å‘é‡åŒ–çš„æ–‡æœ¬
                texts_for_embedding = [chunk["text_for_embedding"] for chunk in temp_chunks]
                
                # ğŸš€ æ‰¹é‡ç”Ÿæˆå‘é‡ï¼ˆæ”¯æŒç¼“å­˜ï¼Œä¸€æ¬¡è°ƒç”¨å¤„ç†æ‰€æœ‰æ–‡æœ¬ï¼‰
                embeddings = self._batch_get_embeddings(texts_for_embedding)
                
                embedding_time = time.time() - start_time
                self.logger.info(f"æ‰¹é‡å‘é‡ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {embedding_time:.2f}ç§’ï¼Œå¹³å‡æ¯æ®µ: {embedding_time/len(temp_chunks):.3f}ç§’")
                
                # ç¬¬ä¸‰æ­¥ï¼šç»„è£…æœ€ç»ˆç»“æœ
                for i, chunk in enumerate(temp_chunks):
                    chunk_with_embedding = {
                        "section": chunk["section"],
                        "content": chunk["content"],
                        "level": chunk["level"],
                        "embedding": embeddings[i].tolist(),
                        "image_refs": chunk["image_refs"]
                    }
                    structured_chunks.append(chunk_with_embedding)
                    
            except Exception as e:
                self.logger.error(f"æ‰¹é‡å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªå¤„ç†: {e}")
                # å›é€€åˆ°åŸå§‹æ–¹æ³•
                for chunk in temp_chunks:
                    embedding = self.embedding_model.encode(chunk["text_for_embedding"]).tolist()
                    chunk_with_embedding = {
                        "section": chunk["section"],
                        "content": chunk["content"],
                        "level": chunk["level"],
                        "embedding": embedding,
                        "image_refs": chunk["image_refs"]
                    }
                    structured_chunks.append(chunk_with_embedding)
        
        return structured_chunks
    
    def _extract_image_refs(self, content: str) -> List[str]:
        """æå–å†…å®¹ä¸­çš„å›¾ç‰‡å¼•ç”¨"""
        image_refs = []
        # åŒ¹é…markdownå›¾ç‰‡è¯­æ³•: ![alt](path)
        image_pattern = re.compile(r'!\[.*?\]\((.*?)\)')
        matches = image_pattern.findall(content)
        image_refs.extend(matches)
        
        # åŒ¹é…å…¶ä»–å¯èƒ½çš„å›¾ç‰‡å¼•ç”¨æ ¼å¼
        ref_pattern = re.compile(r'\[å›¾ç‰‡[ï¼š:]\s*(.*?)\]')
        matches = ref_pattern.findall(content)
        image_refs.extend(matches)
        
        return image_refs
    
    async def _compare_with_history(self, structured_chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Step 2: ä¸å†å²çŸ¥è¯†åº“å†…å®¹å¯¹æ¯”
        
        Args:
            structured_chunks: ç»“æ„åŒ–å†…å®¹å—
            
        Returns:
            å†å²å¯¹æ¯”ç»“æœï¼ŒåŒ…å«å¯è¯»çš„å†å²æ–‡æ¡£ä¿¡æ¯
        """
        comparison_results = []
        deleted_items = []
        
        # Step 2-A: å†å²å†…å®¹åŒ¹é…ä¸ç›¸ä¼¼åº¦åˆ¤æ–­
        for chunk in structured_chunks:
            similar_history = await self._find_similar_history(chunk)
            
            if similar_history:
                max_similarity = max(item['similarity'] for item in similar_history)
                
                # æ ¼å¼åŒ–å†å²åŒ¹é…ä¿¡æ¯ï¼Œç¡®ä¿è¿”å›å¯è¯»å†…å®¹è€Œä¸æ˜¯å‘é‡
                formatted_history = []
                for item in similar_history:
                    formatted_item = {
                        "title": item.get('title', ''),
                        "file_path": item.get('file_path', ''),
                        "content": item.get('content', ''),
                        "similarity": item.get('similarity', 0.0),
                        "search_method": item.get('search_method', 'å‘é‡æœç´¢')
                    }
                    # ç§»é™¤embeddingç­‰éå¿…è¦å­—æ®µ
                    formatted_history.append(formatted_item)
                
                if max_similarity >= 0.4:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    comparison_results.append({
                        "current_chunk": {
                            "section": chunk['section'],
                            "content": chunk['content'],
                            "level": chunk['level']
                            # ä¸åŒ…å«embeddingå‘é‡
                        },
                        "matched_history": formatted_history,
                        "change_type": "ä¿®æ”¹" if max_similarity < 0.9 else "ç›¸åŒ",
                        "max_similarity": max_similarity
                    })
                else:
                    comparison_results.append({
                        "current_chunk": {
                            "section": chunk['section'],
                            "content": chunk['content'],
                            "level": chunk['level']
                        },
                        "matched_history": [],
                        "change_type": "æ–°å¢",
                        "max_similarity": max_similarity
                    })
            else:
                comparison_results.append({
                    "current_chunk": {
                        "section": chunk['section'],
                        "content": chunk['content'],
                        "level": chunk['level']
                    },
                    "matched_history": [],
                    "change_type": "æ–°å¢",
                    "max_similarity": 0.0
                })
        
        # Step 2-B: è¯†åˆ«åˆ é™¤é¡¹
        deleted_items = await self._identify_deleted_items(structured_chunks)
        
        return {
            "comparisons": comparison_results,
            "deleted_items": deleted_items,
            "summary": {
                "total_chunks": len(structured_chunks),
                "new_items": len([r for r in comparison_results if r["change_type"] == "æ–°å¢"]),
                "modified_items": len([r for r in comparison_results if r["change_type"] == "ä¿®æ”¹"]),
                "same_items": len([r for r in comparison_results if r["change_type"] == "ç›¸åŒ"]),
                "deleted_items": len(deleted_items)
            }
        }
    
    async def _find_similar_history(self, chunk: Dict[str, Any], top_k: int = 5) -> List[Dict[str, Any]]:
        """åœ¨å†å²çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ç›¸ä¼¼å†…å®¹"""
        try:
            collection = self.weaviate_client.collections.get("Document")
            
            # ä½¿ç”¨å‘é‡æœç´¢ - éœ€è¦å¤„ç†å‘é‡æ ¼å¼å·®å¼‚
            query_vector = chunk['embedding']  # 1024ç»´å‘é‡
            
            # æ£€æŸ¥å†å²æ•°æ®çš„å‘é‡æ ¼å¼
            # å…ˆå°è¯•è·å–ä¸€ä¸ªç¤ºä¾‹æ¥äº†è§£å‘é‡æ ¼å¼
            sample_response = collection.query.fetch_objects(limit=1, include_vector=True)
            
            if sample_response.objects and sample_response.objects[0].vector:
                sample_vector = sample_response.objects[0].vector
                
                # æ£€æŸ¥å‘é‡æ ¼å¼
                if isinstance(sample_vector, dict) and 'default' in sample_vector:
                    # å¦‚æœå†å²æ•°æ®ä½¿ç”¨ {'default': [vector]} æ ¼å¼
                    actual_vector = sample_vector['default']
                    sample_dim = len(actual_vector)
                    
                    self.logger.info(f"å†å²æ•°æ®å‘é‡ç»´åº¦: {sample_dim}, å½“å‰åˆ†æå‘é‡ç»´åº¦: {len(query_vector)}")
                    
                    if sample_dim != len(query_vector):
                        # ç»´åº¦ä¸åŒ¹é…ï¼Œè·³è¿‡å‘é‡æœç´¢ï¼Œä½¿ç”¨å…³é”®è¯æœç´¢æ›¿ä»£
                        self.logger.info(f"å‘é‡ç»´åº¦ä¸åŒ¹é… (å†å²: {sample_dim}, å½“å‰: {len(query_vector)})ï¼Œè·³è¿‡å‘é‡æœç´¢")
                        return await self._fallback_keyword_search(chunk, top_k)
                
                # å¦‚æœç»´åº¦åŒ¹é…ï¼Œç»§ç»­å‘é‡æœç´¢
                response = collection.query.near_vector(
                    near_vector=query_vector,
                    limit=top_k,
                    return_metadata=['distance']
                )
            else:
                # æ²¡æœ‰å†å²æ•°æ®æˆ–æ²¡æœ‰å‘é‡ï¼Œè¿”å›ç©ºç»“æœ
                return []
            
            similar_items = []
            for obj in response.objects:
                similarity = 1 - obj.metadata.distance  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                similar_items.append({
                    "content": obj.properties.get('content', ''),
                    "title": obj.properties.get('title', ''),
                    "file_path": obj.properties.get('file_path', ''),
                    "similarity": similarity
                })
            
            return similar_items
            
        except Exception as e:
            self.logger.error(f"å†å²å†…å®¹æœç´¢å¤±è´¥: {e}")
            # é™çº§åˆ°å…³é”®è¯æœç´¢
            return await self._fallback_keyword_search(chunk, top_k)
    
    async def _fallback_keyword_search(self, chunk: Dict[str, Any], top_k: int = 5) -> List[Dict[str, Any]]:
        """å½“å‘é‡æœç´¢å¤±è´¥æ—¶ï¼Œä½¿ç”¨å…³é”®è¯æœç´¢ä½œä¸ºé™çº§æ–¹æ¡ˆ"""
        try:
            collection = self.weaviate_client.collections.get("AnalyDesignDocuments")
            
            # ä½¿ç”¨BM25å…³é”®è¯æœç´¢
            search_text = f"{chunk['section']} {chunk['content']}"
            
            response = collection.query.bm25(
                query=search_text,
                limit=top_k,
                return_metadata=['score']
            )
            
            similar_items = []
            for obj in response.objects:
                # BM25å¾—åˆ†è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆç®€å•æ˜ å°„ï¼‰
                similarity = min(obj.metadata.score / 10.0, 1.0) if obj.metadata.score else 0.1
                similar_items.append({
                    "content": obj.properties.get('content', ''),
                    "title": obj.properties.get('title', ''),
                    "file_path": obj.properties.get('file_path', ''),
                    "similarity": similarity,
                    "search_method": "BM25å…³é”®è¯æœç´¢"
                })
            
            self.logger.info(f"ä½¿ç”¨å…³é”®è¯æœç´¢æ‰¾åˆ° {len(similar_items)} ä¸ªç›¸ä¼¼é¡¹")
            return similar_items
            
        except Exception as e:
            self.logger.error(f"å…³é”®è¯æœç´¢ä¹Ÿå¤±è´¥: {e}")
            return []
    
    async def _identify_deleted_items(self, structured_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«åˆ é™¤é¡¹"""
        deleted_items = []
        
        # åœ¨å½“å‰æ–‡æ¡£ä¸­æœç´¢åˆ é™¤ç›¸å…³çš„å…³é”®è¯
        delete_keywords = [
            r"åˆ é™¤[äº†]?(.+?)åŠŸèƒ½",
            r"å»é™¤[äº†]?(.+?)æ¥å£",
            r"å–æ¶ˆ[äº†]?(.+?)æœåŠ¡",
            r"ç§»é™¤[äº†]?(.+)",
            r"ä¸å†(.+)",
            r"~~(.+?)~~"  # markdownåˆ é™¤çº¿
        ]
        
        for chunk in structured_chunks:
            content = chunk['content']
            section = chunk['section']
            
            for pattern in delete_keywords:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    deleted_items.append({
                        "section": section,
                        "deleted_item": match.strip(),
                        "context": content,
                        "detection_method": "å…³é”®è¯åŒ¹é…"
                    })
        
        return deleted_items
    
    async def _analyze_changes(self, structured_chunks: List[Dict[str, Any]], 
                             history_comparison: Dict[str, Any], document_content: str) -> Dict[str, Any]:
        """
        Step 3: å¤§æ¨¡å‹å˜æ›´åˆ¤æ–­ä¸ç»“æ„åŒ–è¾“å‡º
        
        Args:
            structured_chunks: ç»“æ„åŒ–å†…å®¹å—
            history_comparison: å†å²å¯¹æ¯”ç»“æœ
            document_content: æ–‡æ¡£å†…å®¹
            
        Returns:
            å˜æ›´åˆ†æç»“æœ
        """
        change_analyses = []
        
        for comparison in history_comparison['comparisons']:
            if comparison['change_type'] in ['ä¿®æ”¹', 'æ–°å¢'] and comparison['matched_history']:
                # å¯¹æœ‰å†å²åŒ¹é…çš„å†…å®¹è¿›è¡Œè¯¦ç»†åˆ†æ
                analysis = await self._llm_change_analysis(comparison, document_content)
                if analysis:
                    change_analyses.append(analysis)
        
        # å¯¹åˆ é™¤é¡¹è¿›è¡Œåˆ†æ
        deletion_analyses = []
        for deleted_item in history_comparison['deleted_items']:
            analysis = await self._llm_deletion_analysis(deleted_item, document_content)
            if analysis:
                deletion_analyses.append(analysis)
        
        return {
            "change_analyses": change_analyses,
            "deletion_analyses": deletion_analyses,
            "summary": {
                "total_changes": len(change_analyses),
                "total_deletions": len(deletion_analyses),
                "analysis_method": "LLMæ™ºèƒ½åˆ†æ"
            }
        }
    
    async def _llm_change_analysis(self, comparison: Dict[str, Any], document_content: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMåˆ†æå˜æ›´"""
        current_content = f"{comparison['current_chunk']['section']}\n{comparison['current_chunk']['content']}"
        self.logger.info(f"ä½¿ç”¨LLMåˆ†æå˜æ›´: {current_content}")

        # æ ¼å¼åŒ–å†å²å†…å®¹ä¿¡æ¯ï¼Œæä¾›æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡
        history_context = ""
        history_files = []
        self.logger.info(f"ä½¿ç”¨LLMçš„coparison: {comparison}")
        if comparison['matched_history']:
            history_items = []
            for idx, item in enumerate(comparison['matched_history'][:3]):  # å–å‰3ä¸ªæœ€ç›¸ä¼¼çš„
                history_item = f"""
å†å²æ–‡æ¡£{idx+1}:
- æ–‡ä»¶: {item.get('file_path', 'æœªçŸ¥æ–‡ä»¶')}
- æ ‡é¢˜: {item.get('title', 'æœªçŸ¥æ ‡é¢˜')}
- ç›¸ä¼¼åº¦: {item.get('similarity', 0):.2f}
- å†…å®¹: {item.get('content', '')}
"""
                history_items.append(history_item)
                history_files.append(item.get('file_path', ''))
            
            history_context = "\n".join(history_items)
        else:
            history_context = "æ— åŒ¹é…çš„å†å²æ–‡æ¡£"
        
        system_prompt = """ä½ æ˜¯ä¸“ä¸šçš„éœ€æ±‚æ–‡æ¡£å˜æ›´åˆ†æä¸“å®¶ã€‚è¯·ä»”ç»†æ¯”è¾ƒå½“å‰éœ€æ±‚å†…å®¹ä¸å†å²éœ€æ±‚å†…å®¹ï¼Œè¯†åˆ«å‡ºå…·ä½“çš„å˜æ›´ç‚¹ï¼Œå¹¶æŒ‰ç…§æŒ‡å®šçš„JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœã€‚

åˆ†æè¦ç‚¹ï¼š
1. å¯¹æ¯”åŠŸèƒ½éœ€æ±‚çš„å·®å¼‚
2. è¯†åˆ«æ–°å¢ã€ä¿®æ”¹ã€åˆ é™¤çš„å…·ä½“åŠŸèƒ½ç‚¹
3. åˆ†æä¸šåŠ¡é€»è¾‘çš„å˜åŒ–
4. æä¾›æ¸…æ™°çš„å˜æ›´è¯´æ˜"""
        
        user_prompt = f"""
ã€å®Œæ•´æ–‡æ¡£å†…å®¹ã€‘ï¼š
{document_content}

ã€å½“å‰ç‰ˆæœ¬éœ€æ±‚å†…å®¹ã€‘ï¼š
{current_content}

ã€å†å²ç‰ˆæœ¬éœ€æ±‚å†…å®¹ã€‘ï¼š
{history_context}

è¯·ä»”ç»†åˆ†æå½“å‰éœ€æ±‚ä¸å†å²éœ€æ±‚çš„å·®å¼‚ï¼Œç»“åˆå®Œæ•´æ–‡æ¡£å†…å®¹çš„ä¸Šä¸‹æ–‡ï¼ŒæŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š

{{
    "changeType": "æ–°å¢ | ä¿®æ”¹ | åˆ é™¤",
    "changeReason": "è¯¦ç»†è¯´æ˜å˜æ›´çš„åŸå› å’Œåˆ¤æ–­ä¾æ®",
    "changeItems": ["å…·ä½“å˜æ›´ç‚¹1", "å…·ä½“å˜æ›´ç‚¹2", "å…·ä½“å˜æ›´ç‚¹3"],
    "version": {history_files[:1] if history_files else ["æ— å†å²ç‰ˆæœ¬"]}
}}

æ³¨æ„ï¼š
- è¯·ç»“åˆå®Œæ•´æ–‡æ¡£å†…å®¹ç†è§£å½“å‰éœ€æ±‚çš„ä½ç½®å’Œä½œç”¨
- changeItemsåº”è¯¥åŒ…å«å…·ä½“çš„åŠŸèƒ½ç‚¹å˜æ›´æè¿°
- changeReasonè¦è¯´æ˜ä¸ºä»€ä¹ˆåˆ¤æ–­ä¸ºå˜æ›´ç±»å‹
- å¦‚æœæ˜¯ä¿®æ”¹ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºä¿®æ”¹å‰åçš„å·®å¼‚
"""
        
        try:
            response = await self._call_llm(user_prompt, system_prompt, max_tokens=12000)
            if response:
                # å°è¯•è§£æJSONå“åº”
                try:
                    # å…ˆæ¸…ç†å“åº”å†…å®¹
                    cleaned_response = self._clean_json_response(response)
                    analysis_result = json.loads(cleaned_response)
                    return analysis_result
                except json.JSONDecodeError as e:
                    self.logger.warning(f"LLMå“åº”JSONè§£æå¤±è´¥: {str(e)}")
                    self.logger.debug(f"åŸå§‹å“åº”å‰500å­—ç¬¦: {response[:500]}")
                    
                    # å°è¯•ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
                    try:
                        fixed_response = self._fix_incomplete_json(response)
                        analysis_result = json.loads(fixed_response)
                        self.logger.info("æˆåŠŸä¿®å¤å¹¶è§£æJSONå“åº”")
                        return analysis_result
                    except:
                        self.logger.warning("JSONä¿®å¤å¤±è´¥ï¼Œè¿”å›æ ¼å¼åŒ–çš„åŸå§‹å“åº”")
                        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›æ ¼å¼åŒ–çš„åŸå§‹å“åº”
                        return {
                            "changeType": comparison['change_type'],
                            "changeReason": "LLMåˆ†æå®Œæˆï¼Œä½†JSONæ ¼å¼è§£æå¤±è´¥",
                            "changeItems": [f"LLMåŸå§‹åˆ†æ: {response[:300]}{'...' if len(response) > 300 else ''}"],
                            "version": history_files[:1] if history_files else ["æ— å†å²ç‰ˆæœ¬"]
                        }
        except Exception as e:
            self.logger.error(f"LLMå˜æ›´åˆ†æå¤±è´¥: {e}")
        
        return None
    
    def _clean_json_response(self, response: str) -> str:
        """
        æ¸…ç†LLMå“åº”ï¼Œæå–JSONå†…å®¹ï¼ˆæ”¯æŒæ•°ç»„å’Œå¯¹è±¡ï¼‰
        
        Args:
            response: LLMåŸå§‹å“åº”
            
        Returns:
            æ¸…ç†åçš„JSONå­—ç¬¦ä¸²
        """
        # ç§»é™¤ä»£ç å—æ ‡è®°
        response = re.sub(r'```json\s*', '', response)
        response = re.sub(r'\s*```$', '', response)
        response = re.sub(r'```.*$', '', response, flags=re.MULTILINE)
        
        # ç§»é™¤é¢å¤–çš„ç©ºç™½å­—ç¬¦
        response = response.strip()
        
        # ä¼˜å…ˆæŸ¥æ‰¾JSONæ•°ç»„çš„å¼€å§‹å’Œç»“æŸ
        array_start = response.find('[')
        object_start = response.find('{')
        
        # å¦‚æœåŒæ—¶å­˜åœ¨æ•°ç»„å’Œå¯¹è±¡ï¼Œé€‰æ‹©æœ€å…ˆå‡ºç°çš„
        if array_start != -1 and (object_start == -1 or array_start < object_start):
            # å¤„ç†JSONæ•°ç»„
            start_idx = array_start
            bracket_count = 0
            brace_count = 0
            end_idx = -1
            
            for i in range(start_idx, len(response)):
                if response[i] == '[':
                    bracket_count += 1
                elif response[i] == ']':
                    bracket_count -= 1
                    if bracket_count == 0:
                        end_idx = i
                        break
                elif response[i] == '{':
                    brace_count += 1
                elif response[i] == '}':
                    brace_count -= 1
            
            if end_idx != -1:
                response = response[start_idx:end_idx + 1]
        elif object_start != -1:
            # å¤„ç†JSONå¯¹è±¡
            start_idx = object_start
            brace_count = 0
            end_idx = -1
            
            for i in range(start_idx, len(response)):
                if response[i] == '{':
                    brace_count += 1
                elif response[i] == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        end_idx = i
                        break
            
            if end_idx != -1:
                response = response[start_idx:end_idx + 1]
        
        return response
    
    def _fix_incomplete_json(self, response: str) -> str:
        """
        å°è¯•ä¿®å¤ä¸å®Œæ•´çš„JSONå“åº”
        
        Args:
            response: å¯èƒ½ä¸å®Œæ•´çš„JSONå­—ç¬¦ä¸²
            
        Returns:
            ä¿®å¤åçš„JSONå­—ç¬¦ä¸²
        """
        try:
            # å…ˆæ¸…ç†å“åº”
            cleaned = self._clean_json_response(response)
            
            # ç»Ÿè®¡æ‹¬å·å’Œå¼•å·çš„æ•°é‡
            open_braces = cleaned.count('{')
            close_braces = cleaned.count('}')
            open_brackets = cleaned.count('[')
            close_brackets = cleaned.count(']')
            
            # ä¿®å¤ç¼ºå¤±çš„ç»“æŸæ‹¬å·
            while close_braces < open_braces:
                cleaned += '}'
                close_braces += 1
            
            while close_brackets < open_brackets:
                cleaned += ']'
                close_brackets += 1
            
            # æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦æ­£ç¡®é—­åˆ
            lines = cleaned.split('\n')
            fixed_lines = []
            
            for line in lines:
                # æ£€æŸ¥æ˜¯å¦æœ‰æœªé—­åˆçš„å­—ç¬¦ä¸²
                if line.count('"') % 2 == 1:
                    # å¥‡æ•°ä¸ªå¼•å·ï¼Œå¯èƒ½ç¼ºå°‘ç»“æŸå¼•å·
                    if not line.rstrip().endswith('"') and not line.rstrip().endswith('",'):
                        line = line.rstrip() + '"'
                
                fixed_lines.append(line)
            
            fixed_response = '\n'.join(fixed_lines)
            
            # ç¡®ä¿æœ€åçš„è¯­æ³•æ­£ç¡®
            if not fixed_response.rstrip().endswith('}'):
                fixed_response = fixed_response.rstrip() + '}'
            
            return fixed_response
            
        except Exception as e:
            self.logger.error(f"JSONä¿®å¤è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            # å¦‚æœä¿®å¤å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬çš„æœ‰æ•ˆJSON
            return '{"error": "JSONæ ¼å¼ä¿®å¤å¤±è´¥"}'
    
    async def _llm_deletion_analysis(self, deleted_item: Dict[str, Any], document_content: str) -> Dict[str, Any]:
        """ä½¿ç”¨LLMåˆ†æåˆ é™¤é¡¹"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚æ–‡æ¡£åˆ†æå¸ˆï¼Œè¯·åˆ†ææ–‡æ¡£ä¸­æåˆ°çš„åˆ é™¤é¡¹ï¼Œå¹¶ç¡®è®¤å…¶å†å²å­˜åœ¨æ€§ã€‚"""
        
        user_prompt = f"""
ã€å®Œæ•´æ–‡æ¡£å†…å®¹ã€‘ï¼š
{document_content}

ã€åˆ é™¤é¡¹ä¿¡æ¯ã€‘ï¼š
ç« èŠ‚: {deleted_item['section']}
åˆ é™¤å†…å®¹: {deleted_item['deleted_item']}
ä¸Šä¸‹æ–‡: {deleted_item['context']}

è¯·ç»“åˆå®Œæ•´æ–‡æ¡£å†…å®¹åˆ†æè¯¥åˆ é™¤é¡¹çš„åˆç†æ€§ï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

{{
    "changeType": "åˆ é™¤",
    "deletedItem": "{deleted_item['deleted_item']}",
    "section": "{deleted_item['section']}",
    "analysisResult": "åˆ é™¤åŸå› å’Œå½±å“åˆ†æ"
}}
"""
        
        try:
            response = await self._call_llm(user_prompt, system_prompt, max_tokens=12000)
            if response:
                try:
                    # å…ˆæ¸…ç†å“åº”å†…å®¹
                    cleaned_response = self._clean_json_response(response)
                    analysis_result = json.loads(cleaned_response)
                    return analysis_result
                except json.JSONDecodeError:
                    # å°è¯•ä¿®å¤JSONæ ¼å¼
                    try:
                        fixed_response = self._fix_incomplete_json(response)
                        analysis_result = json.loads(fixed_response)
                        return analysis_result
                    except:
                        return {
                            "changeType": "åˆ é™¤",
                            "deletedItem": deleted_item['deleted_item'],
                            "section": deleted_item['section'],
                            "analysisResult": response[:200] + "..."
                        }
        except Exception as e:
            self.logger.error(f"LLMåˆ é™¤åˆ†æå¤±è´¥: {e}")
        
        return None
    
    async def _extract_detailed_changes(self, change_analysis: Dict[str, Any], document_content: str) -> Dict[str, Any]:
        """
        Step 4: å¯¹å˜æ›´é¡¹ä»åŸæ–‡æ¡£æå–è¯¦ç»†å˜æ›´ç‚¹
        å¤„ç†æ•´ä¸ªå˜æ›´åˆ†æç»“æœï¼Œä¸ºæ¯ä¸ªå˜æ›´é¡¹æå–è¯¦ç»†å†…å®¹
        
        Args:
            change_analysis: Step3çš„å®Œæ•´åˆ†æç»“æœ
            document_content: å®Œæ•´çš„å½“å‰æ–‡æ¡£å†…å®¹
            
        Returns:
            å¢å¼ºåçš„å®Œæ•´å˜æ›´åˆ†æç»“æœï¼ŒåŒ…å«è¯¦ç»†å˜æ›´å†…å®¹
        """
        try:
            enhanced_analysis = change_analysis.copy()
            
            # å¤„ç†change_analysesåˆ—è¡¨
            if 'change_analyses' in change_analysis and change_analysis['change_analyses']:
                enhanced_change_analyses = []
                for analysis in change_analysis['change_analyses']:
                    enhanced_analysis_item = await self._extract_single_change_details(analysis, document_content)
                    enhanced_change_analyses.append(enhanced_analysis_item)
                enhanced_analysis['change_analyses'] = enhanced_change_analyses
            
            # å¤„ç†deletion_analysesåˆ—è¡¨
            if 'deletion_analyses' in change_analysis and change_analysis['deletion_analyses']:
                enhanced_deletion_analyses = []
                for analysis in change_analysis['deletion_analyses']:
                    enhanced_analysis_item = await self._extract_single_change_details(analysis, document_content)
                    enhanced_deletion_analyses.append(enhanced_analysis_item)
                enhanced_analysis['deletion_analyses'] = enhanced_deletion_analyses
            
            # æ›´æ–°summaryä¿¡æ¯
            if 'summary' in enhanced_analysis:
                enhanced_analysis['summary']['analysis_method'] = "LLMæ™ºèƒ½åˆ†æ+è¯¦ç»†å†…å®¹æå–"
            
            return enhanced_analysis
            
        except Exception as e:
            self.logger.error(f"è¯¦ç»†å˜æ›´å†…å®¹æå–å¤±è´¥: {e}")
            # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›åŸå§‹åˆ†æç»“æœå¹¶æ·»åŠ é”™è¯¯ä¿¡æ¯
            error_analysis = change_analysis.copy() if change_analysis else {}
            error_analysis['step4_error'] = f"è¯¦ç»†å†…å®¹æå–å¤±è´¥: {str(e)}"
            return error_analysis
    
    async def _extract_single_change_details(self, change_analysis: Dict[str, Any], document_content: str) -> Dict[str, Any]:
        """
        å¯¹å•ä¸ªå˜æ›´åˆ†æç»“æœæå–è¯¦ç»†å†…å®¹
        
        Args:
            change_analysis: å•ä¸ªå˜æ›´åˆ†æç»“æœ
            document_content: å®Œæ•´çš„å½“å‰æ–‡æ¡£å†…å®¹
            
        Returns:
            å¢å¼ºåçš„å•ä¸ªå˜æ›´åˆ†æç»“æœï¼ŒåŒ…å«è¯¦ç»†å˜æ›´å†…å®¹
        """
        try:
            # æ£€æŸ¥åˆ†æç»“æœçš„ç»“æ„
            if not change_analysis or not isinstance(change_analysis, dict):
                return change_analysis
            
            enhanced_analysis = change_analysis.copy()
            
            # å¤„ç†current_changeåˆ—è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰
            if 'current_change' in change_analysis and isinstance(change_analysis['current_change'], list):
                enhanced_changes = []
                
                for change_item in change_analysis['current_change']:
                    enhanced_change = change_item.copy()
                    
                    # æå–changeItemsä¸­çš„å˜æ›´ç‚¹è¯¦ç»†å†…å®¹
                    if 'changeItems' in change_item and change_item['changeItems']:
                        change_items = change_item['changeItems']
                        if isinstance(change_items, list):
                            change_items_text = ', '.join(change_items)
                        else:
                            change_items_text = str(change_items)
                        
                        # ä½¿ç”¨LLMä»åŸæ–‡æ¡£ä¸­æå–è¯¦ç»†å†…å®¹
                        detailed_content = await self._llm_extract_details(change_items_text, document_content, change_item)
                        enhanced_change['changeDetails'] = detailed_content
                    else:
                        enhanced_change['changeDetails'] = "æ— å…·ä½“å˜æ›´é¡¹å¯æå–"
                    
                    enhanced_changes.append(enhanced_change)
                
                enhanced_analysis['current_change'] = enhanced_changes
            
            # å¤„ç†æ–°æ ¼å¼ï¼šç›´æ¥çš„å˜æ›´å¯¹è±¡ï¼ˆæœ‰changeItemså­—æ®µï¼‰
            elif 'changeItems' in change_analysis and change_analysis['changeItems']:
                change_items = change_analysis['changeItems']
                if isinstance(change_items, list):
                    change_items_text = ', '.join(change_items)
                else:
                    change_items_text = str(change_items)
                
                # ä½¿ç”¨LLMä»åŸæ–‡æ¡£ä¸­æå–è¯¦ç»†å†…å®¹
                detailed_content = await self._llm_extract_details(change_items_text, document_content, change_analysis)
                enhanced_analysis['changeDetails'] = detailed_content
            
            # å¤„ç†å…¶ä»–å¯èƒ½çš„ç»“æ„ï¼ˆå¦‚åˆ é™¤åˆ†æï¼‰
            elif 'changeType' in change_analysis:
                deleted_item = change_analysis.get('deletedItem', '')
                if deleted_item:
                    detailed_content = await self._llm_extract_details(deleted_item, document_content, change_analysis)
                    enhanced_analysis['changeDetails'] = detailed_content
                else:
                    enhanced_analysis['changeDetails'] = "æ— å…·ä½“åˆ é™¤é¡¹å¯æå–"
            
            return enhanced_analysis
            
        except Exception as e:
            self.logger.error(f"å•ä¸ªå˜æ›´è¯¦ç»†å†…å®¹æå–å¤±è´¥: {e}")
            # å¦‚æœæå–å¤±è´¥ï¼Œè¿”å›åŸå§‹åˆ†æç»“æœå¹¶æ·»åŠ é”™è¯¯ä¿¡æ¯
            enhanced_analysis = change_analysis.copy() if change_analysis else {}
            enhanced_analysis['changeDetails'] = f"è¯¦ç»†å†…å®¹æå–å¤±è´¥: {str(e)}"
            return enhanced_analysis
    
    async def _llm_extract_details(self, change_items_text: str, document_content: str, change_context: Dict[str, Any]) -> str:
        """
        ä½¿ç”¨LLMä»åŸæ–‡æ¡£ä¸­æå–å˜æ›´ç‚¹çš„è¯¦ç»†å†…å®¹
        
        Args:
            change_items_text: å˜æ›´ç‚¹æ–‡æœ¬
            document_content: å®Œæ•´æ–‡æ¡£å†…å®¹
            change_context: å˜æ›´ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            æå–çš„è¯¦ç»†å†…å®¹
        """
        system_prompt = """ä½ æ˜¯ä¸€åä¸“ä¸šçš„éœ€æ±‚æ–‡æ¡£åˆ†æå¸ˆï¼Œä¸“é—¨è´Ÿè´£ä»éœ€æ±‚æ–‡æ¡£ä¸­æå–ç‰¹å®šå˜æ›´ç‚¹çš„è¯¦ç»†å†…å®¹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. æ ¹æ®ç»™å®šçš„å˜æ›´ç‚¹ï¼Œåœ¨å®Œæ•´æ–‡æ¡£ä¸­æŸ¥æ‰¾ç›¸å…³çš„è¯¦ç»†éœ€æ±‚æè¿°
2. æå–å®Œæ•´çš„å­—æ®µå®šä¹‰ã€ä¸šåŠ¡è§„åˆ™ã€æ¶‰åŠæƒé™ã€æŠ€æœ¯è§„èŒƒç­‰
3. ç¡®ä¿æå–çš„å†…å®¹å…·æœ‰å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
4. å¯¹äºå¤æ‚å˜æ›´ï¼Œæä¾›å……åˆ†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯"""

        change_type = change_context.get('changeType', 'æœªçŸ¥')
        change_reason = change_context.get('changeReason', '')
        
        user_prompt = f"""
ã€å˜æ›´ç±»å‹ã€‘ï¼š{change_type}
ã€å˜æ›´åŸå› ã€‘ï¼š{change_reason}
ã€å˜æ›´ç‚¹ã€‘ï¼š{change_items_text}

ã€å®Œæ•´éœ€æ±‚æ–‡æ¡£ã€‘ï¼š
{document_content}

è¯·æ ¹æ®ä¸Šè¿°å˜æ›´ç‚¹ï¼Œä»å®Œæ•´éœ€æ±‚æ–‡æ¡£ä¸­æå–ç›¸å…³çš„è¯¦ç»†å†…å®¹ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š
- å…·ä½“çš„åŠŸèƒ½æè¿°
- å­—æ®µå®šä¹‰å’Œæ•°æ®ç»“æ„
- ä¸šåŠ¡è§„åˆ™å’Œé€»è¾‘
- æ¶‰åŠæƒé™
- æ¥å£è§„èŒƒ
- æµç¨‹è¯´æ˜
- çº¦æŸæ¡ä»¶ã€é™åˆ¶

è¦æ±‚ï¼š
1. æå–çš„å†…å®¹å¿…é¡»ä¸å˜æ›´ç‚¹ç›´æ¥ç›¸å…³
2. ä¿æŒå†…å®¹çš„å®Œæ•´æ€§ï¼Œä¸è¦æˆªæ–­é‡è¦ä¿¡æ¯
3. å¦‚æœæ¶‰åŠå¤šä¸ªç›¸å…³éƒ¨åˆ†ï¼Œè¯·éƒ½åŒ…å«è¿›æ¥
4. å¦‚æœåœ¨ä¸Šä¸€æ­¥åŠä¹‹å‰æ­¥éª¤æå–çš„å˜æ›´è¯¦æƒ…å·²ç»å‡ºç°è¿‡ç›¸ä¼¼å†…å®¹ï¼Œå½“å‰å˜æ›´è¯¦æƒ…ä¸è¦é‡å¤æå–
5. ç”¨æ¸…æ™°çš„ç»“æ„ç»„ç»‡æå–çš„å†…å®¹
6. å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³å†…å®¹ï¼Œè¯·æ˜ç¡®è¯´æ˜
7. å¦‚æœæ˜¯å‰ç«¯æ–°å¢é¡¹ï¼Œè¯·å°†åŸæ–‡æ¡£ä¸­å¯¹åº”çš„é¡µé¢æˆªå›¾è·¯å¾„é™„ä¸Š

è¯·ç›´æ¥è¿”å›æå–çš„è¯¦ç»†å†…å®¹ï¼Œä¸éœ€è¦JSONæ ¼å¼ã€‚
"""
        
        try:
            response = await self._call_llm(user_prompt, system_prompt, max_tokens=12000)
            if response and response.strip():
                return response.strip()
            else:
                return f"æœªèƒ½ä»æ–‡æ¡£ä¸­æ‰¾åˆ°å˜æ›´ç‚¹ã€Œ{change_items_text}ã€çš„è¯¦ç»†å†…å®¹"
                
        except Exception as e:
            self.logger.error(f"LLMè¯¦ç»†å†…å®¹æå–å¤±è´¥: {e}")
            return f"è¯¦ç»†å†…å®¹æå–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
    
    async def _merge_similar_changes(self, change_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Step 3.5: æ™ºèƒ½åˆå¹¶ç›¸ä¼¼çš„å˜æ›´é¡¹
        
        Args:
            change_analysis: Step3çš„å˜æ›´åˆ†æç»“æœ
            
        Returns:
            åˆå¹¶åçš„å˜æ›´åˆ†æç»“æœ
        """
        try:
            merged_analysis = change_analysis.copy()
            
            # å¤„ç†change_analysesåˆ—è¡¨
            if 'change_analyses' in change_analysis and change_analysis['change_analyses']:
                original_changes = change_analysis['change_analyses']
                
                # é€‰æ‹©åˆå¹¶ç­–ç•¥ï¼šå¯ä»¥æ ¹æ®é…ç½®é€‰æ‹©ä¸åŒç­–ç•¥
                merge_strategy = "llm_smart"  # å¯é€‰: "similarity", "keyword", "llm_smart", "rule_based"
                
                if merge_strategy == "llm_smart":
                    merged_changes = await self._llm_smart_merge(original_changes)
                elif merge_strategy == "similarity":
                    merged_changes = await self._similarity_based_merge(original_changes)
                elif merge_strategy == "keyword":
                    merged_changes = await self._keyword_based_merge(original_changes)
                else:
                    merged_changes = await self._rule_based_merge(original_changes)
                
                merged_analysis['change_analyses'] = merged_changes
                
                # æ›´æ–°summary
                if 'summary' in merged_analysis:
                    merged_analysis['summary']['total_changes'] = len(merged_changes)
                    merged_analysis['summary']['merge_info'] = {
                        "original_count": len(original_changes),
                        "merged_count": len(merged_changes),
                        "reduction_ratio": round((len(original_changes) - len(merged_changes)) / len(original_changes) * 100, 2) if len(original_changes) > 0 else 0,
                        "strategy": merge_strategy
                    }
            
            return merged_analysis
            
        except Exception as e:
            self.logger.error(f"ç›¸ä¼¼å˜æ›´é¡¹åˆå¹¶å¤±è´¥: {e}")
            return change_analysis
    
    async def _llm_smart_merge(self, changes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ–¹æ¡ˆ1: åŸºäºLLMçš„æ™ºèƒ½åˆå¹¶"""
        if len(changes) <= 1:
            return changes
        
        # Step 1: é¢„å¤„ç† - å…ˆä¿®æ­£å˜æ›´ç±»å‹
        preprocessed_changes = await self._preprocess_change_types(changes)
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„éœ€æ±‚åˆ†æå¸ˆï¼Œè´Ÿè´£åˆå¹¶ç›¸ä¼¼çš„å˜æ›´é¡¹ã€‚

ä½ çš„ä»»åŠ¡ï¼š
1. è¯†åˆ«åŠŸèƒ½ç›¸å…³ã€ä¸»é¢˜ç›¸ä¼¼çš„å˜æ›´é¡¹
2. å°†ç›¸ä¼¼çš„å˜æ›´é¡¹åˆå¹¶æˆä¸€ä¸ªæ›´å®Œæ•´çš„å˜æ›´é¡¹
3. ä¿æŒå˜æ›´é¡¹çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
4. é¿å…ä¿¡æ¯ä¸¢å¤±"""
        
        changes_text = []
        for i, change in enumerate(preprocessed_changes):  # ä½¿ç”¨é¢„å¤„ç†åçš„æ•°æ®
            change_info = f"""å˜æ›´é¡¹{i+1}:
- ç±»å‹: {change.get('changeType', 'æœªçŸ¥')}
- åŸå› : {change.get('changeReason', 'æœªçŸ¥')}
- å˜æ›´ç‚¹: {change.get('changeItems', [])}
- ç‰ˆæœ¬: {change.get('version', [])}
"""
            changes_text.append(change_info)
        
        user_prompt = f"""
è¯·åˆ†æä»¥ä¸‹å˜æ›´é¡¹ï¼ŒæŒ‰ç…§å˜æ›´ç±»å‹å’ŒåŠŸèƒ½æ¨¡å—è¿›è¡Œåˆç†åˆ†ç±»åˆå¹¶ï¼š

{chr(10).join(changes_text)}

ã€ä¿å®ˆåˆå¹¶åŸåˆ™ã€‘ï¼š
1. **å˜æ›´ç±»å‹ä¸¥æ ¼åˆ†ç¦»**ï¼šä¸åŒå˜æ›´ç±»å‹ï¼ˆæ–°å¢/ä¿®æ”¹/åˆ é™¤ï¼‰ç»å¯¹ä¸èƒ½åˆå¹¶
2. **åŠŸèƒ½å­æ¨¡å—ç»†åˆ†**ï¼šå³ä½¿åœ¨åŒä¸€åŠŸèƒ½æ¨¡å—å†…ï¼Œä¹Ÿè¦æŒ‰å…·ä½“åŠŸèƒ½å­æ¨¡å—è¿›è¡Œç»†åˆ†
3. **é‡è¦åŠŸèƒ½ä¿ç•™**ï¼šé‡è¦çš„å…·ä½“åŠŸèƒ½ï¼ˆå¦‚å¯¼å‡ºã€æŸ¥è¯¢ã€å®šæ—¶ä»»åŠ¡ã€ç­›é€‰ç­‰ï¼‰å¿…é¡»ä¿ç•™ï¼Œä¸èƒ½è¢«åˆå¹¶æ‰
4. **é¿å…è¿‡åº¦åˆå¹¶**ï¼šåªæœ‰é«˜åº¦ç›¸ä¼¼ä¸”é‡å¤çš„å˜æ›´ç‚¹æ‰è¿›è¡Œåˆå¹¶

ã€è¯¦ç»†åˆå¹¶è§„åˆ™ã€‘ï¼š
- **æ¥å£æ ¡éªŒç›¸å…³**ï¼šåªåˆå¹¶å®Œå…¨é‡å¤æè¿°åŒä¸€æ ¡éªŒè§„åˆ™è°ƒæ•´çš„å˜æ›´ç‚¹
- **æ“ä½œåŠŸèƒ½ç›¸å…³**ï¼šæŒ‰å…·ä½“åŠŸèƒ½ç»†åˆ†ï¼Œå¦‚"åŠŸèƒ½é‡å‘½å"ã€"æ–°å¢æŒ‰é’®"ã€"æ–°å¢å­—æ®µ"ã€"å¯¼å‡ºåŠŸèƒ½"ã€"å®šæ—¶ä»»åŠ¡"ã€"æƒé™ç®¡ç†"ã€"æ•°æ®å‡çº§"ç­‰åº”åˆ†åˆ«ä¿ç•™
- **é¡µé¢åŠŸèƒ½ç›¸å…³**ï¼šæŸ¥è¯¢ã€ç­›é€‰ã€å¯¼å‡ºã€æ±‡æ€»ç­‰å…·ä½“åŠŸèƒ½åº”è¯¥åˆ†åˆ«ä¿ç•™
- **æ•°æ®å­—æ®µç›¸å…³**ï¼šæ–°å¢å­—æ®µã€å­—æ®µç±»å‹ç­‰åº”è¯¥åˆ†åˆ«ä¿ç•™

ã€åˆå¹¶ç­–ç•¥ã€‘ï¼š
- åªåˆå¹¶æè¿°å®Œå…¨ç›¸åŒæˆ–é«˜åº¦é‡å¤çš„å˜æ›´ç‚¹
- ä¸åŒå…·ä½“åŠŸèƒ½ç»å¯¹ä¸åˆå¹¶ï¼ˆå¦‚å¯¼å‡º â‰  æŸ¥è¯¢ â‰  ç­›é€‰ï¼‰
- ä¿æŒåŠŸèƒ½çš„å®Œæ•´æ€§å’Œç‹¬ç«‹æ€§
- å®å¯å¤šåˆ†å‡ ä¸ªå˜æ›´é¡¹ï¼Œä¹Ÿä¸è¦ä¸¢å¤±é‡è¦åŠŸèƒ½ç»†èŠ‚

ã€è¾“å‡ºè¦æ±‚ã€‘ï¼š
- ä¿æŒå˜æ›´ç±»å‹çš„çº¯ç²¹æ€§ï¼Œä¸è¦æ··åˆä¸åŒç±»å‹
- ä¿ç•™æ‰€æœ‰é‡è¦çš„å…·ä½“åŠŸèƒ½ï¼Œé¿å…ä¸¢å¤±åŠŸèƒ½ç»†èŠ‚
- å¦‚æœè¾“å…¥çš„å˜æ›´ç‚¹ä¸­åŒ…å«å¤šä¸ªåŠŸèƒ½ï¼Œåˆ™éœ€è¦å°†æ¯ä¸ªåŠŸèƒ½å•ç‹¬ä¿ç•™å†åˆå¹¶
- changeReasonåº”è¯¥è¯´æ˜å…·ä½“çš„å˜æ›´åŸå› ï¼Œä¸è¦è¿‡äºç¬¼ç»Ÿ
- changeItemsåº”è¯¥ä¿ç•™å…·ä½“åŠŸèƒ½çš„ç‹¬ç«‹æ€§
- versionå­—æ®µåˆå¹¶ç›¸å…³ç‰ˆæœ¬
- mergedFromå­—æ®µè®°å½•åŸå§‹å˜æ›´é¡¹ç¼–å·

è¯·è¿”å›JSONæ ¼å¼çš„åˆå¹¶ç»“æœã€‚**æŒ‰åŠŸèƒ½å­æ¨¡å—ç»†åˆ†ï¼Œä¿ç•™é‡è¦åŠŸèƒ½ç»†èŠ‚**ï¼š
[
  {{
    "changeType": "å…·ä½“çš„å˜æ›´ç±»å‹ï¼ˆæ–°å¢/ä¿®æ”¹/åˆ é™¤ï¼‰",
    "changeReason": "å˜æ›´åŸå› è¯´æ˜ï¼ˆè¯´æ˜ä¸ºä»€ä¹ˆè¿™äº›å˜æ›´ç‚¹å¯ä»¥åœ¨å½“å‰å˜æ›´ç±»å‹ä¸‹",
    "changeItems": ["å…·ä½“çš„å˜æ›´ç‚¹1", "å˜æ›´ç‚¹2", ...],
    "version": ["ç›¸å…³ç‰ˆæœ¬1", "ç‰ˆæœ¬2", ...],
    "mergedFrom": [åŸå§‹å˜æ›´é¡¹ç¼–å·åˆ—è¡¨],
  }}
]

**é‡è¦æé†’ï¼š**
1. ä¸åŒå˜æ›´ç±»å‹ï¼ˆæ–°å¢/ä¿®æ”¹/åˆ é™¤ï¼‰ç»å¯¹ä¸èƒ½åˆå¹¶ï¼
2. å»é™¤å®Œå…¨é‡å¤æè¿°çš„å˜æ›´ç‚¹ï¼Œä½†ä¿ç•™ä¸åŒå…·ä½“åŠŸèƒ½çš„å˜æ›´ç‚¹
3. é‡è¦åŠŸèƒ½ï¼ˆå¯¼å‡ºã€æŸ¥è¯¢ã€ç­›é€‰ã€æ±‡æ€»ç­‰ï¼‰å¿…é¡»å•ç‹¬ä¿ç•™ï¼
4. è¿”å›ç»“æœå¿…é¡»æ˜¯JSONæ•°ç»„æ ¼å¼ï¼
5. å®å¯å¤šä¿ç•™å‡ ä¸ªå˜æ›´é¡¹ï¼Œä¹Ÿä¸è¦ä¸¢å¤±é‡è¦åŠŸèƒ½ç»†èŠ‚ï¼
6. æ¯ä¸ªç»“æœé¡¹åº”è¯¥ä»£è¡¨ä¸€ä¸ªå…·ä½“çš„åŠŸèƒ½å˜æ›´ï¼Œè€Œä¸æ˜¯å¤§è€Œå…¨çš„åˆå¹¶ï¼
"""
        
        try:
            self.logger.info(f"LLMåŸå§‹è¾“å…¥: {user_prompt}")
            response = await self._call_llm(user_prompt, system_prompt, max_tokens=12000)
            if response:
                self.logger.info(f"LLMåŸå§‹å“åº”: {response}")
                cleaned_response = self._clean_json_response(response)
                self.logger.info(f"LLMæ¸…ç†åå“åº”é•¿åº¦: {len(cleaned_response)}")
                self.logger.info(f"LLMæ¸…ç†åå“åº”å¼€å¤´: {cleaned_response[:200]}...")
                if len(cleaned_response) > 500:
                    self.logger.info(f"LLMæ¸…ç†åå“åº”ç»“å°¾: ...{cleaned_response[-200:]}")
                
                try:
                    merged_changes = json.loads(cleaned_response)
                    self.logger.info(f"JSONè§£ææˆåŠŸï¼Œç±»å‹: {type(merged_changes)}, å†…å®¹: {merged_changes}")
                except json.JSONDecodeError as json_error:
                    self.logger.error(f"JSONè§£æå¤±è´¥: {json_error}")
                    self.logger.error(f"å°è¯•è§£æçš„å†…å®¹: {cleaned_response}")
                    return preprocessed_changes
                
                # å¤„ç†LLMè¿”å›å•ä¸ªå¯¹è±¡çš„æƒ…å†µ
                if isinstance(merged_changes, dict):
                    # å¦‚æœè¿”å›çš„æ˜¯å•ä¸ªåˆå¹¶å¯¹è±¡ï¼Œè½¬æ¢ä¸ºæ•°ç»„
                    if 'changeType' in merged_changes and 'changeItems' in merged_changes:
                        merged_changes = [merged_changes]
                        self.logger.info(f"LLMè¿”å›å•ä¸ªåˆå¹¶å¯¹è±¡ï¼Œè½¬æ¢ä¸ºæ•°ç»„: {merged_changes}")
                    else:
                        self.logger.warning("LLMè¿”å›çš„å•ä¸ªå¯¹è±¡æ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨é¢„å¤„ç†æ•°æ®")
                        return preprocessed_changes
                
                # éªŒè¯åˆå¹¶ç»“æœ - ç§»é™¤æ•°é‡é™åˆ¶ï¼Œå…è®¸åˆ†æ‹†å‡ºæ›´å¤šå˜æ›´é¡¹
                if isinstance(merged_changes, list) and len(merged_changes) > 0:
                    # éªŒè¯æ¯ä¸ªåˆå¹¶é¡¹æ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
                    valid_changes = []
                    for change in merged_changes:
                        if isinstance(change, dict) and 'changeType' in change and 'changeItems' in change:
                            valid_changes.append(change)
                    
                    if len(valid_changes) > 0:
                        self.logger.info(f"LLMæ™ºèƒ½åˆå¹¶æˆåŠŸ: {len(changes)} -> {len(valid_changes)}")
                        return valid_changes
                    else:
                        self.logger.warning("LLMåˆå¹¶ç»“æœæ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨é¢„å¤„ç†æ•°æ®")
                        return preprocessed_changes
                else:
                    self.logger.warning(f"LLMåˆå¹¶ç»“æœæ ¼å¼å¼‚å¸¸: åŸå§‹{len(changes)}é¡¹ -> åˆå¹¶{len(merged_changes) if isinstance(merged_changes, list) else 'N/A'}é¡¹ï¼Œä½¿ç”¨é¢„å¤„ç†æ•°æ®")
                    return preprocessed_changes
        except json.JSONDecodeError as e:
            self.logger.error(f"LLMåˆå¹¶å“åº”JSONè§£æå¤±è´¥: {e}")
            return preprocessed_changes
        except Exception as e:
            self.logger.error(f"LLMæ™ºèƒ½åˆå¹¶å¤±è´¥: {e}")
        
        return preprocessed_changes
    
    async def _similarity_based_merge(self, changes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ–¹æ¡ˆ2: åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦çš„åˆå¹¶"""
        if len(changes) <= 1:
            return changes
        
        from difflib import SequenceMatcher
        
        merged_changes = []
        used_indices = set()
        
        for i, change in enumerate(changes):
            if i in used_indices:
                continue
                
            similar_group = [change]
            similar_indices = {i}
            
            change_text = f"{change.get('changeReason', '')} {' '.join(change.get('changeItems', []))}"
            
            for j, other_change in enumerate(changes[i+1:], i+1):
                if j in used_indices:
                    continue
                    
                other_text = f"{other_change.get('changeReason', '')} {' '.join(other_change.get('changeItems', []))}"
                similarity = SequenceMatcher(None, change_text, other_text).ratio()
                
                # ç›¸ä¼¼åº¦é˜ˆå€¼
                if similarity > 0.6:
                    similar_group.append(other_change)
                    similar_indices.add(j)
            
            used_indices.update(similar_indices)
            
            if len(similar_group) > 1:
                # åˆå¹¶ç›¸ä¼¼é¡¹
                merged_change = self._merge_change_group(similar_group)
                merged_changes.append(merged_change)
            else:
                merged_changes.append(change)
        
        self.logger.info(f"ç›¸ä¼¼åº¦åˆå¹¶: {len(changes)} -> {len(merged_changes)}")
        return merged_changes
    
    async def _keyword_based_merge(self, changes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ–¹æ¡ˆ3: åŸºäºå…³é”®è¯çš„åˆ†ç»„åˆå¹¶"""
        if len(changes) <= 1:
            return changes
        
        # å®šä¹‰å…³é”®è¯ç»„
        keyword_groups = {
            "æ•°æ®åº“ç›¸å…³": ["æ•°æ®åº“", "å­—æ®µ", "è¡¨", "ç´¢å¼•", "æŸ¥è¯¢", "å­˜å‚¨"],
            "æ¥å£ç›¸å…³": ["æ¥å£", "API", "æœåŠ¡", "è°ƒç”¨", "è¯·æ±‚", "å“åº”"],
            "åŠŸèƒ½æ¨¡å—": ["åŠŸèƒ½", "æ¨¡å—", "é¡µé¢", "æŒ‰é’®", "æ“ä½œ"],
            "æƒé™ç›¸å…³": ["æƒé™", "è®¤è¯", "æˆæƒ", "è§’è‰²", "ç”¨æˆ·"],
            "ä¸šåŠ¡æµç¨‹": ["æµç¨‹", "æ­¥éª¤", "å®¡æ‰¹", "å¤„ç†", "ä¸šåŠ¡"]
        }
        
        grouped_changes = {}
        ungrouped_changes = []
        
        for change in changes:
            change_text = f"{change.get('changeReason', '')} {' '.join(change.get('changeItems', []))}"
            grouped = False
            
            for group_name, keywords in keyword_groups.items():
                if any(keyword in change_text for keyword in keywords):
                    if group_name not in grouped_changes:
                        grouped_changes[group_name] = []
                    grouped_changes[group_name].append(change)
                    grouped = True
                    break
            
            if not grouped:
                ungrouped_changes.append(change)
        
        merged_changes = []
        
        # åˆå¹¶å„ä¸ªå…³é”®è¯ç»„
        for group_name, group_changes in grouped_changes.items():
            if len(group_changes) > 1:
                merged_change = self._merge_change_group(group_changes)
                merged_change['groupType'] = group_name
                merged_changes.append(merged_change)
            else:
                merged_changes.extend(group_changes)
        
        # æ·»åŠ æœªåˆ†ç»„çš„å˜æ›´
        merged_changes.extend(ungrouped_changes)
        
        self.logger.info(f"å…³é”®è¯åˆå¹¶: {len(changes)} -> {len(merged_changes)}")
        return merged_changes
    
    async def _rule_based_merge(self, changes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ–¹æ¡ˆ4: åŸºäºè§„åˆ™çš„æ¨¡å¼åŒ¹é…åˆå¹¶"""
        if len(changes) <= 1:
            return changes
        
        merged_changes = []
        used_indices = set()
        
        # è§„åˆ™1: ç›¸åŒchangeTypeçš„å˜æ›´é¡¹å¯ä»¥è€ƒè™‘åˆå¹¶
        change_type_groups = {}
        for i, change in enumerate(changes):
            change_type = change.get('changeType', 'æœªçŸ¥')
            if change_type not in change_type_groups:
                change_type_groups[change_type] = []
            change_type_groups[change_type].append((i, change))
        
        for change_type, type_changes in change_type_groups.items():
            if len(type_changes) > 1:
                # æ£€æŸ¥æ˜¯å¦æœ‰å…±åŒçš„å…³é”®è¯
                all_items = []
                for _, change in type_changes:
                    all_items.extend(change.get('changeItems', []))
                
                # å¦‚æœæœ‰é‡å¤çš„å…³é”®è¯ï¼Œè¯´æ˜å¯èƒ½ç›¸å…³
                item_text = ' '.join(all_items)
                common_keywords = self._extract_common_keywords(item_text)
                
                if len(common_keywords) > 0:
                    # åˆå¹¶è¿™ç»„å˜æ›´
                    group_changes = [change for _, change in type_changes]
                    merged_change = self._merge_change_group(group_changes)
                    merged_changes.append(merged_change)
                    used_indices.update(i for i, _ in type_changes)
                else:
                    # ä¸åˆå¹¶ï¼Œåˆ†åˆ«æ·»åŠ 
                    for i, change in type_changes:
                        if i not in used_indices:
                            merged_changes.append(change)
                            used_indices.add(i)
            else:
                i, change = type_changes[0]
                if i not in used_indices:
                    merged_changes.append(change)
                    used_indices.add(i)
        
        self.logger.info(f"è§„åˆ™åˆå¹¶: {len(changes)} -> {len(merged_changes)}")
        return merged_changes
    
    def _merge_change_group(self, changes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶ä¸€ç»„ç›¸ä¼¼çš„å˜æ›´é¡¹"""
        if not changes:
            return {}
        
        if len(changes) == 1:
            return changes[0]
        
        # åˆå¹¶é€»è¾‘
        merged_change = {
            "changeType": changes[0].get('changeType', 'æœªçŸ¥'),
            "changeReason": self._merge_reasons([c.get('changeReason', '') for c in changes]),
            "changeItems": self._merge_items([c.get('changeItems', []) for c in changes]),
            "version": self._merge_versions([c.get('version', []) for c in changes]),
            "mergedCount": len(changes)
        }
        
        return merged_change
    
    def _merge_reasons(self, reasons: List[str]) -> str:
        """åˆå¹¶å˜æ›´åŸå› """
        unique_reasons = []
        for reason in reasons:
            if reason and reason not in unique_reasons:
                unique_reasons.append(reason)
        
        if len(unique_reasons) == 1:
            return unique_reasons[0]
        else:
            return f"ç»¼åˆå˜æ›´åŸå› : {'; '.join(unique_reasons)}"
    
    def _merge_items(self, item_lists: List[List[str]]) -> List[str]:
        """åˆå¹¶å˜æ›´é¡¹ï¼Œå»é‡ä½†ä¿æŒé€»è¾‘åˆ†ç»„"""
        all_items = []
        for items in item_lists:
            all_items.extend(items)
        
        # ç®€å•å»é‡ï¼Œä¿æŒé¡ºåº
        unique_items = []
        for item in all_items:
            if item not in unique_items:
                unique_items.append(item)
        
        return unique_items
    
    def _merge_versions(self, version_lists: List[List[str]]) -> List[str]:
        """åˆå¹¶ç‰ˆæœ¬ä¿¡æ¯"""
        all_versions = []
        for versions in version_lists:
            all_versions.extend(versions)
        
        unique_versions = list(set(all_versions))
        return unique_versions
    
    def _extract_common_keywords(self, text: str) -> List[str]:
        """æå–å¸¸è§å…³é”®è¯"""
        import re
        
        # ç®€å•çš„å…³é”®è¯æå–
        words = re.findall(r'[\u4e00-\u9fff]+', text)  # æå–ä¸­æ–‡è¯
        word_count = {}
        
        for word in words:
            if len(word) >= 2:  # åªè€ƒè™‘é•¿åº¦å¤§äºç­‰äº2çš„è¯
                word_count[word] = word_count.get(word, 0) + 1
        
        # è¿”å›å‡ºç°æ¬¡æ•°å¤§äº1çš„è¯
        common_keywords = [word for word, count in word_count.items() if count > 1]
        return common_keywords
    
    async def _preprocess_change_types(self, changes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        é¢„å¤„ç†å˜æ›´ç±»å‹ï¼Œæ­£ç¡®è¯†åˆ«æ–°å¢ã€ä¿®æ”¹ã€åˆ é™¤ç±»å‹
        
        Args:
            changes: åŸå§‹å˜æ›´é¡¹åˆ—è¡¨
            
        Returns:
            ç±»å‹ä¿®æ­£åçš„å˜æ›´é¡¹åˆ—è¡¨
        """
        preprocessed_changes = []
        
        for change in changes:
            change_items = change.get('changeItems', [])
            
            # åˆ†æå˜æ›´é¡¹çš„å†…å®¹ï¼Œåˆ¤æ–­å®é™…çš„å˜æ›´ç±»å‹
            new_items = []  # æ–°å¢åŠŸèƒ½é¡¹
            modify_items = []  # ä¿®æ”¹åŠŸèƒ½é¡¹
            delete_items = []  # åˆ é™¤åŠŸèƒ½é¡¹
            
            self.logger.info(f"æ­£åœ¨åˆ†æå˜æ›´é¡¹: {change.get('changeType', 'æœªçŸ¥')} - {change_items}")
            
            for item in change_items:
                change_type = self._analyze_single_change_item(item)
                
                if change_type == 'æ–°å¢':
                    new_items.append(item)
                elif change_type == 'åˆ é™¤':
                    delete_items.append(item)
                else:  # ä¿®æ”¹
                    modify_items.append(item)
                
                self.logger.info(f"  å˜æ›´ç‚¹åˆ†æ: '{item[:50]}...' -> {change_type}")
            
            # åˆ›å»ºåŸºç¡€å˜æ›´ä¿¡æ¯
            base_change = {
                'changeReason': change.get('changeReason', ''),
                'version': change.get('version', [])
            }
            
            # æ ¹æ®åˆ†æç»“æœåˆ›å»ºå˜æ›´é¡¹
            created_changes = []
            
            if new_items:
                new_change = base_change.copy()
                new_change.update({
                    'changeType': 'æ–°å¢',
                    'changeItems': new_items,
                    'changeReason': self._generate_change_reason('æ–°å¢', base_change['changeReason'], new_items)
                })
                created_changes.append(new_change)
            
            if modify_items:
                modify_change = base_change.copy()
                modify_change.update({
                    'changeType': 'ä¿®æ”¹',
                    'changeItems': modify_items,
                    'changeReason': self._generate_change_reason('ä¿®æ”¹', base_change['changeReason'], modify_items)
                })
                created_changes.append(modify_change)
            
            if delete_items:
                delete_change = base_change.copy()
                delete_change.update({
                    'changeType': 'åˆ é™¤',
                    'changeItems': delete_items,
                    'changeReason': self._generate_change_reason('åˆ é™¤', base_change['changeReason'], delete_items)
                })
                created_changes.append(delete_change)
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•å˜æ›´é¡¹è¢«å½’ç±»ï¼Œä¿ç•™åŸå˜æ›´é¡¹
            if not created_changes:
                original_change = change.copy()
                original_change['changeType'] = 'ä¿®æ”¹'  # é»˜è®¤ä¸ºä¿®æ”¹
                created_changes.append(original_change)
            
            preprocessed_changes.extend(created_changes)
            
            self.logger.info(f"  å˜æ›´é¡¹æ‹†åˆ†ç»“æœ: 1 -> {len(created_changes)} é¡¹")
        
        self.logger.info(f"å˜æ›´ç±»å‹é¢„å¤„ç†å®Œæˆ: {len(changes)} -> {len(preprocessed_changes)}")
        for i, change in enumerate(preprocessed_changes):
            self.logger.info(f"é¢„å¤„ç†åå˜æ›´é¡¹{i+1}: {change.get('changeType')} - åŒ…å«{len(change.get('changeItems', []))}ä¸ªå˜æ›´ç‚¹")
        
        return preprocessed_changes
    
    def _analyze_single_change_item(self, item: str) -> str:
        """
        åˆ†æå•ä¸ªå˜æ›´é¡¹çš„ç±»å‹
        
        Args:
            item: å•ä¸ªå˜æ›´æè¿°
            
        Returns:
            å˜æ›´ç±»å‹: 'æ–°å¢', 'ä¿®æ”¹', 'åˆ é™¤'
        """
        item_text = item.strip()
        
        # å®šä¹‰æ›´ç²¾ç¡®çš„å…³é”®è¯åŒ¹é…è§„åˆ™
        new_keywords = [
            'æ–°å¢', 'æ–°å»º', 'æ·»åŠ ', 'å¢åŠ ', 'åˆ›å»º', 'å¼•å…¥', 
            'æ”¯æŒ', 'æä¾›', 'å®ç°', 'è®¾ç½®',
            'æ–°å¢åŠŸèƒ½', 'æ–°å¢æŒ‰é’®', 'æ–°å¢å­—æ®µ', 'æ–°å¢é¡µé¢',
            'åˆ—è¡¨é¡µ', 'è¯¦æƒ…é¡µ'  # é€šå¸¸æŒ‡æ–°å¢é¡µé¢
        ]
        
        delete_keywords = [
            'åˆ é™¤', 'ç§»é™¤', 'å»é™¤', 'å–æ¶ˆ', 'åºŸå¼ƒ', 'åœç”¨',
            'ä¸å†', 'ç¦ç”¨', 'éšè—'
        ]
        
        modify_keywords = [
            'è°ƒæ•´', 'ä¿®æ”¹', 'å˜æ›´', 'ä¼˜åŒ–', 'æ›´æ–°', 'æ”¹ä¸º',
            'ç”±.*å˜æ›´ä¸º', 'ä».*è°ƒæ•´ä¸º', 'å°†.*ä¿®æ”¹ä¸º',
            'åç§°.*å˜æ›´', 'åŠŸèƒ½.*è°ƒæ•´'
        ]
        
        # ä¼˜å…ˆçº§æ£€æŸ¥ï¼šå…ˆæ£€æŸ¥æ˜ç¡®çš„æ–°å¢æ ‡è¯†
        for keyword in new_keywords:
            if keyword in item_text:
                # è¿›ä¸€æ­¥éªŒè¯æ˜¯å¦çœŸçš„æ˜¯æ–°å¢
                if any(exclude in item_text for exclude in ['è°ƒæ•´', 'ä¿®æ”¹', 'å˜æ›´ä¸º', 'æ”¹ä¸º']):
                    # å¦‚æœåŒæ—¶åŒ…å«ä¿®æ”¹å…³é”®è¯ï¼Œå¯èƒ½æ˜¯ä¿®æ”¹ç°æœ‰åŠŸèƒ½
                    continue
                return 'æ–°å¢'
        
        # æ£€æŸ¥åˆ é™¤å…³é”®è¯
        for keyword in delete_keywords:
            if keyword in item_text:
                return 'åˆ é™¤'
        
        # æ£€æŸ¥ä¿®æ”¹å…³é”®è¯
        import re
        for keyword in modify_keywords:
            if re.search(keyword, item_text):
                return 'ä¿®æ”¹'
        
        # ç‰¹æ®Šæƒ…å†µå¤„ç†
        # 1. åŠŸèƒ½åç§°å˜æ›´é€šå¸¸æ˜¯ä¿®æ”¹
        if re.search(r'åç§°.*(?:ç”±|ä»).+(?:å˜æ›´ä¸º|æ”¹ä¸º|è°ƒæ•´ä¸º)', item_text):
            return 'ä¿®æ”¹'
        
        # 2. åŒ…å«"æ–°å¢"ä½†ä¹ŸåŒ…å«"åœ¨...ä¸‹"å¯èƒ½æ˜¯åœ¨ç°æœ‰åŠŸèƒ½ä¸‹æ–°å¢å­åŠŸèƒ½
        if 'æ–°å¢' in item_text and any(phrase in item_text for phrase in ['åœ¨.*ä¸‹', 'åŠŸèƒ½ä¸‹', 'é¡µé¢.*æ–°å¢']):
            return 'æ–°å¢'
        
        # 3. æŒ‰é’®ã€å­—æ®µã€é¡µé¢ç›¸å…³çš„æè¿°
        if any(element in item_text for element in ['æŒ‰é’®', 'å­—æ®µ', 'é¡µé¢', 'åŠŸèƒ½', 'æ¥å£', 'åˆ—è¡¨']):
            # å¦‚æœæ˜ç¡®è¯´æ˜¯æ–°å¢è¿™äº›å…ƒç´ 
            if any(action in item_text for action in ['æ–°å¢', 'æ·»åŠ ', 'åˆ›å»º']):
                return 'æ–°å¢'
            # å¦‚æœæ˜¯è°ƒæ•´ç°æœ‰å…ƒç´ 
            elif any(action in item_text for action in ['è°ƒæ•´', 'ä¿®æ”¹', 'å˜æ›´', 'ä¼˜åŒ–']):
                return 'ä¿®æ”¹'
        
        # é»˜è®¤å½’ç±»ä¸ºä¿®æ”¹ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        return 'ä¿®æ”¹'
    
    def _generate_change_reason(self, change_type: str, original_reason: str, items: List[str]) -> str:
        """
        ç”Ÿæˆæ›´ç²¾ç¡®çš„å˜æ›´åŸå› 
        
        Args:
            change_type: å˜æ›´ç±»å‹
            original_reason: åŸå§‹å˜æ›´åŸå› 
            items: å˜æ›´é¡¹åˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„å˜æ›´åŸå› 
        """
        # ç®€åŒ–å¤„ç†ï¼šç›´æ¥åŸºäºå˜æ›´ç±»å‹å’Œæ•°é‡ç”Ÿæˆé€šç”¨æè¿°
        if len(items) == 1:
            return f"{change_type}ç›¸å…³åŠŸèƒ½"
        else:
            return f"{change_type}å¤šé¡¹ç›¸å…³åŠŸèƒ½"
    
    async def _deduplicate_and_sort_changes(self, change_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """
        Step 6: å¯¹å˜æ›´åˆ†æç»“æœè¿›è¡Œå»é‡å’Œæ’åº
        
        Args:
            change_analysis: Step5çš„å˜æ›´åˆ†æç»“æœ
            
        Returns:
            å»é‡æ’åºåçš„å˜æ›´åˆ†æç»“æœ
        """
        try:
            result = change_analysis.copy()
            
            # å¤„ç†change_analysesåˆ—è¡¨
            if 'change_analyses' in change_analysis and change_analysis['change_analyses']:
                # Step 1: ä¼ ç»Ÿå»é‡
                traditional_deduplicated = self._deduplicate_changes(change_analysis['change_analyses'])
                
                # Step 2: å¤§æ¨¡å‹æ™ºèƒ½å»é‡ï¼ˆè¯†åˆ«ä¸åŒå˜æ›´æ¨¡å—ä¸­çš„ç›¸åŒåŠŸèƒ½ï¼‰
                llm_deduplicated = await self._llm_intelligent_deduplicate(traditional_deduplicated)
                
                # Step 3: æ’åº
                sorted_changes = self._sort_changes(llm_deduplicated)
                result['change_analyses'] = sorted_changes
                
                self.logger.info(f"change_analysesæ™ºèƒ½å»é‡æ’åº: {len(change_analysis['change_analyses'])} -> {len(traditional_deduplicated)} -> {len(llm_deduplicated)} -> {len(sorted_changes)}")
            
            # å¤„ç†deletion_analysesåˆ—è¡¨
            if 'deletion_analyses' in change_analysis and change_analysis['deletion_analyses']:
                # Step 1: ä¼ ç»Ÿå»é‡
                traditional_deduplicated = self._deduplicate_changes(change_analysis['deletion_analyses'])
                
                # Step 2: å¤§æ¨¡å‹æ™ºèƒ½å»é‡
                llm_deduplicated = await self._llm_intelligent_deduplicate(traditional_deduplicated)
                
                # Step 3: æ’åº
                sorted_deletions = self._sort_changes(llm_deduplicated)
                result['deletion_analyses'] = sorted_deletions
                
                self.logger.info(f"deletion_analysesæ™ºèƒ½å»é‡æ’åº: {len(change_analysis['deletion_analyses'])} -> {len(traditional_deduplicated)} -> {len(llm_deduplicated)} -> {len(sorted_deletions)}")
            
            # æ›´æ–°summaryä¿¡æ¯
            if 'summary' in result:
                result['summary']['analysis_method'] = "LLM+å‘é‡æ•°æ®åº“åˆ†æ+è¯¦ç»†å†…å®¹æå–+æ™ºèƒ½åˆå¹¶+å¤§æ¨¡å‹æ™ºèƒ½å»é‡+æ’åº"
            
            # æ·»åŠ å»é‡å…ƒæ•°æ®
            if 'metadata' not in result:
                result['metadata'] = {}
            result['metadata']['deduplication_applied'] = True
            result['metadata']['llm_deduplication_applied'] = True
            result['metadata']['deduplication_timestamp'] = time.time()
            
            return result
            
        except Exception as e:
            self.logger.error(f"å»é‡æ’åºå¤±è´¥: {e}")
            return change_analysis
    
    def _deduplicate_changes(self, changes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¯¹å˜æ›´é¡¹è¿›è¡Œå»é‡å¤„ç†
        
        Args:
            changes: å˜æ›´é¡¹åˆ—è¡¨
            
        Returns:
            å»é‡åçš„å˜æ›´é¡¹åˆ—è¡¨
        """
        if len(changes) <= 1:
            return changes
        
        deduplicated = []
        used_indices = set()
        
        for i, change in enumerate(changes):
            if i in used_indices:
                continue
            
            # æŸ¥æ‰¾ç›¸ä¼¼çš„å˜æ›´é¡¹
            similar_changes = [change]
            similar_indices = {i}
            
            for j, other_change in enumerate(changes[i+1:], i+1):
                if j in used_indices:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå¹¶
                if self._should_merge_changes(change, other_change):
                    similar_changes.append(other_change)
                    similar_indices.add(j)
            
            used_indices.update(similar_indices)
            
            # å¦‚æœæ‰¾åˆ°ç›¸ä¼¼é¡¹ï¼Œè¿›è¡Œåˆå¹¶
            if len(similar_changes) > 1:
                merged_change = self._merge_similar_change_details(similar_changes)
                deduplicated.append(merged_change)
                self.logger.info(f"åˆå¹¶äº† {len(similar_changes)} ä¸ªç›¸ä¼¼å˜æ›´é¡¹")
            else:
                deduplicated.append(change)
        
        return deduplicated
    
    def _should_merge_changes(self, change1: Dict[str, Any], change2: Dict[str, Any]) -> bool:
        """
        åˆ¤æ–­ä¸¤ä¸ªå˜æ›´é¡¹æ˜¯å¦åº”è¯¥åˆå¹¶
        
        Args:
            change1: å˜æ›´é¡¹1
            change2: å˜æ›´é¡¹2
            
        Returns:
            æ˜¯å¦åº”è¯¥åˆå¹¶
        """
        # 1. å˜æ›´ç±»å‹å¿…é¡»ç›¸åŒ
        if change1.get('changeType') != change2.get('changeType'):
            return False
        
        # 2. æ£€æŸ¥å˜æ›´ç‚¹æ˜¯å¦å®Œå…¨ç›¸åŒï¼ˆå»é‡ï¼‰
        items1 = set(change1.get('changeItems', []))
        items2 = set(change2.get('changeItems', []))
        
        # å¦‚æœå˜æ›´ç‚¹å®Œå…¨ç›¸åŒï¼Œåº”è¯¥åˆå¹¶
        if items1 == items2:
            return True
        
        # 3. æ£€æŸ¥å˜æ›´ç‚¹ä¹‹é—´çš„é«˜ç›¸ä¼¼åº¦ï¼ˆé’ˆå¯¹æ„æ€ç›¸åŒä½†è¡¨è¿°ç•¥æœ‰ä¸åŒçš„æƒ…å†µï¼‰
        items1_list = change1.get('changeItems', [])
        items2_list = change2.get('changeItems', [])
        
        # å¦‚æœä¸¤ä¸ªå˜æ›´é¡¹éƒ½åªæœ‰ä¸€ä¸ªchangeItemï¼Œä¸”é«˜åº¦ç›¸ä¼¼ï¼Œåˆ™åˆå¹¶
        if len(items1_list) == 1 and len(items2_list) == 1:
            item_similarity = self._calculate_text_similarity(items1_list[0], items2_list[0])
            if item_similarity >= 0.85:  # 85%ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé’ˆå¯¹æ„æ€ç›¸åŒä½†è¡¨è¿°ç•¥æœ‰ä¸åŒçš„æƒ…å†µ
                self.logger.info(f"å‘ç°é«˜ç›¸ä¼¼åº¦å˜æ›´ç‚¹ (ç›¸ä¼¼åº¦: {item_similarity:.2f}): '{items1_list[0][:50]}...' vs '{items2_list[0][:50]}...'")
                return True
        
        # 4. æ£€æŸ¥å˜æ›´ç‚¹åˆ—è¡¨çš„æ•´ä½“ç›¸ä¼¼åº¦
        if items1_list and items2_list:
            # è®¡ç®—å˜æ›´ç‚¹åˆ—è¡¨çš„æ–‡æœ¬ç›¸ä¼¼åº¦
            text1 = ' '.join(items1_list)
            text2 = ' '.join(items2_list)
            items_similarity = self._calculate_text_similarity(text1, text2)
            if items_similarity >= 0.8:  # 80%ç›¸ä¼¼åº¦é˜ˆå€¼
                self.logger.info(f"å‘ç°é«˜ç›¸ä¼¼åº¦å˜æ›´ç‚¹åˆ—è¡¨ (ç›¸ä¼¼åº¦: {items_similarity:.2f})")
                return True
        
        # 5. æ£€æŸ¥å˜æ›´è¯¦æƒ…ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæœ‰changeDetailså­—æ®µï¼‰
        details1 = change1.get('changeDetails', '')
        details2 = change2.get('changeDetails', '')
        
        if details1 and details2:
            similarity = self._calculate_text_similarity(details1, details2)
            if similarity >= 0.8:  # 80%ç›¸ä¼¼åº¦é˜ˆå€¼
                self.logger.info(f"å‘ç°é«˜ç›¸ä¼¼åº¦å˜æ›´è¯¦æƒ… (ç›¸ä¼¼åº¦: {similarity:.2f})")
                return True
        
        # 6. æ£€æŸ¥å˜æ›´åŸå› ç›¸ä¼¼åº¦
        reason1 = change1.get('changeReason', '')
        reason2 = change2.get('changeReason', '')
        
        if reason1 and reason2:
            reason_similarity = self._calculate_text_similarity(reason1, reason2)
            if reason_similarity >= 0.9:  # å˜æ›´åŸå› 90%ç›¸ä¼¼
                return True
        
        return False
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆé’ˆå¯¹ä¸­æ–‡æ–‡æœ¬ä¼˜åŒ–ï¼‰
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            ç›¸ä¼¼åº¦ (0-1)
        """
        from difflib import SequenceMatcher
        import re
        
        # å¦‚æœæ–‡æœ¬ä¸ºç©ºï¼Œè¿”å›0
        if not text1 or not text2:
            return 0.0
        
        # å»é™¤å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™æ ¸å¿ƒå†…å®¹
        clean_text1 = re.sub(r'[ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿ\s]+', '', text1.strip())
        clean_text2 = re.sub(r'[ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿ\s]+', '', text2.strip())
        
        # æ–¹æ³•1: åŸºç¡€å­—ç¬¦åºåˆ—ç›¸ä¼¼åº¦
        basic_similarity = SequenceMatcher(None, clean_text1, clean_text2).ratio()
        
        # æ–¹æ³•2: å…³é”®è¯åŒ¹é…ç›¸ä¼¼åº¦ï¼ˆé’ˆå¯¹ä¸šåŠ¡æœ¯è¯­ï¼‰
        # æå–å…³é”®ä¸šåŠ¡è¯æ±‡
        keywords1 = self._extract_business_keywords(text1)
        keywords2 = self._extract_business_keywords(text2)
        
        if keywords1 and keywords2:
            common_keywords = len(set(keywords1) & set(keywords2))
            total_keywords = len(set(keywords1) | set(keywords2))
            keyword_similarity = common_keywords / total_keywords if total_keywords > 0 else 0.0
        else:
            keyword_similarity = 0.0
        
        # æ–¹æ³•3: è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆæ£€æŸ¥æ ¸å¿ƒæ¦‚å¿µï¼‰
        semantic_similarity = self._check_semantic_similarity(text1, text2)
        
        # ç»¼åˆç›¸ä¼¼åº¦è®¡ç®—ï¼ˆæƒé‡åˆ†é…ï¼‰
        final_similarity = (
            basic_similarity * 0.5 +      # åŸºç¡€å­—ç¬¦ç›¸ä¼¼åº¦æƒé‡50%
            keyword_similarity * 0.3 +    # å…³é”®è¯ç›¸ä¼¼åº¦æƒé‡30%
            semantic_similarity * 0.2     # è¯­ä¹‰ç›¸ä¼¼åº¦æƒé‡20%
        )
        
        self.logger.debug(f"ç›¸ä¼¼åº¦è®¡ç®—: åŸºç¡€={basic_similarity:.3f}, å…³é”®è¯={keyword_similarity:.3f}, è¯­ä¹‰={semantic_similarity:.3f}, ç»¼åˆ={final_similarity:.3f}")
        
        return final_similarity
    
    def _extract_business_keywords(self, text: str) -> List[str]:
        """
        æå–ä¸šåŠ¡å…³é”®è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        import re
        
        # å®šä¹‰ä¸šåŠ¡å…³é”®è¯æ¨¡å¼
        business_patterns = [
            r'ç¡®æƒä¸šåŠ¡ç”³è¯·',
            r'bizSerialNo',
            r'æ ¡éªŒè§„åˆ™',
            r'ä¸šåŠ¡ç¼–å·',
            r'æ ¸å¿ƒä¼ä¸š',
            r'å†…éƒ¨ç³»ç»Ÿ',
            r'ä¿®æ”¹',
            r'æ¨é€',
            r'ç›¸åŒ',
            r'å…è®¸',
            r'æ•°æ®',
            r'æ¥å£',
            r'è°ƒæ•´'
        ]
        
        keywords = []
        for pattern in business_patterns:
            if re.search(pattern, text):
                keywords.append(pattern)
        
        # æå–å…¶ä»–ä¸­æ–‡å…³é”®è¯ï¼ˆ2-6ä¸ªå­—ç¬¦ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,6}', text)
        keywords.extend(chinese_words)
        
        return list(set(keywords))  # å»é‡
    
    async def _llm_intelligent_deduplicate(self, changes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œæ™ºèƒ½å»é‡ï¼Œè¯†åˆ«ä¸åŒå˜æ›´æ¨¡å—ä¸­çš„ç›¸åŒåŠŸèƒ½
        
        Args:
            changes: å·²ç»è¿‡ä¼ ç»Ÿå»é‡çš„å˜æ›´é¡¹åˆ—è¡¨
            
        Returns:
            ç»è¿‡å¤§æ¨¡å‹æ™ºèƒ½å»é‡çš„å˜æ›´é¡¹åˆ—è¡¨
        """
        if len(changes) <= 1:
            return changes
        
        try:
            # æ„å»ºå˜æ›´é¡¹æ‘˜è¦ä¾›å¤§æ¨¡å‹åˆ†æ
            change_summaries = []
            for i, change in enumerate(changes):
                summary = {
                    'index': i,
                    'changeType': change.get('changeType', 'æœªçŸ¥'),
                    'changeReason': change.get('changeReason', ''),
                    'changeItems': change.get('changeItems', []),
                    'changeDetails': change.get('changeDetails', '')
                }
                change_summaries.append(summary)
            
            # æ„å»ºå¤§æ¨¡å‹æç¤ºè¯
            prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£å˜æ›´åˆ†æä¸“å®¶ã€‚ç°åœ¨éœ€è¦ä½ åˆ†æä»¥ä¸‹{len(changes)}ä¸ªå˜æ›´é¡¹ï¼Œè¯†åˆ«å…¶ä¸­åŠŸèƒ½ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„é¡¹ç›®å¹¶è¿›è¡Œå»é‡åˆå¹¶ã€‚

åˆ†æåŸåˆ™ï¼š
1. è¯†åˆ«è¯­ä¹‰ç›¸åŒä½†è¡¨è¿°ä¸åŒçš„å˜æ›´é¡¹ï¼ˆå¦‚ï¼š"è°ƒæ•´äº†ç¡®æƒä¸šåŠ¡ç”³è¯·æ¥å£ä¸­å…³äºbizSerialNoçš„æ ¡éªŒè§„åˆ™ï¼Œå…è®¸ç›¸åŒä¸šåŠ¡ç¼–å·çš„æ•°æ®åœ¨æ ¸å¿ƒä¼ä¸šå†…éƒ¨ç³»ç»Ÿä¿®æ”¹åé‡æ–°æ¨é€" å’Œ "è°ƒæ•´äº†ç¡®æƒä¸šåŠ¡ç”³è¯·æ¥å£ä¸­å…³äºbizSerialNoçš„æ ¡éªŒè§„åˆ™ï¼Œå…è®¸ç›¸åŒä¸šåŠ¡ç¼–å·çš„æ•°æ®åœ¨æ ¸å¿ƒä¼ä¸šå†…éƒ¨ç³»ç»Ÿä¿®æ”¹åå†æ¬¡æ¨é€"ï¼‰
2. åªæœ‰å˜æ›´ç±»å‹ç›¸åŒçš„é¡¹ç›®æ‰èƒ½åˆå¹¶
3. åˆå¹¶æ—¶ä¿ç•™æœ€å®Œæ•´ã€æœ€å‡†ç¡®çš„æè¿°
4. å¦‚æœå˜æ›´é¡¹æè¿°çš„æ˜¯ä¸åŒçš„åŠŸèƒ½ç‚¹ï¼Œå³ä½¿è¡¨è¿°ç›¸ä¼¼ä¹Ÿä¸è¦åˆå¹¶

å˜æ›´é¡¹åˆ—è¡¨ï¼š
{json.dumps(change_summaries, ensure_ascii=False, indent=2)}

è¯·åˆ†æè¿™äº›å˜æ›´é¡¹ï¼Œè¿”å›JSONæ ¼å¼çš„å»é‡ç»“æœï¼š
{{
  "merged_groups": [
    {{
      "merge_indices": [0, 3],  // éœ€è¦åˆå¹¶çš„å˜æ›´é¡¹ç´¢å¼•
      "reason": "è¿™ä¸¤ä¸ªå˜æ›´é¡¹éƒ½æ˜¯å…³äºåŒä¸€ä¸ªæ¥å£çš„ç›¸åŒæ ¡éªŒè§„åˆ™è°ƒæ•´ï¼Œåªæ˜¯è¡¨è¿°ç•¥æœ‰ä¸åŒ",
      "merged_changeReason": "åˆå¹¶åçš„å˜æ›´åŸå› ",
      "merged_changeItems": ["åˆå¹¶åçš„å˜æ›´ç‚¹1", "åˆå¹¶åçš„å˜æ›´ç‚¹2"],
      "merged_changeDetails": "åˆå¹¶åçš„å˜æ›´è¯¦æƒ…"
    }}
  ],
  "keep_separate": [1, 2, 4]  // ä¿æŒç‹¬ç«‹çš„å˜æ›´é¡¹ç´¢å¼•
}}

å¦‚æœæ²¡æœ‰éœ€è¦åˆå¹¶çš„é¡¹ç›®ï¼Œè¿”å›ï¼š
{{
  "merged_groups": [],
  "keep_separate": [0, 1, 2, ...]  // æ‰€æœ‰ç´¢å¼•
}}
"""

            # è°ƒç”¨å¤§æ¨¡å‹
            response = self.llm_client.chat(
                messages=[{"role": "user", "content": prompt}], 
                temperature=0.1
            )  # ä½¿ç”¨è¾ƒä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„ç»“æœ
            
            if not response:
                self.logger.warning("å¤§æ¨¡å‹æ™ºèƒ½å»é‡å¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ")
                return changes
            
            # è§£æå¤§æ¨¡å‹å“åº”
            response_text = response.strip()
            cleaned_response = self._clean_json_response(response_text)
            
            try:
                dedup_result = json.loads(cleaned_response)
            except json.JSONDecodeError as e:
                self.logger.error(f"å¤§æ¨¡å‹å»é‡å“åº”è§£æå¤±è´¥: {e}")
                return changes
            
            # åº”ç”¨å»é‡ç»“æœ
            final_changes = []
            processed_indices = set()
            
            # å¤„ç†åˆå¹¶ç»„
            for merge_group in dedup_result.get('merged_groups', []):
                merge_indices = merge_group.get('merge_indices', [])
                if len(merge_indices) < 2:
                    continue
                
                # è·å–è¦åˆå¹¶çš„å˜æ›´é¡¹
                changes_to_merge = [changes[i] for i in merge_indices if i < len(changes)]
                
                if changes_to_merge:
                    # åˆ›å»ºåˆå¹¶åçš„å˜æ›´é¡¹
                    merged_change = self._create_llm_merged_change(changes_to_merge, merge_group)
                    final_changes.append(merged_change)
                    processed_indices.update(merge_indices)
                    
                    self.logger.info(f"å¤§æ¨¡å‹æ™ºèƒ½åˆå¹¶å˜æ›´é¡¹ {merge_indices}: {merge_group.get('reason', 'æœªçŸ¥åŸå› ')}")
            
            # æ·»åŠ ä¿æŒç‹¬ç«‹çš„å˜æ›´é¡¹
            for i, change in enumerate(changes):
                if i not in processed_indices:
                    final_changes.append(change)
            
            self.logger.info(f"å¤§æ¨¡å‹æ™ºèƒ½å»é‡å®Œæˆ: {len(changes)} -> {len(final_changes)}")
            return final_changes
            
        except Exception as e:
            self.logger.error(f"å¤§æ¨¡å‹æ™ºèƒ½å»é‡å¤±è´¥: {e}")
            return changes
    
    def _create_llm_merged_change(self, changes_to_merge: List[Dict[str, Any]], merge_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¹æ®å¤§æ¨¡å‹çš„åˆå¹¶å»ºè®®åˆ›å»ºåˆå¹¶åçš„å˜æ›´é¡¹
        
        Args:
            changes_to_merge: è¦åˆå¹¶çš„å˜æ›´é¡¹åˆ—è¡¨
            merge_info: å¤§æ¨¡å‹æä¾›çš„åˆå¹¶ä¿¡æ¯
            
        Returns:
            åˆå¹¶åçš„å˜æ›´é¡¹
        """
        base_change = changes_to_merge[0].copy()
        
        # ä½¿ç”¨å¤§æ¨¡å‹å»ºè®®çš„åˆå¹¶ç»“æœï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ä¼ ç»Ÿåˆå¹¶æ–¹æ³•
        if 'merged_changeReason' in merge_info:
            base_change['changeReason'] = merge_info['merged_changeReason']
        
        if 'merged_changeItems' in merge_info:
            base_change['changeItems'] = merge_info['merged_changeItems']
        else:
            # åˆå¹¶æ‰€æœ‰å˜æ›´ç‚¹
            all_items = []
            for change in changes_to_merge:
                all_items.extend(change.get('changeItems', []))
            base_change['changeItems'] = self._intelligent_dedup_items(all_items)
        
        if 'merged_changeDetails' in merge_info:
            base_change['changeDetails'] = merge_info['merged_changeDetails']
        else:
            # é€‰æ‹©æœ€é•¿çš„å˜æ›´è¯¦æƒ…
            details_list = [change.get('changeDetails', '') for change in changes_to_merge]
            base_change['changeDetails'] = max(details_list, key=len) if details_list else ''
        
        # åˆå¹¶ç‰ˆæœ¬ä¿¡æ¯
        all_versions = []
        for change in changes_to_merge:
            all_versions.extend(change.get('version', []))
        base_change['version'] = list(set(all_versions))
        
        # æ·»åŠ åˆå¹¶ä¿¡æ¯
        base_change['mergedFrom'] = [
            f"å˜æ›´é¡¹{i+1}" for i in range(len(changes_to_merge))
        ]
        base_change['mergeReason'] = merge_info.get('reason', 'å¤§æ¨¡å‹æ™ºèƒ½åˆå¹¶')
        base_change['mergeMethod'] = 'llm_intelligent'
        base_change['mergedCount'] = len(changes_to_merge)
        
        return base_change
    
    def _check_semantic_similarity(self, text1: str, text2: str) -> float:
        """
        æ£€æŸ¥è¯­ä¹‰ç›¸ä¼¼åº¦
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            è¯­ä¹‰ç›¸ä¼¼åº¦ (0-1)
        """
        # å®šä¹‰è¯­ä¹‰ç­‰ä»·è¯ç»„
        semantic_groups = [
            {'é‡æ–°æ¨é€', 'å†æ¬¡æ¨é€', 'é‡æ¨', 'å†æ¨'},
            {'ä¿®æ”¹å', 'ä¿®æ”¹åçš„', 'ä¿®æ”¹ä¹‹å'},
            {'å…è®¸', 'æ”¯æŒ', 'å¯ä»¥'},
            {'ç›¸åŒ', 'åŒæ ·', 'ç›¸åŒçš„', 'åŒä¸€'},
            {'ä¸šåŠ¡ç¼–å·', 'ä¸šåŠ¡å·', 'bizSerialNo'},
            {'æ ¡éªŒè§„åˆ™', 'éªŒè¯è§„åˆ™', 'æ ¡éªŒ'},
            {'æ ¸å¿ƒä¼ä¸šå†…éƒ¨ç³»ç»Ÿ', 'æ ¸å¿ƒä¼ä¸šç³»ç»Ÿ', 'å†…éƒ¨ç³»ç»Ÿ'},
            {'è°ƒæ•´äº†', 'ä¿®æ”¹äº†', 'å˜æ›´äº†', 'æ›´æ–°äº†'}
        ]
        
        # æ£€æŸ¥ä¸¤ä¸ªæ–‡æœ¬æ˜¯å¦åŒ…å«è¯­ä¹‰ç­‰ä»·çš„è¯ç»„
        semantic_score = 0.0
        total_checks = 0
        
        for group in semantic_groups:
            found_in_text1 = any(word in text1 for word in group)
            found_in_text2 = any(word in text2 for word in group)
            
            if found_in_text1 or found_in_text2:
                total_checks += 1
                if found_in_text1 and found_in_text2:
                    semantic_score += 1.0
        
        return semantic_score / total_checks if total_checks > 0 else 0.0
    
    def _merge_similar_change_details(self, changes: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆå¹¶ç›¸ä¼¼çš„å˜æ›´é¡¹è¯¦æƒ…
        
        Args:
            changes: ç›¸ä¼¼çš„å˜æ›´é¡¹åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„å˜æ›´é¡¹
        """
        if len(changes) == 1:
            return changes[0]
        
        base_change = changes[0].copy()
        
        # è®°å½•è¯¦ç»†çš„åˆå¹¶ä¿¡æ¯ç”¨äºè°ƒè¯•
        self.logger.info(f"å¼€å§‹åˆå¹¶ {len(changes)} ä¸ªç›¸ä¼¼å˜æ›´é¡¹:")
        for i, change in enumerate(changes):
            items = change.get('changeItems', [])
            self.logger.info(f"  å˜æ›´é¡¹{i+1}: {change.get('changeType', 'æœªçŸ¥')} - {items}")
        
        # åˆå¹¶å˜æ›´ç‚¹ï¼ˆå»é‡ä½†ä¿ç•™ç›¸ä¼¼è¡¨è¿°ï¼‰
        all_items = []
        for change in changes:
            all_items.extend(change.get('changeItems', []))
        
        # æ™ºèƒ½å»é‡ï¼šä¿ç•™æœ€è¯¦ç»†çš„è¡¨è¿°
        unique_items = self._intelligent_dedup_items(all_items)
        base_change['changeItems'] = unique_items
        
        self.logger.info(f"  åˆå¹¶åå˜æ›´ç‚¹: {unique_items}")
        
        # åˆå¹¶ç‰ˆæœ¬ä¿¡æ¯
        all_versions = []
        for change in changes:
            all_versions.extend(change.get('version', []))
        base_change['version'] = list(set(all_versions))
        
        # åˆå¹¶mergedFromä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        all_merged_from = []
        for change in changes:
            if 'mergedFrom' in change:
                if isinstance(change['mergedFrom'], list):
                    all_merged_from.extend(change['mergedFrom'])
                else:
                    all_merged_from.append(change['mergedFrom'])
        if all_merged_from:
            base_change['mergedFrom'] = list(set(all_merged_from))
        
        # åˆå¹¶å˜æ›´è¯¦æƒ…ï¼ˆé€‰æ‹©æœ€é•¿çš„æˆ–åˆå¹¶å¤šä¸ªï¼‰
        all_details = []
        for change in changes:
            if 'changeDetails' in change and change['changeDetails']:
                all_details.append(change['changeDetails'])
        
        if len(all_details) > 1:
            # å¦‚æœæœ‰å¤šä¸ªè¯¦æƒ…ï¼Œé€‰æ‹©æœ€è¯¦ç»†çš„ä¸€ä¸ª
            base_change['changeDetails'] = max(all_details, key=len)
        elif len(all_details) == 1:
            base_change['changeDetails'] = all_details[0]
        
        # æ·»åŠ åˆå¹¶æ ‡è®°
        base_change['mergedChangesCount'] = len(changes)
        base_change['originalItems'] = [change.get('changeItems', []) for change in changes]  # ä¿ç•™åŸå§‹ä¿¡æ¯ç”¨äºè°ƒè¯•
        
        self.logger.info(f"å˜æ›´é¡¹åˆå¹¶å®Œæˆ: {len(changes)} -> 1")
        
        return base_change
    
    def _intelligent_dedup_items(self, items: List[str]) -> List[str]:
        """
        æ™ºèƒ½å»é‡å˜æ›´ç‚¹ï¼Œä¿ç•™æœ€è¯¦ç»†çš„è¡¨è¿°
        
        Args:
            items: å˜æ›´ç‚¹åˆ—è¡¨
            
        Returns:
            å»é‡åçš„å˜æ›´ç‚¹åˆ—è¡¨
        """
        if len(items) <= 1:
            return items
        
        unique_items = []
        used_indices = set()
        
        for i, item in enumerate(items):
            if i in used_indices:
                continue
            
            # æŸ¥æ‰¾ç›¸ä¼¼çš„é¡¹
            similar_group = [item]
            similar_indices = {i}
            
            for j, other_item in enumerate(items[i+1:], i+1):
                if j in used_indices:
                    continue
                
                similarity = self._calculate_text_similarity(item, other_item)
                if similarity >= 0.85:  # é«˜ç›¸ä¼¼åº¦é˜ˆå€¼
                    similar_group.append(other_item)
                    similar_indices.add(j)
                    self.logger.info(f"å‘ç°ç›¸ä¼¼å˜æ›´ç‚¹ (ç›¸ä¼¼åº¦: {similarity:.2f}): '{item[:30]}...' vs '{other_item[:30]}...'")
            
            used_indices.update(similar_indices)
            
            if len(similar_group) > 1:
                # é€‰æ‹©æœ€è¯¦ç»†çš„è¡¨è¿°ï¼ˆé€šå¸¸æ˜¯æœ€é•¿çš„ï¼‰
                best_item = max(similar_group, key=len)
                unique_items.append(best_item)
                self.logger.info(f"ç›¸ä¼¼å˜æ›´ç‚¹åˆå¹¶: é€‰æ‹© '{best_item[:50]}...' ä½œä¸ºä»£è¡¨")
            else:
                unique_items.append(item)
        
        return unique_items
    
    def _sort_changes(self, changes: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¯¹å˜æ›´é¡¹è¿›è¡Œæ’åº
        
        Args:
            changes: å˜æ›´é¡¹åˆ—è¡¨
            
        Returns:
            æ’åºåçš„å˜æ›´é¡¹åˆ—è¡¨
        """
        def sort_key(change):
            # æŒ‰ç…§å˜æ›´ç±»å‹ã€å˜æ›´åŸå› ã€å˜æ›´ç‚¹æ•°é‡ã€ç‰ˆæœ¬æ•°é‡è¿›è¡Œæ’åº
            change_type = change.get('changeType', '')
            change_reason = change.get('changeReason', '')
            items_count = len(change.get('changeItems', []))
            version_count = len(change.get('version', []))
            
            # å˜æ›´ç±»å‹æ’åºï¼šæ–°å¢ -> ä¿®æ”¹ -> åˆ é™¤
            type_order = {'æ–°å¢': 1, 'ä¿®æ”¹': 2, 'åˆ é™¤': 3}
            type_priority = type_order.get(change_type, 4)
            
            return (type_priority, change_reason, -items_count, -version_count)
        
        return sorted(changes, key=sort_key)
    
    def _get_cached_embedding(self, text: str):
        """
        è·å–å¸¦ç¼“å­˜çš„å‘é‡åµŒå…¥
        
        Args:
            text: éœ€è¦å‘é‡åŒ–çš„æ–‡æœ¬
            
        Returns:
            å‘é‡åµŒå…¥ï¼ˆnumpyæ•°ç»„æˆ–åˆ—è¡¨ï¼‰
        """
        import hashlib
        
        # è®¡ç®—æ–‡æœ¬å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        
        # æ£€æŸ¥ç¼“å­˜
        if text_hash in self._embedding_cache:
            self.logger.debug(f"å‘½ä¸­å‘é‡ç¼“å­˜: {text[:50]}...")
            return self._embedding_cache[text_hash]
        
        # ç”Ÿæˆæ–°å‘é‡
        embedding = self.embedding_model.encode(text)
        
        # ç¼“å­˜ç®¡ç†ï¼šå¦‚æœè¶…è¿‡æœ€å¤§ç¼“å­˜æ•°é‡ï¼Œæ¸…ç†ä¸€äº›æ—§ç¼“å­˜
        if len(self._embedding_cache) >= self._cache_max_size:
            # ç®€å•ç­–ç•¥ï¼šæ¸…ç†ä¸€åŠç¼“å­˜
            keys_to_remove = list(self._embedding_cache.keys())[:self._cache_max_size // 2]
            for key in keys_to_remove:
                del self._embedding_cache[key]
            self.logger.info(f"æ¸…ç†å‘é‡ç¼“å­˜ï¼Œåˆ é™¤ {len(keys_to_remove)} ä¸ªæ¡ç›®")
        
        # å­˜å‚¨åˆ°ç¼“å­˜
        self._embedding_cache[text_hash] = embedding
        self.logger.debug(f"æ–°å¢å‘é‡ç¼“å­˜: {text[:50]}...")
        
        return embedding
    
    def _batch_get_embeddings(self, texts: List[str]):
        """
        æ‰¹é‡è·å–å‘é‡åµŒå…¥ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åµŒå…¥åˆ—è¡¨
        """
        import hashlib
        
        embeddings = []
        uncached_texts = []
        uncached_indices = []
        
        # ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ç¼“å­˜
        for i, text in enumerate(texts):
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
            if text_hash in self._embedding_cache:
                embeddings.append(self._embedding_cache[text_hash])
            else:
                embeddings.append(None)  # å ä½ç¬¦
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡å¤„ç†æœªç¼“å­˜çš„æ–‡æœ¬
        if uncached_texts:
            self.logger.info(f"æ‰¹é‡ç”Ÿæˆ {len(uncached_texts)} ä¸ªæ–°å‘é‡ï¼ˆå·²ç¼“å­˜: {len(texts) - len(uncached_texts)}ï¼‰")
            new_embeddings = self.embedding_model.encode(uncached_texts, show_progress_bar=True)
            
            # ç¬¬ä¸‰æ­¥ï¼šæ›´æ–°ç¼“å­˜å’Œç»“æœ
            for i, (text, embedding) in enumerate(zip(uncached_texts, new_embeddings)):
                text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                self._embedding_cache[text_hash] = embedding
                embeddings[uncached_indices[i]] = embedding
        
        return embeddings
   