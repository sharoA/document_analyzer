"""
æ–‡æ¡£è§£ææœåŠ¡
å®ç°å®Œæ•´çš„æ–‡æ¡£è§£æåŠŸèƒ½ï¼šæ ¼å¼è¯†åˆ«ã€ç»“æ„è§£æã€å†…å®¹æå–ã€è´¨é‡åˆ†æã€ç‰ˆæœ¬ä¿¡æ¯æå–
"""

import os
import json
import time
import re
from typing import Dict, Any, List, Optional
from pathlib import Path
from .base_service import BaseAnalysisService

class DocumentParserService(BaseAnalysisService):
    """æ–‡æ¡£è§£ææœåŠ¡ç±» - é˜¶æ®µ1ï¼šæ–‡æ¡£è§£æ"""
    
    def __init__(self, llm_client=None, vector_db=None):
        super().__init__(llm_client, vector_db)
        self.stage_name = "document_parsing"
    
    async def analyze(self, task_id: str, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ–‡æ¡£è§£æåˆ†æ - ç¬¬ä¸€é˜¶æ®µ
        
        Args:
            task_id: ä»»åŠ¡ID
            input_data: åŒ…å«æ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶å†…å®¹ç­‰ä¿¡æ¯
            
        Returns:
            è§£æç»“æœå­—å…¸
        """
        start_time = time.time()
        
        try:
            # æå–è¾“å…¥æ•°æ®
            file_path = input_data.get("file_path", "")
            file_content = input_data.get("file_content", "")
            file_name = input_data.get("file_name", "")
            
            self.logger.info(f"ğŸ“„ å¼€å§‹æ–‡æ¡£è§£æ - Task: {task_id}")
            self.logger.info(f"   - æ–‡ä»¶è·¯å¾„: {file_path}")
            self.logger.info(f"   - æ–‡ä»¶å: {file_name}")
            self.logger.info(f"   - åŸå§‹å†…å®¹é•¿åº¦: {len(file_content)}")
            
            # ğŸ”§ ä¿®å¤ï¼šå¦‚æœfile_contentä¸ºç©ºä½†æœ‰file_pathï¼Œå°è¯•ä»æ–‡ä»¶è¯»å–å†…å®¹
            if not file_content and file_path and os.path.exists(file_path):
                self.logger.info(f"ğŸ“– ä»æ–‡ä»¶è·¯å¾„è¯»å–å†…å®¹: {file_path}")
                file_content = await self._read_file_content(file_path, file_name)
                self.logger.info(f"ğŸ“– è¯»å–åå†…å®¹é•¿åº¦: {len(file_content)}")
            
            # å¦‚æœä»ç„¶æ²¡æœ‰å†…å®¹ï¼Œç”Ÿæˆé»˜è®¤ç»“æœ
            if not file_content:
                self.logger.warning(f"âš ï¸ æ–‡ä»¶å†…å®¹ä¸ºç©º - Task: {task_id}")
                return await self._generate_empty_content_result(task_id, file_name, file_path)
            
            self._log_analysis_start(task_id, "æ–‡æ¡£è§£æ", len(file_content))
            
            # 1. æ–‡ä»¶æ ¼å¼è¯†åˆ«å’ŒåŸºæœ¬ä¿¡æ¯
            file_info = await self.identify_file_type(file_path, file_name)
            
            # 2. æ–‡æ¡£ç»“æ„è§£æ
            structure_info = await self.parse_document_structure(file_content, file_info['file_type'])
            
            # 3. å†…å®¹å…ƒç´ æå–
            content_elements = await self.extract_content_elements(file_content, file_info['file_type'])
            
            # 4. æ–‡æ¡£è´¨é‡åˆ†æ
            quality_info = await self.analyze_document_quality(file_content, structure_info)
            
            # 5. ç‰ˆæœ¬ä¿¡æ¯å’Œå…ƒæ•°æ®æå–
            version_info = await self.extract_version_info(file_content)
            metadata = await self.extract_metadata(file_content, file_name)
            
            # åˆå¹¶è§£æç»“æœ
            parsing_result = {
                "file_info": file_info,
                "structure": structure_info,
                "content_elements": content_elements,
                "quality_info": quality_info,
                "version_info": version_info,
                "metadata": metadata,
                "analysis_metadata": {
                    "stage": "document_parsing",
                    "parsing_method": "enhanced_parser",
                    "analysis_time": time.time() - start_time,
                    "content_length": len(file_content)
                }
            }
            
            duration = time.time() - start_time
            self._log_analysis_complete(task_id, "æ–‡æ¡£è§£æ", duration, len(str(parsing_result)))
            
            return self._create_response(
                success=True,
                data=parsing_result,
                metadata={"analysis_duration": duration, "stage": "document_parsing"}
            )
            
        except Exception as e:
            self._log_error(task_id, "æ–‡æ¡£è§£æ", e)
            return self._create_response(
                success=False,
                error=f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}",
                metadata={"stage": "document_parsing"}
            )
    
    async def identify_file_type(self, file_path: str, file_name: str) -> Dict[str, Any]:
        """æ–‡ä»¶æ ¼å¼è¯†åˆ«å’ŒåŸºæœ¬ä¿¡æ¯"""
        try:
            file_ext = Path(file_name).suffix.lower()
            file_size = os.path.getsize(file_path) if os.path.exists(file_path) else 0
            
            # æ–‡ä»¶ç±»å‹æ˜ å°„
            file_type_map = {
                '.pdf': 'pdf',
                '.docx': 'docx', '.doc': 'doc',
                '.xlsx': 'xlsx', '.xls': 'xls',
                '.pptx': 'pptx', '.ppt': 'ppt',
                '.md': 'markdown',
                '.txt': 'text'
            }
            
            file_type = file_type_map.get(file_ext, 'unknown')
            
            # è·å–æ–‡ä»¶æ—¶é—´ä¿¡æ¯
            file_info = {
                "file_type": file_type,
                "file_extension": file_ext,
                "file_size": file_size,
                "file_size_mb": round(file_size / (1024 * 1024), 2),
                "file_name": file_name,
                "supported": file_type in ['pdf', 'docx', 'doc', 'xlsx', 'xls', 'pptx', 'ppt', 'markdown', 'text']
            }
            
            if os.path.exists(file_path):
                stat = os.stat(file_path)
                file_info.update({
                    "creation_date": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_ctime)),
                    "modification_date": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_mtime)),
                    "access_date": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(stat.st_atime))
                })
            
            # åŸºäºæ–‡ä»¶ç±»å‹æ·»åŠ ç‰¹æ®Šå±æ€§
            if file_type == 'pdf':
                file_info.update({
                    "is_encrypted": False,  # éœ€è¦å®é™…æ£€æµ‹
                    "has_digital_signature": False,  # éœ€è¦å®é™…æ£€æµ‹
                    "pdf_version": "unknown"  # éœ€è¦å®é™…æ£€æµ‹
                })
            
            return file_info
            
        except Exception as e:
            return {
                "file_type": "unknown",
                "error": f"æ–‡ä»¶ä¿¡æ¯è·å–å¤±è´¥: {str(e)}"
            }
    
    async def parse_document_structure(self, content: str, file_type: str) -> Dict[str, Any]:
        """æ–‡æ¡£ç»“æ„è§£æ"""
        try:
            # æå–æ ‡é¢˜å’Œç« èŠ‚ç»“æ„
            sections = self._extract_sections(content)
            
            # åˆ†æç›®å½•ç»“æ„
            toc_info = self._analyze_table_of_contents(content, sections)
            
            # æ£€æµ‹æ–‡æ¡£ç»„ä»¶
            has_tables = bool(re.search(r'\|.*\|.*\|', content) or 'è¡¨æ ¼' in content or 'table' in content.lower())
            has_images = bool(re.search(r'!\[.*\]|å›¾\d+|å›¾ç‰‡|image', content, re.IGNORECASE))
            has_code = bool(re.search(r'```|ä»£ç |code|function|class', content, re.IGNORECASE))
            has_lists = bool(re.search(r'^\s*[-*+]\s+|^\s*\d+\.\s+', content, re.MULTILINE))
            
            structure_info = {
                "title": self._extract_document_title(content, sections),
                "sections": sections,
                "total_sections": len(sections),
                "max_depth": max([s.get('level', 1) for s in sections]) if sections else 1,
                "toc_available": toc_info['has_toc'],
                "toc_info": toc_info,
                "document_components": {
                    "has_tables": has_tables,
                    "has_images": has_images,
                    "has_code": has_code,
                    "has_lists": has_lists,
                    "has_links": bool(re.search(r'http[s]?://|www\.', content))
                },
                "structure_quality": self._assess_structure_quality(sections, content)
            }
            
            return structure_info
            
        except Exception as e:
            return {"error": f"ç»“æ„è§£æå¤±è´¥: {str(e)}"}
    
    async def extract_content_elements(self, content: str, file_type: str) -> Dict[str, Any]:
        """å†…å®¹å…ƒç´ æå–"""
        try:
            elements = {
                "text_content": content,
                "word_count": len(content.split()),
                "character_count": len(content),
                "paragraph_count": len([p for p in content.split('\n\n') if p.strip()]),
                "line_count": len(content.split('\n')),
                "tables": self._extract_tables(content),
                "images": self._extract_images(content),
                "lists": self._extract_lists(content),
                "code_blocks": self._extract_code_blocks(content),
                "hyperlinks": self._extract_hyperlinks(content),
                "key_phrases": await self._extract_key_phrases(content),
                "language": self._detect_language(content)
            }
            
            return elements
            
        except Exception as e:
            return {"error": f"å†…å®¹æå–å¤±è´¥: {str(e)}"}
    
    async def analyze_document_quality(self, content: str, structure_info: Dict[str, Any]) -> Dict[str, Any]:
        """æ–‡æ¡£è´¨é‡åˆ†æ"""
        try:
            # å¯è¯»æ€§è¯„åˆ†
            readability = self._assess_readability(content)
            
            # å®Œæ•´æ€§è¯„åˆ†
            completeness = self._assess_completeness(content, structure_info)
            
            # æ ¼å¼åŒ–è¯„åˆ†
            formatting = self._assess_formatting(content, structure_info)
            
            # ä¸€è‡´æ€§è¯„åˆ†
            consistency = self._assess_consistency(content, structure_info)
            
            quality_info = {
                "overall_score": round((readability['score'] + completeness['score'] + 
                                     formatting['score'] + consistency['score']) / 4, 1),
                "readability": readability,
                "completeness": completeness,
                "formatting": formatting,
                "consistency": consistency,
                "quality_level": self._get_quality_level(
                    (readability['score'] + completeness['score'] + 
                     formatting['score'] + consistency['score']) / 4
                ),
                "recommendations": self._generate_quality_recommendations(
                    readability, completeness, formatting, consistency
                )
            }
            
            return quality_info
            
        except Exception as e:
            return {"error": f"è´¨é‡åˆ†æå¤±è´¥: {str(e)}"}
    
    async def extract_version_info(self, content: str) -> Dict[str, Any]:
        """ç‰ˆæœ¬ä¿¡æ¯æå–"""
        try:
            # ç‰ˆæœ¬å·è¯†åˆ«
            version_patterns = [
                r'ç‰ˆæœ¬\s*[:ï¼š]?\s*([vV]?\d+\.\d+(?:\.\d+)?)',
                r'version\s*[:ï¼š]?\s*([vV]?\d+\.\d+(?:\.\d+)?)',
                r'v(\d+\.\d+(?:\.\d+)?)',
                r'V(\d+\.\d+(?:\.\d+)?)'
            ]
            
            document_version = None
            for pattern in version_patterns:
                match = re.search(pattern, content, re.IGNORECASE)
                if match:
                    document_version = match.group(1)
                    break
            
            # å˜æ›´æ—¥å¿—è¯†åˆ«
            change_log = self._extract_change_log(content)
            
            # ç‰ˆæœ¬å†å²è¯†åˆ«
            version_history = self._extract_version_history(content)
            
            version_info = {
                "document_version": document_version or "æœªæ‰¾åˆ°ç‰ˆæœ¬ä¿¡æ¯",
                "version_history": version_history,
                "change_log": change_log,
                "last_modified": self._extract_last_modified_date(content),
                "version_control_info": self._extract_version_control_info(content)
            }
            
            return version_info
            
        except Exception as e:
            return {"error": f"ç‰ˆæœ¬ä¿¡æ¯æå–å¤±è´¥: {str(e)}"}
    
    async def extract_metadata(self, content: str, file_name: str) -> Dict[str, Any]:
        """å…ƒæ•°æ®æå–"""
        try:
            # ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ‘˜è¦å’Œæå–å…³é”®è¯
            keywords = await self._extract_key_phrases(content)
            summary = await self._generate_summary(content)
            
            metadata = {
                "author": self._extract_author(content),
                "company": self._extract_company(content),
                "project": self._extract_project_name(content, file_name),
                "keywords": keywords,
                "summary": summary,
                "category": self._classify_document(content),
                "importance_level": self._assess_importance(content),
                "technical_level": self._assess_technical_level(content),
                "target_audience": self._identify_target_audience(content)
            }
            
            return metadata
            
        except Exception as e:
            return {"error": f"å…ƒæ•°æ®æå–å¤±è´¥: {str(e)}"}
    
    # è¾…åŠ©æ–¹æ³•å®ç°
    def _extract_sections(self, content: str) -> List[Dict[str, Any]]:
        """æå–ç« èŠ‚ç»“æ„"""
        sections = []
        lines = content.split('\n')
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            # Markdownæ ¼å¼æ ‡é¢˜
            if line.startswith('#'):
                level = len(line) - len(line.lstrip('#'))
                title = line.lstrip('#').strip()
                if title:
                    sections.append({
                        "level": level,
                        "title": title,
                        "line_number": i + 1,
                        "type": "markdown_header"
                    })
            
            # æ•°å­—ç¼–å·æ ‡é¢˜
            elif re.match(r'^\d+(\.\d+)*\.?\s+', line):
                level = line.count('.') + 1
                title = re.sub(r'^\d+(\.\d+)*\.?\s+', '', line)
                if title:
                    sections.append({
                        "level": level,
                        "title": title,
                        "line_number": i + 1,
                        "type": "numbered_header"
                    })
        
        return sections
    
    def _extract_document_title(self, content: str, sections: List[Dict]) -> str:
        """æå–æ–‡æ¡£æ ‡é¢˜"""
        lines = content.split('\n')
        
        # å°è¯•ä»ç¬¬ä¸€è¡Œè·å–
        first_line = lines[0].strip() if lines else ""
        if first_line and len(first_line) < 100:
            return first_line
        
        # å°è¯•ä»ç¬¬ä¸€ä¸ªæ ‡é¢˜è·å–
        if sections:
            return sections[0]['title']
        
        # ä»æ–‡ä»¶åæ¨æ–­
        return "æœªæ‰¾åˆ°æ–‡æ¡£æ ‡é¢˜"
    
    def _assess_readability(self, content: str) -> Dict[str, Any]:
        """è¯„ä¼°å¯è¯»æ€§"""
        words = content.split()
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
        
        avg_word_per_sentence = len(words) / max(len(sentences), 1)
        
        # ç®€å•çš„å¯è¯»æ€§è¯„åˆ†
        if avg_word_per_sentence < 15:
            score = 90
            level = "ä¼˜ç§€"
        elif avg_word_per_sentence < 25:
            score = 75
            level = "è‰¯å¥½"
        elif avg_word_per_sentence < 35:
            score = 60
            level = "ä¸€èˆ¬"
        else:
            score = 40
            level = "è¾ƒå·®"
        
        return {
            "score": score,
            "level": level,
            "avg_words_per_sentence": round(avg_word_per_sentence, 1),
            "total_sentences": len(sentences)
        }
    
    def _assess_completeness(self, content: str, structure_info: Dict) -> Dict[str, Any]:
        """è¯„ä¼°å®Œæ•´æ€§"""
        required_sections = ["æ¦‚è¿°", "éœ€æ±‚", "åŠŸèƒ½", "æŠ€æœ¯", "æ€»ç»“"]
        found_sections = []
        
        content_lower = content.lower()
        for section in required_sections:
            if section in content or section.lower() in content_lower:
                found_sections.append(section)
        
        completeness_ratio = len(found_sections) / len(required_sections)
        score = int(completeness_ratio * 100)
        
        return {
            "score": score,
            "found_sections": found_sections,
            "missing_sections": [s for s in required_sections if s not in found_sections],
            "section_coverage": round(completeness_ratio * 100, 1)
        }
    
    def _assess_formatting(self, content: str, structure_info: Dict) -> Dict[str, Any]:
        """è¯„ä¼°æ ¼å¼åŒ–è´¨é‡"""
        sections = structure_info.get('sections', [])
        
        # æ£€æŸ¥æ ‡é¢˜ä¸€è‡´æ€§
        consistent_headers = len(set(s.get('type', '') for s in sections)) <= 2
        
        # æ£€æŸ¥ç»“æ„å±‚æ¬¡
        proper_hierarchy = all(s.get('level', 1) <= 4 for s in sections)
        
        score = 80
        if consistent_headers:
            score += 10
        if proper_hierarchy:
            score += 10
        
        return {
            "score": min(score, 100),
            "consistent_headers": consistent_headers,
            "proper_hierarchy": proper_hierarchy,
            "total_sections": len(sections)
        }
    
    def _assess_consistency(self, content: str, structure_info: Dict) -> Dict[str, Any]:
        """è¯„ä¼°ä¸€è‡´æ€§"""
        # ç®€å•çš„ä¸€è‡´æ€§æ£€æŸ¥
        score = 80  # åŸºç¡€åˆ†æ•°
        
        return {
            "score": score,
            "terminology_consistent": True,
            "style_consistent": True
        }
    
    def _get_quality_level(self, score: float) -> str:
        """è·å–è´¨é‡ç­‰çº§"""
        if score >= 85:
            return "ä¼˜ç§€"
        elif score >= 70:
            return "è‰¯å¥½"
        elif score >= 60:
            return "ä¸€èˆ¬"
        else:
            return "éœ€è¦æ”¹è¿›"
    
    def _generate_quality_recommendations(self, readability: Dict, completeness: Dict, 
                                        formatting: Dict, consistency: Dict) -> List[str]:
        """ç”Ÿæˆè´¨é‡æ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if readability['score'] < 70:
            recommendations.append("å»ºè®®ç¼©çŸ­å¥å­é•¿åº¦ï¼Œæé«˜å¯è¯»æ€§")
        
        if completeness['score'] < 80:
            recommendations.append(f"å»ºè®®è¡¥å……ç¼ºå¤±ç« èŠ‚: {', '.join(completeness['missing_sections'])}")
        
        if formatting['score'] < 80:
            recommendations.append("å»ºè®®ç»Ÿä¸€æ ‡é¢˜æ ¼å¼ï¼Œæ”¹å–„æ–‡æ¡£ç»“æ„")
        
        if not recommendations:
            recommendations.append("æ–‡æ¡£è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¿æŒ")
        
        return recommendations
    
    # ... å…¶ä»–è¾…åŠ©æ–¹æ³•çš„å®ç°
    def _extract_tables(self, content: str) -> List[Dict]:
        """æå–è¡¨æ ¼"""
        # ç®€åŒ–å®ç°
        return []
    
    def _extract_images(self, content: str) -> List[Dict]:
        """æå–å›¾ç‰‡ä¿¡æ¯"""
        # ç®€åŒ–å®ç°
        return []
    
    def _extract_lists(self, content: str) -> List[Dict]:
        """æå–åˆ—è¡¨"""
        # ç®€åŒ–å®ç°
        return []
    
    def _extract_code_blocks(self, content: str) -> List[Dict]:
        """æå–ä»£ç å—"""
        # ç®€åŒ–å®ç°
        return []
    
    def _extract_hyperlinks(self, content: str) -> List[Dict]:
        """æå–è¶…é“¾æ¥"""
        # ç®€åŒ–å®ç°
        return []
    
    async def _extract_key_phrases(self, content: str) -> List[str]:
        """ä½¿ç”¨å¤§æ¨¡å‹æå–å…³é”®çŸ­è¯­"""
        if not self.llm_client:
            # å›é€€åˆ°ç®€å•çš„å…³é”®è¯æå–
            return self._extract_keywords_fallback(content)
        
        try:
            # æ„å»ºprompt
            prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ä¸­æå–8-12ä¸ªæœ€é‡è¦çš„å…³é”®è¯æˆ–å…³é”®çŸ­è¯­ã€‚

è¦æ±‚ï¼š
1. æå–èƒ½å¤Ÿä»£è¡¨æ–‡æ¡£æ ¸å¿ƒå†…å®¹çš„å…³é”®è¯
2. åŒ…æ‹¬æŠ€æœ¯æœ¯è¯­ã€ä¸šåŠ¡æœ¯è¯­ã€åŠŸèƒ½æ¨¡å—ç­‰
3. æ¯ä¸ªå…³é”®è¯ä¸è¶…è¿‡10ä¸ªå­—ç¬¦
4. æŒ‰é‡è¦æ€§æ’åº
5. åªè¿”å›å…³é”®è¯åˆ—è¡¨ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”

æ–‡æ¡£å†…å®¹ï¼š
{content[:2000]}

å…³é”®è¯ï¼š"""

            # è°ƒç”¨å¤§æ¨¡å‹
            response = self.llm_client.chat([
                {"role": "user", "content": prompt}
            ])
            
            if response:
                keywords_text = response.strip()
                # è§£æå…³é”®è¯
                keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
                return keywords[:12]  # æœ€å¤šè¿”å›12ä¸ªå…³é”®è¯
            
        except Exception as e:
            self.logger.warning(f"å¤§æ¨¡å‹æå–å…³é”®è¯å¤±è´¥: {e}, ä½¿ç”¨å›é€€æ–¹æ¡ˆ")
        
        return self._extract_keywords_fallback(content)
    
    def _detect_language(self, content: str) -> str:
        """æ£€æµ‹è¯­è¨€"""
        chinese_chars = sum(1 for char in content if '\u4e00' <= char <= '\u9fff')
        english_chars = sum(1 for char in content if char.isalpha() and ord(char) < 128)
        
        if chinese_chars > english_chars:
            return "ä¸­æ–‡"
        elif english_chars > 0:
            return "è‹±æ–‡"
        else:
            return "æœªçŸ¥"
    
    def _analyze_table_of_contents(self, content: str, sections: List) -> Dict:
        """åˆ†æç›®å½•ç»“æ„"""
        return {
            "has_toc": len(sections) > 3,
            "toc_quality": "è‰¯å¥½" if len(sections) > 5 else "ä¸€èˆ¬"
        }
    
    def _assess_structure_quality(self, sections: List, content: str) -> str:
        """è¯„ä¼°ç»“æ„è´¨é‡"""
        if len(sections) > 5:
            return "ç»“æ„æ¸…æ™°"
        elif len(sections) > 2:
            return "ç»“æ„ä¸€èˆ¬"
        else:
            return "ç»“æ„ç®€å•"
    
    def _extract_change_log(self, content: str) -> List[Dict]:
        """æå–å˜æ›´æ—¥å¿—"""
        return []
    
    def _extract_version_history(self, content: str) -> List[Dict]:
        """æå–ç‰ˆæœ¬å†å²"""
        return []
    
    def _extract_last_modified_date(self, content: str) -> Optional[str]:
        """æå–æœ€åä¿®æ”¹æ—¥æœŸ"""
        return None
    
    def _extract_version_control_info(self, content: str) -> Dict:
        """æå–ç‰ˆæœ¬æ§åˆ¶ä¿¡æ¯"""
        return {}
    
    def _extract_author(self, content: str) -> str:
        """æå–ä½œè€…"""
        return "æœªæ‰¾åˆ°ä½œè€…ä¿¡æ¯"
    
    def _extract_company(self, content: str) -> str:
        """æå–å…¬å¸ä¿¡æ¯"""
        return "æœªæ‰¾åˆ°å…¬å¸ä¿¡æ¯"
    
    def _extract_project_name(self, content: str, file_name: str) -> str:
        """æå–é¡¹ç›®åç§°"""
        return Path(file_name).stem
    
    def _extract_keywords_fallback(self, content: str) -> List[str]:
        """å›é€€æ–¹æ¡ˆï¼šç®€å•çš„å…³é”®è¯æå–"""
        import re
        
        # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2-6å­—ç¬¦ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,6}', content)
        
        # æå–è‹±æ–‡å•è¯
        english_words = re.findall(r'\b[A-Za-z]{3,12}\b', content)
        
        # å¸¸è§æŠ€æœ¯è¯æ±‡ä¼˜å…ˆçº§æ›´é«˜
        tech_keywords = []
        common_tech_terms = [
            'ç³»ç»Ÿ', 'æ¨¡å—', 'åŠŸèƒ½', 'æ¥å£', 'æ•°æ®åº“', 'æœåŠ¡', 'æ¶æ„', 'è®¾è®¡',
            'éœ€æ±‚', 'ä¼˜åŒ–', 'é…ç½®', 'ç®¡ç†', 'ç”¨æˆ·', 'æƒé™', 'å®‰å…¨', 'æ€§èƒ½',
            'API', 'HTTP', 'JSON', 'XML', 'SQL', 'Redis', 'MySQL', 'MongoDB'
        ]
        
        for term in common_tech_terms:
            if term in content:
                tech_keywords.append(term)
        
        # åˆå¹¶å¹¶å»é‡
        all_keywords = tech_keywords + chinese_words + english_words
        unique_keywords = list(dict.fromkeys(all_keywords))  # ä¿æŒé¡ºåºå»é‡
        
        return unique_keywords[:10]  # è¿”å›å‰10ä¸ªå…³é”®è¯
    
    def _extract_keywords(self, content: str) -> List[str]:
        """æå–å…³é”®è¯ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰"""
        return self._extract_keywords_fallback(content)
    
    async def _generate_summary(self, content: str) -> str:
        """ä½¿ç”¨å¤§æ¨¡å‹ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        if not self.llm_client:
            # å›é€€åˆ°ç®€å•æˆªå–
            return content[:200] + "..." if len(content) > 200 else content
        
        try:
            # æ„å»ºprompt
            prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆä¸€ä¸ª150-300å­—çš„å‡†ç¡®æ‘˜è¦ã€‚

è¦æ±‚ï¼š
1. æ¦‚æ‹¬æ–‡æ¡£çš„ä¸»è¦å†…å®¹å’Œç›®çš„
2. åŒ…å«å…³é”®åŠŸèƒ½ã€æŠ€æœ¯è¦ç‚¹æˆ–ä¸šåŠ¡éœ€æ±‚
3. è¯­è¨€ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹
4. ä¿æŒå®¢è§‚ä¸­æ€§çš„è¯­è°ƒ
5. å¦‚æœæ˜¯æŠ€æœ¯æ–‡æ¡£ï¼Œè¦æåŠæ ¸å¿ƒæŠ€æœ¯æ ˆæˆ–æ¶æ„
6. å¦‚æœæ˜¯éœ€æ±‚æ–‡æ¡£ï¼Œè¦è¯´æ˜ä¸»è¦åŠŸèƒ½å’Œä¸šåŠ¡ç›®æ ‡

æ–‡æ¡£å†…å®¹ï¼š
{content[:3000]}

æ‘˜è¦ï¼š"""

            # è°ƒç”¨å¤§æ¨¡å‹
            response = self.llm_client.chat([
                {"role": "user", "content": prompt}
            ])
            
            if response:
                summary = response.strip()
                # ç¡®ä¿æ‘˜è¦é•¿åº¦åˆç†
                if len(summary) > 500:
                    summary = summary[:497] + "..."
                return summary
            
        except Exception as e:
            self.logger.warning(f"å¤§æ¨¡å‹ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}, ä½¿ç”¨å›é€€æ–¹æ¡ˆ")
        
        # å›é€€æ–¹æ¡ˆ
        return content[:200] + "..." if len(content) > 200 else content
    
    def _classify_document(self, content: str) -> str:
        """æ–‡æ¡£åˆ†ç±»"""
        if "éœ€æ±‚" in content:
            return "éœ€æ±‚æ–‡æ¡£"
        elif "è®¾è®¡" in content:
            return "è®¾è®¡æ–‡æ¡£"
        elif "æŠ€æœ¯" in content:
            return "æŠ€æœ¯æ–‡æ¡£"
        else:
            return "å…¶ä»–æ–‡æ¡£"
    
    def _assess_importance(self, content: str) -> str:
        """è¯„ä¼°é‡è¦æ€§"""
        return "ä¸­ç­‰"
    
    def _assess_technical_level(self, content: str) -> str:
        """è¯„ä¼°æŠ€æœ¯æ°´å¹³"""
        return "ä¸­ç­‰"
    
    def _identify_target_audience(self, content: str) -> str:
        """è¯†åˆ«ç›®æ ‡å—ä¼—"""
        return "é¡¹ç›®å›¢é˜Ÿ"
    
    async def _read_file_content(self, file_path: str, file_name: str) -> str:
        """æ ¹æ®æ–‡ä»¶ç±»å‹è¯»å–æ–‡ä»¶å†…å®¹"""
        try:
            file_ext = Path(file_name).suffix.lower()
            
            if file_ext == '.docx':
                return await self._read_docx_content(file_path)
            elif file_ext == '.doc':
                return await self._read_doc_content(file_path)
            elif file_ext == '.pdf':
                return await self._read_pdf_content(file_path)
            elif file_ext in ['.txt', '.md', '.csv', '.log', '.json', '.xml', '.html']:
                return await self._read_text_content(file_path)
            else:
                # å¯¹äºæœªçŸ¥æ ¼å¼ï¼Œå°è¯•ä½œä¸ºæ–‡æœ¬è¯»å–
                return await self._read_text_content(file_path)
                
        except Exception as e:
            self.logger.error(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return ""
    
    async def _read_docx_content(self, file_path: str) -> str:
        """è¯»å–Word DOCXæ–‡æ¡£å†…å®¹"""
        try:
            # å°è¯•ä½¿ç”¨python-docxåº“
            try:
                from docx import Document
                doc = Document(file_path)
                content = []
                for paragraph in doc.paragraphs:
                    if paragraph.text.strip():
                        content.append(paragraph.text.strip())
                return '\n\n'.join(content)
            except ImportError:
                self.logger.warning("python-docxåº“æœªå®‰è£…ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
                # å¤‡ç”¨æ–¹æ³•ï¼šå°è¯•è§£å‹docxæ–‡ä»¶å¹¶è¯»å–XML
                return await self._read_docx_fallback(file_path)
        except Exception as e:
            self.logger.error(f"è¯»å–DOCXæ–‡ä»¶å¤±è´¥: {e}")
            return ""
    
    async def _read_docx_fallback(self, file_path: str) -> str:
        """DOCXæ–‡ä»¶å¤‡ç”¨è¯»å–æ–¹æ³•"""
        try:
            import zipfile
            import xml.etree.ElementTree as ET
            
            with zipfile.ZipFile(file_path, 'r') as docx:
                # è¯»å–ä¸»æ–‡æ¡£å†…å®¹
                try:
                    xml_content = docx.read('word/document.xml')
                    root = ET.fromstring(xml_content)
                    
                    # æå–æ–‡æœ¬å†…å®¹
                    namespaces = {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}
                    texts = []
                    for paragraph in root.findall('.//w:p', namespaces):
                        para_text = []
                        for text_elem in paragraph.findall('.//w:t', namespaces):
                            if text_elem.text:
                                para_text.append(text_elem.text)
                        if para_text:
                            texts.append(''.join(para_text))
                    
                    return '\n\n'.join(texts)
                except:
                    # å¦‚æœXMLè§£æå¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬çš„å ä½å†…å®¹
                    return f"Wordæ–‡æ¡£è§£æå¤±è´¥ï¼Œæ–‡ä»¶è·¯å¾„: {file_path}"
        except Exception as e:
            self.logger.error(f"DOCXå¤‡ç”¨è¯»å–æ–¹æ³•å¤±è´¥: {e}")
            return ""
    
    async def _read_doc_content(self, file_path: str) -> str:
        """è¯»å–Word DOCæ–‡æ¡£å†…å®¹ï¼ˆæ—§æ ¼å¼ï¼‰"""
        try:
            # å¯¹äº.docæ–‡ä»¶ï¼Œæˆ‘ä»¬å…ˆè¿”å›ä¸€ä¸ªå ä½å†…å®¹
            # å®é™…ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦å®‰è£…python-docx2txtæˆ–å…¶ä»–åº“
            return f"Word DOCæ–‡æ¡£ï¼ˆæ—§æ ¼å¼ï¼‰ï¼Œéœ€è¦ä¸“é—¨çš„è§£æåº“ã€‚æ–‡ä»¶è·¯å¾„: {file_path}"
        except Exception as e:
            self.logger.error(f"è¯»å–DOCæ–‡ä»¶å¤±è´¥: {e}")
            return ""
    
    async def _read_pdf_content(self, file_path: str) -> str:
        """è¯»å–PDFæ–‡æ¡£å†…å®¹"""
        try:
            # å¯¹äºPDFæ–‡ä»¶ï¼Œæˆ‘ä»¬å…ˆè¿”å›ä¸€ä¸ªå ä½å†…å®¹
            # å®é™…ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦å®‰è£…PyPDF2æˆ–pdfplumber
            return f"PDFæ–‡æ¡£ï¼Œéœ€è¦ä¸“é—¨çš„è§£æåº“ã€‚æ–‡ä»¶è·¯å¾„: {file_path}"
        except Exception as e:
            self.logger.error(f"è¯»å–PDFæ–‡ä»¶å¤±è´¥: {e}")
            return ""
    
    async def _read_text_content(self, file_path: str) -> str:
        """è¯»å–çº¯æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    return f.read()
            except:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
        except Exception as e:
            self.logger.error(f"è¯»å–æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
            return ""
    
    async def _generate_empty_content_result(self, task_id: str, file_name: str, file_path: str) -> Dict[str, Any]:
        """ç”Ÿæˆç©ºå†…å®¹çš„é»˜è®¤è§£æç»“æœ"""
        self.logger.info(f"ç”Ÿæˆç©ºå†…å®¹çš„é»˜è®¤è§£æç»“æœ - Task: {task_id}")
        
        parsing_result = {
            "file_info": {
                "file_type": "unknown",
                "file_extension": Path(file_name).suffix.lower() if file_name else "",
                "file_size": 0,
                "file_size_mb": 0.0,
                "file_name": file_name,
                "supported": False,
                "error": "æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è¯»å–"
            },
            "structure": {
                "title": "æœªæ‰¾åˆ°æ–‡æ¡£æ ‡é¢˜",
                "sections": [],
                "total_sections": 0,
                "max_depth": 1,
                "toc_available": False,
                "toc_info": {"has_toc": False, "toc_quality": "æ— "},
                "document_components": {
                    "has_tables": False,
                    "has_images": False,
                    "has_code": False,
                    "has_lists": False,
                    "has_links": False
                },
                "structure_quality": "æ— ç»“æ„"
            },
            "content_elements": {
                "text_content": "",
                "word_count": 0,
                "character_count": 0,
                "paragraph_count": 0,
                "line_count": 0,
                "tables": [],
                "images": [],
                "lists": [],
                "code_blocks": [],
                "hyperlinks": [],
                "key_phrases": ["æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æ"],
                "language": "æœªçŸ¥"
            },
            "quality_info": {
                "overall_score": 0,
                "readability": {"score": 0, "level": "æ— æ³•è¯„ä¼°"},
                "completeness": {"score": 0, "found_sections": [], "missing_sections": []},
                "formatting": {"score": 0, "consistent_headers": False},
                "consistency": {"score": 0, "terminology_consistent": False},
                "quality_level": "æ— æ³•è¯„ä¼°",
                "recommendations": ["è¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå†…å®¹å®Œæ•´æ€§"]
            },
            "version_info": {
                "document_version": "æœªæ‰¾åˆ°ç‰ˆæœ¬ä¿¡æ¯",
                "version_history": [],
                "change_log": [],
                "last_modified": None,
                "version_control_info": {}
            },
            "metadata": {
                "author": "æœªæ‰¾åˆ°ä½œè€…ä¿¡æ¯",
                "company": "æœªæ‰¾åˆ°å…¬å¸ä¿¡æ¯",
                "project": file_name or "æœªçŸ¥é¡¹ç›®",
                "keywords": ["ç©ºæ–‡æ¡£", "è§£æå¤±è´¥"],
                "summary": "æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•è§£æï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚",
                "category": "å…¶ä»–æ–‡æ¡£",
                "importance_level": "ä½",
                "technical_level": "æ— ",
                "target_audience": "æœªçŸ¥"
            },
            "analysis_metadata": {
                "stage": "document_parsing",
                "parsing_method": "empty_content_handler",
                "analysis_time": 0.1,
                "content_length": 0
            }
        }
        
        return self._create_response(
            success=True,
            data=parsing_result,
            metadata={"analysis_duration": 0.1, "stage": "document_parsing"}
        ) 