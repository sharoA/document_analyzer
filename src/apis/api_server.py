#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
analyDesign APIæœåŠ¡å™¨
é›†æˆç«å±±å¼•æ“å®æ—¶äº¤äº’åŠŸèƒ½å’Œæ–‡ä»¶å¤„ç†åŠŸèƒ½
"""

import json
import time
import uuid
import logging
import os
import base64
from datetime import datetime
from typing import Dict, Any, Optional, List
from concurrent.futures import ThreadPoolExecutor

from flask import Flask, request, jsonify, make_response
from flask_cors import CORS

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—é…ç½®
try:
    from ..utils.logger_config import initialize_logging, get_logger, log_api_request, log_analysis_step
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç»å¯¹å¯¼å…¥
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from src.utils.logger_config import initialize_logging, get_logger, log_api_request, log_analysis_step

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
initialize_logging()

# è·å–æ—¥å¿—å™¨
logger = get_logger('api_server')

# å¯¼å…¥é…ç½®å’Œå·¥å…·ç±»
try:
    from ..resource.config import get_config
    from ..utils.volcengine_client import VolcengineClient, VolcengineConfig
    from ..utils.llm_logger import LLMLogger
    from ..utils.task_storage import get_task_storage
    from ..utils.redis_task_storage import get_redis_task_storage
    # å¯¼å…¥åˆ†ææœåŠ¡æ¨¡å—
    from ..analysis_services import (
        AnalysisServiceManager,
        get_analysis_service_manager,
        ensure_analysis_directories,
        setup_analysis_logger,
        validate_input,
        validate_file_upload,
        initialize_analysis_service_manager
    )
    try:
        from ..utils.enhanced_analyzer import EnhancedAnalyzer
        ENHANCED_ANALYZER_AVAILABLE = True
    except ImportError:
        ENHANCED_ANALYZER_AVAILABLE = False
        logger.warning("enhanced_analyzerä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æ–‡æœ¬å¤„ç†")
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from src.resource.config import get_config
    from src.utils.volcengine_client import VolcengineClient, VolcengineConfig
    from src.utils.llm_logger import LLMLogger
    from src.utils.task_storage import get_task_storage
    from src.utils.redis_task_storage import get_redis_task_storage
    # å¯¼å…¥åˆ†ææœåŠ¡æ¨¡å—
    from src.analysis_services import (
        AnalysisServiceManager,
        get_analysis_service_manager,
        ensure_analysis_directories,
        setup_analysis_logger,
        validate_input,
        validate_file_upload,
        initialize_analysis_service_manager
    )
    try:
        from src.utils.enhanced_analyzer import EnhancedAnalyzer
        ENHANCED_ANALYZER_AVAILABLE = True
    except ImportError:
        ENHANCED_ANALYZER_AVAILABLE = False
        logger.warning("enhanced_analyzerä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æ–‡æœ¬å¤„ç†")

# è·å–é…ç½®
config = get_config()
task_storage = get_task_storage()
redis_task_storage = get_redis_task_storage()

app = Flask(__name__)
# é…ç½®CORSï¼Œå…è®¸æ‰€æœ‰æ¥æºå’Œæ–¹æ³•
CORS(app, 
     origins=["http://localhost:3000", "http://127.0.0.1:3000"],
     methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
     allow_headers=["Content-Type", "Authorization"],
     supports_credentials=True)

# æ·»åŠ è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
@app.before_request
def log_request_info():
    """è®°å½•è¯·æ±‚ä¿¡æ¯"""
    import time
    request.start_time = time.time()

@app.after_request  
def log_request_result(response):
    """è®°å½•è¯·æ±‚ç»“æœ"""
    import time
    duration = time.time() - getattr(request, 'start_time', time.time())
    log_api_request(
        method=request.method,
        endpoint=request.endpoint or request.path,
        status_code=response.status_code,
        duration=duration
    )
    return response

# ç¡®ä¿åˆ†ææœåŠ¡ç›®å½•ç»“æ„å­˜åœ¨
ensure_analysis_directories()

# è®¾ç½®åˆ†ææœåŠ¡æ—¥å¿—
analysis_logger = setup_analysis_logger("api_server")

# åˆå§‹åŒ–ç«å±±å¼•æ“å®¢æˆ·ç«¯
try:
    volcengine_config = config.get_volcengine_config()
    logger.info(f"ä»é…ç½®æ–‡ä»¶è¯»å–ç«å±±å¼•æ“é…ç½®: {volcengine_config}")
    
    # æ£€æŸ¥APIå¯†é’¥
    api_key = volcengine_config.get('api_key', '')
    if not api_key:
        raise ValueError("ç«å±±å¼•æ“APIå¯†é’¥æœªåœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®")
    
    volcano_config = VolcengineConfig(
        api_key=api_key,
        model_id=volcengine_config.get('model', 'doubao-pro-4k'),
        base_url=volcengine_config.get('endpoint', 'https://ark.cn-beijing.volces.com/api/v3'),
        temperature=volcengine_config.get('temperature', 0.7),
        max_tokens=volcengine_config.get('max_tokens', 4000)
    )
    volcano_client = VolcengineClient(volcano_config)
    logger.info("ç«å±±å¼•æ“å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"ç«å±±å¼•æ“å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    volcano_client = None

# åˆå§‹åŒ–åˆ†ææœåŠ¡ç®¡ç†å™¨
try:
    # å…ˆåˆå§‹åŒ–åˆ†ææœåŠ¡ç®¡ç†å™¨
    # ä»é…ç½®æ–‡ä»¶è¯»å–å‘é‡æ•°æ®åº“ç±»å‹å’Œé…ç½®
    vector_db_config = config.get_vector_database_config()
    vector_db_type = vector_db_config.get('type', 'mock')  # é»˜è®¤ä½¿ç”¨mock
    
    # å‡†å¤‡å‘é‡æ•°æ®åº“å‚æ•°
    vector_db_kwargs = {}
    if vector_db_type.lower() == 'weaviate':
        weaviate_config = vector_db_config.get('weaviate', {})
        vector_db_kwargs.update({
            'host': weaviate_config.get('host', 'localhost'),
            'port': weaviate_config.get('port', 8080),
            'grpc_port': weaviate_config.get('grpc_port', 50051),
            'scheme': weaviate_config.get('scheme', 'http'),
            'api_key': weaviate_config.get('api_key'),
            'collection_name': weaviate_config.get('default_collection', {}).get('name', 'AnalyDesignDocuments')
        })
        logger.info(f"Weaviateé…ç½®: {weaviate_config.get('scheme', 'http')}://{weaviate_config.get('host', 'localhost')}:{weaviate_config.get('port', 8080)}")
    
    analysis_service_manager = initialize_analysis_service_manager(
        llm_client=volcano_client,
        vector_db_type=vector_db_type,
        **vector_db_kwargs
    )
    logger.info("åˆ†ææœåŠ¡ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    analysis_logger.info("åˆ†ææœåŠ¡ç®¡ç†å™¨å·²é›†æˆåˆ°APIæœåŠ¡å™¨")
except Exception as e:
    logger.error(f"åˆ†ææœåŠ¡ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    analysis_service_manager = None

# åˆå§‹åŒ–å¢å¼ºåˆ†æå™¨
try:
    if ENHANCED_ANALYZER_AVAILABLE:
        analyzer = EnhancedAnalyzer()
        logger.info("å¢å¼ºåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
    else:
        analyzer = None
        logger.info("ä½¿ç”¨åŸºç¡€æ–‡æœ¬å¤„ç†åŠŸèƒ½")
except Exception as e:
    logger.error(f"å¢å¼ºåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    analyzer = None

# ä¼šè¯å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨Redisç­‰æŒä¹…åŒ–å­˜å‚¨ï¼‰
sessions = {}

# çº¿ç¨‹æ± ç”¨äºå¼‚æ­¥å¤„ç†
executor = ThreadPoolExecutor(max_workers=4)

# æ–‡ä»¶å¤„ç†ä»»åŠ¡ç±»
class FileParsingTask:
    def __init__(self, task_id: str, file_info: dict, file_path: str = None):
        self.id = task_id
        self.file_info = file_info
        self.file_content = None  # ä¸å†å­˜å‚¨æ–‡ä»¶å†…å®¹ï¼Œåªå­˜å‚¨è·¯å¾„
        self.file_path = file_path
        self.status = "pending"
        self.progress = 0
        self.steps = []
        self.result = None
        self.content_analysis = None
        self.ai_analysis = None
        self.markdown_content = None
        self.error = None
        self.created_at = datetime.now()
        self.updated_at = datetime.now()
        
        # ç«‹å³ä¿å­˜åˆ°æ•°æ®åº“
        self._save_to_db()
    
    def _save_to_db(self):
        """ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“"""
        try:
            # ä¿å­˜åˆ°SQLiteæ•°æ®åº“
            task_storage.create_task(
                filename=self.file_info.get('filename', ''),
                file_size=self.file_info.get('size', 0),
                file_type=self.file_info.get('type', ''),
                task_id=self.id
            )
            # åŒæ—¶ä¿å­˜åˆ°Redis
            redis_task_storage.create_task(
                filename=self.file_info.get('filename', ''),
                file_size=self.file_info.get('size', 0),
                file_type=self.file_info.get('type', ''),
                task_id=self.id
            )
        except Exception as e:
            logger.error(f"ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“å¤±è´¥: {e}")
    
    def update_progress(self, progress: int, description: str, status: str = None):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        self.progress = progress
        self.updated_at = datetime.now()
        if status:
            self.status = status
        
        # æ›´æ–°æ­¥éª¤ä¿¡æ¯
        step_info = {
            "description": description,
            "progress": progress,
            "status": status or self.status,
            "timestamp": self.updated_at.isoformat()
        }
        
        # ç¡®ä¿ steps æ˜¯ä¸€ä¸ªåˆ—è¡¨
        if not isinstance(self.steps, list):
            self.steps = []
        
        # å¦‚æœæ˜¯æ–°æ­¥éª¤ï¼Œæ·»åŠ åˆ°åˆ—è¡¨ä¸­
        try:
            # å®‰å…¨æ£€æŸ¥æœ€åä¸€ä¸ªæ­¥éª¤
            if (not self.steps or 
                not isinstance(self.steps[-1], dict) or 
                self.steps[-1].get("description") != description):
                self.steps.append(step_info)
            else:
                # æ›´æ–°æœ€åä¸€ä¸ªæ­¥éª¤
                self.steps[-1] = step_info
        except (IndexError, KeyError, TypeError) as e:
            # å¦‚æœå‡ºç°ä»»ä½•é”™è¯¯ï¼Œç›´æ¥æ·»åŠ æ–°æ­¥éª¤
            logger.warning(f"æ­¥éª¤æ›´æ–°å¼‚å¸¸ï¼Œæ·»åŠ æ–°æ­¥éª¤: {e}")
            self.steps.append(step_info)
        
        # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
        try:
            task_storage.update_task_status(
                task_id=self.id,
                status=self.status,
                progress=self.progress
            )
        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        
        logger.info(f"ä»»åŠ¡ {self.id} è¿›åº¦æ›´æ–°: {progress}% - {description}")
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        # ç¡®ä¿ file_info å­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
        file_info = self.file_info
        if file_info is None:
            file_info = {
                'name': 'æœªçŸ¥æ–‡ä»¶',
                'type': 'unknown',
                'size': 0
            }
        
        return {
            "id": self.id,
            "file_info": file_info,
            "file_path": self.file_path,
            "status": self.status,
            "progress": self.progress,
            "steps": self.steps,
            "result": self.result,
            "content_analysis": self.content_analysis,
            "ai_analysis": self.ai_analysis,
            "markdown_content": self.markdown_content,
            "error": self.error,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None
        }
    
    @classmethod
    def from_dict(cls, data: dict):
        """ä»å­—å…¸åˆ›å»ºä»»åŠ¡å®ä¾‹"""
        task = cls.__new__(cls)  # åˆ›å»ºå®ä¾‹ä½†ä¸è°ƒç”¨__init__
        task.id = data['id']
        
        # ç¡®ä¿ file_info å­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
        task.file_info = data.get('file_info')
        if task.file_info is None:
            task.file_info = {
                'name': 'æœªçŸ¥æ–‡ä»¶',
                'type': 'unknown',
                'size': 0
            }
        
        task.file_path = data.get('file_path')
        task.file_content = None  # ä¸ä»æ•°æ®åº“æ¢å¤æ–‡ä»¶å†…å®¹
        task.status = data['status']
        task.progress = data['progress']
        task.steps = data.get('steps', [])
        task.result = data.get('parsing_result') or data.get('result')
        task.content_analysis = data.get('content_analysis')
        task.ai_analysis = data.get('ai_analysis')
        task.markdown_content = data.get('markdown_content')
        task.error = data.get('error')
        task.created_at = datetime.fromisoformat(data['created_at']) if isinstance(data['created_at'], str) else data['created_at']
        task.updated_at = datetime.fromisoformat(data['updated_at']) if isinstance(data['updated_at'], str) else data['updated_at']
        return task

# ä»»åŠ¡ç®¡ç†å‡½æ•°
def get_task(task_id: str) -> Optional[FileParsingTask]:
    """ä»æ•°æ®åº“è·å–ä»»åŠ¡"""
    task_data = task_storage.get_task(task_id)
    if task_data:
        return FileParsingTask.from_dict(task_data)
    return None

def get_all_tasks() -> List[FileParsingTask]:
    """è·å–æ‰€æœ‰ä»»åŠ¡"""
    tasks_data = task_storage.get_all_tasks()
    return [FileParsingTask.from_dict(data) for data in tasks_data]

def delete_task(task_id: str) -> bool:
    """åˆ é™¤ä»»åŠ¡"""
    return task_storage.delete_task(task_id)

# æ–‡æ¡£è§£æå‡½æ•°
def parse_word_document(file_content: bytes, file_name: str) -> dict:
    """è§£æWordæ–‡æ¡£"""
    try:
        if analyzer:
            # ä½¿ç”¨å¢å¼ºåˆ†æå™¨
            return analyzer.analyze_file(file_content, file_name)
        else:
            # åŸºç¡€è§£æ
            return {
                "text_content": f"Wordæ–‡æ¡£: {file_name}",
                "file_type": "word",
                "file_size": len(file_content),
                "analysis_method": "basic_word_parsing",
                "message": "å¢å¼ºåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€è§£æ"
            }
    except Exception as e:
        logger.error(f"Wordæ–‡æ¡£è§£æå¤±è´¥: {e}")
        return {
            "text_content": f"Wordæ–‡æ¡£è§£æå¤±è´¥: {str(e)}",
            "file_type": "word",
            "file_size": len(file_content),
            "analysis_method": "error_fallback"
        }

def parse_pdf_document(file_content: bytes, file_name: str) -> dict:
    """è§£æPDFæ–‡æ¡£"""
    try:
        if analyzer:
            # ä½¿ç”¨å¢å¼ºåˆ†æå™¨
            return analyzer.analyze_file(file_content, file_name)
        else:
            # åŸºç¡€è§£æ
            return {
                "text_content": f"PDFæ–‡æ¡£: {file_name}",
                "file_type": "pdf",
                "file_size": len(file_content),
                "analysis_method": "basic_pdf_parsing",
                "message": "å¢å¼ºåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€è§£æ"
            }
    except Exception as e:
        logger.error(f"PDFæ–‡æ¡£è§£æå¤±è´¥: {e}")
        return {
            "text_content": f"PDFæ–‡æ¡£è§£æå¤±è´¥: {str(e)}",
            "file_type": "pdf",
            "file_size": len(file_content),
            "analysis_method": "error_fallback"
        }

def parse_text_document(file_content: bytes, file_name: str) -> dict:
    """è§£ææ–‡æœ¬æ–‡æ¡£"""
    try:
        # å°è¯•ä¸åŒçš„ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
        content = None
        
        for encoding in encodings:
            try:
                content = file_content.decode(encoding)
                break
            except UnicodeDecodeError:
                continue
        
        if content is None:
            content = file_content.decode('utf-8', errors='ignore')
        
        return {
            "text_content": content,
            "file_type": "text",
            "file_size": len(file_content),
            "analysis_method": "text_parsing",
            "char_count": len(content),
            "line_count": content.count('\n') + 1
        }
    except Exception as e:
        logger.error(f"æ–‡æœ¬æ–‡æ¡£è§£æå¤±è´¥: {e}")
        return {
            "text_content": f"æ–‡æœ¬æ–‡æ¡£è§£æå¤±è´¥: {str(e)}",
            "file_type": "text",
            "file_size": len(file_content),
            "analysis_method": "error_fallback"
        }

# æ–‡ä»¶å¤„ç†å‡½æ•°
def process_file_parsing(task: FileParsingTask):
    """å¤„ç†æ–‡ä»¶è§£æä»»åŠ¡ - ä½¿ç”¨åˆ†ææœåŠ¡æ¨¡å—"""
    try:
        log_analysis_step(task.id, "æ–‡ä»¶è§£æ", "å¼€å§‹", f"æ–‡ä»¶: {task.file_info.get('name', 'unknown')}")
        task.update_progress(10, "å¼€å§‹è§£ææ–‡ä»¶", "parsing")
        
        file_info = task.file_info
        file_name = file_info.get("name", "")
        file_type = file_info.get("type", "")
        

        # ç›´æ¥è°ƒç”¨æ–°æ–¹æ³•
        extracted_text = extract_text_from_file(task.file_path)
        logger.info("æå–çš„æ–‡æœ¬å†…å®¹å·²å‡†å¤‡å®Œæ¯•")
        extracted_preview = extracted_text.replace('{', '{{').replace('}', '}}') if extracted_text else "æ— å†…å®¹"
        logger.info(f"è½¬æ¢åå†…å®¹é¢„è§ˆ: {extracted_preview}")

        # å®‰å…¨çš„æ—¥å¿—è®°å½•ï¼Œé¿å…æ ¼å¼åŒ–é”™è¯¯
        logger.info("æå–çš„æ–‡æœ¬å†…å®¹å·²å‡†å¤‡å®Œæ¯•")
        extracted_preview = extracted_text.replace('{', '{{').replace('}', '}}') if extracted_text else "æ— å†…å®¹"
        logger.info(f"è½¬æ¢åå†…å®¹é¢„è§ˆ: {extracted_preview}")

        
        # éªŒè¯è¾“å…¥ - ä½¿ç”¨æå–çš„æ–‡æœ¬å†…å®¹è¿›è¡ŒéªŒè¯
        validation = validate_input(task.id, extracted_text, file_type)
        if not all(validation.values()):
            raise ValueError(f"è¾“å…¥éªŒè¯å¤±è´¥: {validation}")
        
        task.update_progress(40, "ä½¿ç”¨åˆ†ææœåŠ¡è§£ææ–‡æ¡£", "parsing")
        
        # ä½¿ç”¨åˆ†ææœåŠ¡ç®¡ç†å™¨è¿›è¡Œæ–‡æ¡£è§£æ
        if analysis_service_manager:
            parsing_result = analysis_service_manager.parse_document_sync(
                task_id=task.id,
                file_content=extracted_text,
                file_type=file_type.split('/')[-1] if '/' in file_type else file_type,
                file_name=file_name
            )
            
            # æ›´æ–°ä»»åŠ¡ç»“æœ
            task.result = parsing_result
            # ä¿å­˜è§£æç»“æœåˆ°Redis
            redis_task_storage.save_parsing_result(task.id, parsing_result)
            task.update_progress(100, "æ–‡æ¡£è§£æå®Œæˆ", "parsed")
            log_analysis_step(task.id, "æ–‡ä»¶è§£æ", "å®Œæˆ", f"è§£æç»“æœå·²ä¿å­˜")
            analysis_logger.info(f"âœ… æ–‡ä»¶è§£æå®Œæˆ: {task.id}")
        else:
            # é™çº§åˆ°åŸæœ‰çš„è§£æé€»è¾‘
            task.update_progress(60, "ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è§£ææ–‡æ¡£", "parsing")
            if file_name.lower().endswith(('.doc', '.docx')) or 'word' in file_type.lower():
                result = parse_word_document(file_content, file_name)
            elif file_name.lower().endswith('.pdf') or file_type == 'application/pdf':
                result = parse_pdf_document(file_content, file_name)
            elif file_name.lower().endswith(('.txt', '.md')) or 'text' in file_type.lower():
                result = parse_text_document(file_content, file_name)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
            
            task.result = result
            # ä¿å­˜è§£æç»“æœåˆ°Redis
            redis_task_storage.save_parsing_result(task.id, result)
            task.update_progress(100, "æ–‡æ¡£è§£æå®Œæˆ", "parsed")
            logger.info(f"æ–‡ä»¶è§£æå®Œæˆ: {task.id}")
        
    except Exception as e:
        error_msg = f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}"
        logger.error(f"ä»»åŠ¡ {task.id} è§£æå¤±è´¥: {e}")
        log_analysis_step(task.id, "æ–‡ä»¶è§£æ", "å¤±è´¥", str(e))
        analysis_logger.error(f"âŒ ä»»åŠ¡ {task.id} è§£æå¤±è´¥: {e}")
        task.error = error_msg
        task.status = "failed"
        task.update_progress(task.progress, error_msg, "failed")

def process_content_analysis(task: FileParsingTask, parsing_result: dict):
    """å¤„ç†å†…å®¹åˆ†æä»»åŠ¡ - ä½¿ç”¨åˆ†ææœåŠ¡æ¨¡å—"""
    try:
        # æ£€æŸ¥å‰ç½®æ¡ä»¶ - å…è®¸ content_analyzing çŠ¶æ€ï¼ˆV2æµç¨‹ä¸­å·²ç»é¢„å…ˆè®¾ç½®äº†çŠ¶æ€ï¼‰
        if task.status not in ["parsed", "content_analyzing"]:
            raise ValueError(f"ä»»åŠ¡çŠ¶æ€ä¸æ­£ç¡®ï¼ŒæœŸæœ› 'parsed' æˆ– 'content_analyzing'ï¼Œå®é™… '{task.status}'")
        
        if not task.result:
            raise ValueError("è§£æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æ")
        
        log_analysis_step(task.id, "å†…å®¹åˆ†æ", "å¼€å§‹", f"çŠ¶æ€: {task.status}, è¿›åº¦: {task.progress}%")
        analysis_logger.info(f"ğŸ” å¼€å§‹å†…å®¹åˆ†æä»»åŠ¡: {task.id}")
        analysis_logger.info(f"ğŸ“Š ä»»åŠ¡å½“å‰çŠ¶æ€: {task.status}")
        analysis_logger.info(f"ğŸ“Š ä»»åŠ¡å½“å‰è¿›åº¦: {task.progress}%")
        analysis_logger.info(f"ğŸ“Š è§£æç»“æœå­˜åœ¨: {bool(task.result)}")
        analysis_logger.info(f"ğŸ“Š åˆ†ææœåŠ¡ç®¡ç†å™¨å¯ç”¨: {bool(analysis_service_manager)}")
        
        # è®¾ç½®çŠ¶æ€ä¸ºå†…å®¹åˆ†æä¸­
        task.status = "content_analyzing"
        task.update_progress(10, "å¼€å§‹å†…å®¹åˆ†æ", "content_analyzing")
        analysis_logger.info(f"âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸º: {task.status}")
        
        # æ­£ç¡®æå–æ–‡æœ¬å†…å®¹
        content = extract_text_from_file(task.file_path)
        task.update_progress(30, "è·å–åˆ°ä¸Šä¼ æ–‡ä»¶", "content_analyzing")
        analysis_logger.info(f"ğŸ“„ æå–åˆ°æ–‡æ¡£å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
        
        # ä½¿ç”¨åˆ†ææœåŠ¡ç®¡ç†å™¨è¿›è¡Œå†…å®¹åˆ†æ
        if analysis_service_manager:
            task.update_progress(35, "å†…å®¹åˆ†æç®¡ç†å™¨å¼€å§‹åˆ†æ", "content_analyzing")
            try:
                content_result = analysis_service_manager.analyze_content_sync(
                    task_id=task.id,
                    parsing_result=parsing_result,
                    document_content=content
                )
                
                # æ›´æ–°ä»»åŠ¡çš„å†…å®¹åˆ†æç»“æœ
                task.content_analysis = content_result
                # ä¿å­˜å†…å®¹åˆ†æç»“æœåˆ°Redis
                redis_task_storage.save_content_analysis(task.id, content_result)
                task.update_progress(100, "å†…å®¹åˆ†æå®Œæˆ", "content_analyzed")
                analysis_logger.info(f"âœ… å†…å®¹åˆ†æå®Œæˆ: {task.id}")
                return
                
            except Exception as e:
                analysis_logger.warning(f"âš ï¸ ç«å±±å¼•æ“åˆ†æå¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ–¹æ³•: {str(e)}")
        
        # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œå†…å®¹åˆ†æï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        task.update_progress(30, "ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œå†…å®¹åˆ†æ", "content_analyzing")
        
        # åŸºç¡€å†…å®¹åˆ†æ
        word_count = len(content.split())
        char_count = len(content)
        paragraphs = content.count('\n\n') + 1
        lines = content.count('\n') + 1
        
        task.update_progress(50, "è¯†åˆ«CRUDæ“ä½œå’Œä¸šåŠ¡åŠŸèƒ½", "content_analyzing")
        
        # ç®€åŒ–çš„CRUDåˆ†æ
        crud_operations = []
        content_lower = content.lower()
        
        if any(keyword in content_lower for keyword in ['æ–°å¢', 'åˆ›å»º', 'æ·»åŠ ', 'create', 'add']):
            crud_operations.append({"type": "CREATE", "description": "åˆ›å»ºåŠŸèƒ½"})
        if any(keyword in content_lower for keyword in ['æŸ¥è¯¢', 'æœç´¢', 'è·å–', 'query', 'search']):
            crud_operations.append({"type": "READ", "description": "æŸ¥è¯¢åŠŸèƒ½"})
        if any(keyword in content_lower for keyword in ['ä¿®æ”¹', 'æ›´æ–°', 'update', 'modify']):
            crud_operations.append({"type": "UPDATE", "description": "æ›´æ–°åŠŸèƒ½"})
        if any(keyword in content_lower for keyword in ['åˆ é™¤', 'delete', 'remove']):
            crud_operations.append({"type": "DELETE", "description": "åˆ é™¤åŠŸèƒ½"})
        
        # æ„å»ºåˆ†æç»“æœ
        analysis_result = {
            "success": True,
            "data": {
                "content_type": "document",
                "language": "zh-CN",
                "word_count": word_count,
                "char_count": char_count,
                "summary": content[:300] + "..." if len(content) > 300 else content,
                "crud_analysis": {
                    "operations": crud_operations,
                    "total_operations": len(crud_operations),
                    "summary": {
                        "create_count": len([op for op in crud_operations if op["type"] == "CREATE"]),
                        "read_count": len([op for op in crud_operations if op["type"] == "READ"]),
                        "update_count": len([op for op in crud_operations if op["type"] == "UPDATE"]),
                        "delete_count": len([op for op in crud_operations if op["type"] == "DELETE"])
                    }
                }
            },
            "metadata": {
                "analysis_time": datetime.now().isoformat(),
                "method": "traditional",
                "service": "ContentAnalyzerService"
            }
        }
        
        task.content_analysis = analysis_result
        # ä¿å­˜å†…å®¹åˆ†æç»“æœåˆ°Redis
        redis_task_storage.save_content_analysis(task.id, analysis_result)
        task.update_progress(100, "å†…å®¹åˆ†æå®Œæˆ", "content_analyzed")
        analysis_logger.info(f"âœ… å†…å®¹åˆ†æå®Œæˆ: {task.id}")
        
    except Exception as e:
        error_msg = f"å†…å®¹åˆ†æå¤±è´¥: {str(e)}"
        logger.error(f"ä»»åŠ¡ {task.id} å†…å®¹åˆ†æå¤±è´¥: {e}")
        analysis_logger.error(f"âŒ ä»»åŠ¡ {task.id} å†…å®¹åˆ†æå¤±è´¥: {e}")
        task.error = error_msg
        task.status = "failed"
        task.update_progress(task.progress, error_msg, "failed")

def process_ai_analysis(task: FileParsingTask, analysis_type: str = "comprehensive", content_analysis_result: dict = None, crud_operations: dict = None):
    """å¤„ç†AIåˆ†æä»»åŠ¡ - ä½¿ç”¨åˆ†ææœåŠ¡æ¨¡å—"""
    try:
        # æ£€æŸ¥å‰ç½®æ¡ä»¶ - å…è®¸ ai_analyzing çŠ¶æ€ï¼ˆV2æµç¨‹ä¸­å·²ç»é¢„å…ˆè®¾ç½®äº†çŠ¶æ€ï¼‰
        if task.status not in ["content_analyzed", "ai_analyzing"]:
            raise ValueError(f"ä»»åŠ¡çŠ¶æ€ä¸æ­£ç¡®ï¼ŒæœŸæœ› 'content_analyzed' æˆ– 'ai_analyzing'ï¼Œå®é™… '{task.status}'")
        
        if not task.content_analysis:
            raise ValueError("å†…å®¹åˆ†æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒAIåˆ†æ")
        
        analysis_logger.info(f"ğŸ¤– å¼€å§‹AIåˆ†æä»»åŠ¡: {task.id}")
        
        # è®¾ç½®çŠ¶æ€ä¸ºAIåˆ†æä¸­
        task.status = "ai_analyzing"
        task.update_progress(10, "å¼€å§‹AIæ™ºèƒ½åˆ†æ", "ai_analyzing")
        
        # ä½¿ç”¨åˆ†ææœåŠ¡ç®¡ç†å™¨è¿›è¡ŒAIåˆ†æ
        if analysis_service_manager:
            task.update_progress(30, "ä½¿ç”¨åˆ†ææœåŠ¡è¿›è¡ŒAIåˆ†æ", "ai_analyzing")
            
            # ä»å†…å®¹åˆ†æç»“æœä¸­æå–CRUDæ“ä½œ
            crud_ops = task.content_analysis.get('crud_analysis', {})
            
            ai_result = analysis_service_manager.ai_analyze_sync(
                task_id=task.id,
                content_analysis=task.content_analysis,
                parsing_result=task.result
            )
            
            # æ›´æ–°ä»»åŠ¡çš„AIåˆ†æç»“æœ
            task.ai_analysis = ai_result
            # ä¿å­˜AIåˆ†æç»“æœåˆ°Redis
            redis_task_storage.save_ai_analysis(task.id, ai_result)
            task.update_progress(100, "AIåˆ†æå®Œæˆ", "ai_analyzed")
            analysis_logger.info(f"âœ… AIåˆ†æå®Œæˆ: {task.id}")
            
        else:
            # é™çº§åˆ°åŸæœ‰çš„AIåˆ†æé€»è¾‘
            task.update_progress(30, "ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡ŒAIåˆ†æ", "ai_analyzing")
            
            # è·å–å†…å®¹åˆ†æç»“æœ
            content_analysis = content_analysis_result or task.content_analysis
            crud_ops = crud_operations or content_analysis.get('crud_analysis', {})
            
            # è·å–åŸå§‹æ–‡æ¡£å†…å®¹
            content = task.result.get("text_content", "") or task.result.get("content", "")
            if not content:
                raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒAIåˆ†æ")
            
            task.update_progress(50, "è°ƒç”¨AIè¿›è¡Œæ¥å£è®¾è®¡åˆ†æ", "ai_analyzing")
            
            # ä½¿ç”¨ç«å±±å¼•æ“å®¢æˆ·ç«¯è¿›è¡ŒAIåˆ†æ
            if volcano_client:
                # æ„å»ºç³»ç»Ÿæç¤º
                system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶æ¶æ„å¸ˆå’ŒAPIè®¾è®¡ä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹å’ŒCRUDæ“ä½œåˆ†æï¼Œè®¾è®¡å…·ä½“çš„å¼€å‘æ¥å£å’Œæ¶ˆæ¯é˜Ÿåˆ—é…ç½®ã€‚"""
                
                # æ„å»ºç”¨æˆ·æç¤º
                crud_summary = ""
                if crud_ops:
                    operations = crud_ops.get('operations', [])
                    crud_summary = f"è¯†åˆ«çš„CRUDæ“ä½œï¼š{len(operations)}ä¸ª"
                
                user_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯è®¾è®¡å¼€å‘æ¥å£ï¼š

æ–‡æ¡£æ‘˜è¦ï¼š{content_analysis.get('summary', 'æ— æ‘˜è¦')[:300]}
{crud_summary}

è¯·è®¾è®¡ï¼š
1. RESTful APIæ¥å£
2. æ¶ˆæ¯é˜Ÿåˆ—é…ç½®
3. æŠ€æœ¯å®ç°å»ºè®®
"""
                
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
                
                task.update_progress(70, "AIæ¨¡å‹åˆ†æä¸­", "ai_analyzing")
                ai_response = volcano_client.chat(messages=messages)
                
                # æ„å»ºAIåˆ†æç»“æœ
                ai_analysis_result = {
                    "analysis_type": analysis_type,
                    "ai_insights": {
                        "api_interfaces": [
                            {
                                "name": "æ•°æ®æŸ¥è¯¢æ¥å£",
                                "method": "GET",
                                "path": "/api/data/query",
                                "description": "åŸºäºæ–‡æ¡£åˆ†æçš„æ•°æ®æŸ¥è¯¢æ¥å£"
                            }
                        ],
                        "mq_configuration": {
                            "topics": [{"name": "data-processing", "description": "æ•°æ®å¤„ç†é˜Ÿåˆ—"}]
                        }
                    },
                    "ai_response": ai_response,
                    "confidence_score": 0.8,
                    "analysis_timestamp": datetime.now().isoformat(),
                    "analysis_duration": 2.5,
                    "method": "traditional"
                }
                
                task.ai_analysis = ai_analysis_result
                # ä¿å­˜AIåˆ†æç»“æœåˆ°Redis
                redis_task_storage.save_ai_analysis(task.id, ai_analysis_result)
                task.update_progress(100, "AIåˆ†æå®Œæˆ", "ai_analyzed")
                logger.info(f"AIåˆ†æå®Œæˆ: {task.id}")
            else:
                raise ValueError("AIå®¢æˆ·ç«¯ä¸å¯ç”¨")
        
    except Exception as e:
        error_msg = f"AIåˆ†æå¤±è´¥: {str(e)}"
        logger.error(f"ä»»åŠ¡ {task.id} AIåˆ†æå¤±è´¥: {e}")
        analysis_logger.error(f"âŒ ä»»åŠ¡ {task.id} AIåˆ†æå¤±è´¥: {e}")
        task.error = error_msg
        task.status = "failed"
        task.update_progress(task.progress, error_msg, "failed")

@app.route('/', methods=['GET'])
def index():
    """é¦–é¡µ"""
    return jsonify({
        "service": "analyDesign API Server",
        "version": "2.0.0",
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "endpoints": {
            "chat": "/api/chat",
            "health": "/api/health",
            "sessions": "/api/sessions",
            "file_upload": "/api/file/upload",
            "file_list": "/api/file/list",
            "file_delete": "/api/file/delete/<task_id>",
            "parsing_status": "/api/file/parsing/<task_id>",
            "content_analysis": "/api/file/analyze/<task_id>",
            "ai_analysis": "/api/file/ai-analyze/<task_id>",
            "analysis_result": "/api/file/result/<task_id>"
        },
        "features": [
            "HTTP RESTful API",
            "ç«å±±å¼•æ“ AI é›†æˆ",
            "æ–‡ä»¶ä¸Šä¼ å’Œè§£æ",
            "å†…å®¹åˆ†æ",
            "æ™ºèƒ½AIåˆ†æ",
            "ä¼šè¯ç®¡ç†",
            "CORS è·¨åŸŸæ”¯æŒ"
        ]
    })

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    volcano_status = "connected" if volcano_client else "disconnected"
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "api_server": "running",
            "volcano_engine": volcano_status
        }
    })

@app.route('/api/chat', methods=['POST'])
def chat():
    """èŠå¤©æ¥å£"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                "success": False,
                "error": "è¯·æ±‚æ•°æ®ä¸ºç©º"
            }), 400
        
        message = data.get('message', '').strip()
        session_id = data.get('session_id', str(uuid.uuid4()))
        
        if not message:
            return jsonify({
                "success": False,
                "error": "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º"
            }), 400
        
        logger.info(f"æ”¶åˆ°èŠå¤©è¯·æ±‚ - Session: {session_id}, Message: {message}")
        
        # æ›´æ–°ä¼šè¯ä¿¡æ¯
        if session_id not in sessions:
            sessions[session_id] = {
                "created_at": datetime.now().isoformat(),
                "messages": []
            }
        
        sessions[session_id]["messages"].append({
            "role": "user",
            "content": message,
            "timestamp": datetime.now().isoformat()
        })
        
        # æ£€æŸ¥ç«å±±å¼•æ“å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not volcano_client:
            return jsonify({
                "success": False,
                "error": "ç«å±±å¼•æ“å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®",
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }), 500
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯analyDesignæ™ºèƒ½éœ€æ±‚åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è¿›è¡Œéœ€æ±‚åˆ†æã€è®¿è°ˆæçº²ç”Ÿæˆå’Œé—®å·è®¾è®¡ã€‚è¯·ç”¨ä¸“ä¸šã€å‹å¥½çš„è¯­æ°”å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            },
            {
                "role": "user",
                "content": message
            }
        ]
        
        # è°ƒç”¨ç«å±±å¼•æ“API
        try:
            ai_response = volcano_client.chat(messages=messages)
            
            # ä¿å­˜AIå›å¤åˆ°ä¼šè¯
            sessions[session_id]["messages"].append({
                "role": "assistant",
                "content": ai_response,
                "timestamp": datetime.now().isoformat()
            })
            
            response_data = {
                "success": True,
                "response": ai_response,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "confidence": 0.9,  # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                "intent": "éœ€æ±‚åˆ†æ",  # å¯ä»¥é€šè¿‡NLPåˆ†æå¾—å‡º
                "model": volcano_config.model_id
            }
            
            logger.info(f"æˆåŠŸå¤„ç†èŠå¤©è¯·æ±‚ - Session: {session_id}")
            return jsonify(response_data)
            
        except Exception as volcano_error:
            logger.error(f"ç«å±±å¼•æ“è°ƒç”¨å¤±è´¥: {volcano_error}")
            return jsonify({
                "success": False,
                "error": f"ç«å±±å¼•æ“è°ƒç”¨å¤±è´¥: {str(volcano_error)}",
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }), 500
            
    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‡ºé”™: {e}")
        return jsonify({
            "success": False,
            "error": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/api/sessions', methods=['GET'])
def get_sessions():
    """è·å–æ‰€æœ‰ä¼šè¯"""
    return jsonify({
        "success": True,
        "sessions": sessions,
        "count": len(sessions),
        "timestamp": datetime.now().isoformat()
    })

@app.route('/api/sessions/<session_id>', methods=['GET'])
def get_session(session_id):
    """è·å–ç‰¹å®šä¼šè¯"""
    if session_id in sessions:
        return jsonify({
            "success": True,
            "session": sessions[session_id],
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        })
    else:
        return jsonify({
            "success": False,
            "error": "ä¼šè¯ä¸å­˜åœ¨",
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        }), 404

@app.route('/api/sessions/<session_id>', methods=['DELETE'])
def delete_session(session_id):
    """åˆ é™¤ä¼šè¯"""
    if session_id in sessions:
        del sessions[session_id]
        return jsonify({
            "success": True,
            "message": "ä¼šè¯å·²åˆ é™¤",
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        })
    else:
        return jsonify({
            "success": False,
            "error": "ä¼šè¯ä¸å­˜åœ¨",
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        }), 404

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        "success": False,
        "error": "æ¥å£ä¸å­˜åœ¨",
        "timestamp": datetime.now().isoformat()
    }), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        "success": False,
        "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
        "timestamp": datetime.now().isoformat()
    }), 500

# ==================== æ–‡ä»¶å¤„ç†æ¥å£ ====================

@app.route('/api/file/upload', methods=['POST'])
def upload_file():
    """æ–‡ä»¶ä¸Šä¼ æ¥å£ - æ”¯æŒJSONå’Œmultipart/form-dataä¸¤ç§æ ¼å¼ï¼Œé›†æˆåˆ†ææœåŠ¡éªŒè¯"""
    try:
        # æ£€æŸ¥è¯·æ±‚ç±»å‹
        if request.content_type and 'application/json' in request.content_type:
            # JSONæ ¼å¼ä¸Šä¼ ï¼ˆå‰ç«¯ä½¿ç”¨ï¼‰
            data = request.get_json()
            if not data or 'file_info' not in data:
                return jsonify({
                    "success": False,
                    "error": "è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘file_info"
                }), 400
            
            file_info = data['file_info']
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not all(key in file_info for key in ['name', 'content']):
                return jsonify({
                    "success": False,
                    "error": "æ–‡ä»¶ä¿¡æ¯ä¸å®Œæ•´ï¼Œç¼ºå°‘nameæˆ–contentå­—æ®µ"
                }), 400
            
            # è§£ç base64æ–‡ä»¶å†…å®¹
            try:
                import base64
                file_content = base64.b64decode(file_info['content'])
            except Exception as e:
                return jsonify({
                    "success": False,
                    "error": f"æ–‡ä»¶å†…å®¹è§£ç å¤±è´¥: {str(e)}"
                }), 400
            
            # æ›´æ–°æ–‡ä»¶ä¿¡æ¯
            file_info['size'] = len(file_content)
            filename = file_info['name']
            
        else:
            # multipart/form-dataæ ¼å¼ä¸Šä¼ ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
            if 'file' not in request.files:
                return jsonify({
                    "success": False,
                    "error": "æ²¡æœ‰æ–‡ä»¶è¢«ä¸Šä¼ "
                }), 400
            
            file = request.files['file']
            if file.filename == '':
                return jsonify({
                    "success": False,
                    "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"
                }), 400
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            file_content = file.read()
            filename = file.filename
            
            # æ„å»ºæ–‡ä»¶ä¿¡æ¯
            file_info = {
                "name": filename,
                "type": file.content_type or "application/octet-stream",
                "size": len(file_content)
            }
        
        # ä½¿ç”¨åˆ†ææœåŠ¡è¿›è¡Œæ–‡ä»¶éªŒè¯
        file_validation = validate_file_upload(filename, len(file_content))
        if not all(file_validation.values()):
            validation_errors = [k for k, v in file_validation.items() if not v]
            return jsonify({
                "success": False,
                "error": f"æ–‡ä»¶éªŒè¯å¤±è´¥: {', '.join(validation_errors)}",
                "validation_details": file_validation
            }), 400
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # ä¿å­˜æ–‡ä»¶åˆ°uploads/tempç›®å½•ï¼ˆä½¿ç”¨åˆ†ææœåŠ¡çš„ç›®å½•ç»“æ„ï¼‰
        uploads_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "uploads", "temp")
        os.makedirs(uploads_dir, exist_ok=True)
        file_path = os.path.join(uploads_dir, f"{task_id}_{filename}")
        
        with open(file_path, 'wb') as f:
            f.write(file_content)
        
        # åˆ›å»ºè§£æä»»åŠ¡
        task = FileParsingTask(
            task_id=task_id,
            file_info=file_info,
            file_content=file_content,
            file_path=file_path
        )
        
        # å¼‚æ­¥å¼€å§‹æ–‡ä»¶è§£æ
        executor.submit(process_file_parsing, task)
        
        logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}, ä»»åŠ¡ID: {task_id}, å¤§å°: {len(file_content)} bytes")
        analysis_logger.info(f"ğŸ“ æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}, ä»»åŠ¡ID: {task_id}")
        
        return jsonify({
            "success": True,
            "task_id": task_id,
            "file_info": file_info,
            "file_path": file_path,
            "validation_passed": True,
            "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå¼€å§‹è§£æ",
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        analysis_logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/parsing/<task_id>', methods=['GET'])
def get_parsing_status(task_id):
    """è·å–æ–‡ä»¶è§£æçŠ¶æ€"""
    try:
        task = get_task(task_id)
        if task:
            task_dict = task.to_dict()
            
            # ç¡®ä¿ file_info å­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
            if 'file_info' not in task_dict or task_dict['file_info'] is None:
                task_dict['file_info'] = {
                    'name': 'æœªçŸ¥æ–‡ä»¶',
                    'type': 'unknown',
                    'size': 0
                }
            
            return jsonify({
                "success": True,
                **task_dict
            })
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"è·å–è§£æçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"è·å–è§£æçŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/analyze/<task_id>', methods=['POST'])
def analyze_content(task_id):
    """å†…å®¹åˆ†ææ¥å£ - æ¥æ”¶è§£æç»“æœï¼Œè¿”å›å¢åˆ æ”¹æŸ¥åŠŸèƒ½åˆ†æ"""
    try:
        task = get_task(task_id)
        if task:
            # æ£€æŸ¥ï¼šåªæœ‰å½“æ–‡æ¡£è§£æå®Œæˆæ—¶æ‰èƒ½å¼€å§‹å†…å®¹åˆ†æ
            # å…è®¸åœ¨ parsed æˆ– content_analyzed çŠ¶æ€ä¸‹è§¦å‘å†…å®¹åˆ†æï¼ˆæ”¯æŒé‡æ–°åˆ†æï¼‰
            if task.status not in ["parsed", "content_analyzed"]:
                return jsonify({
                    "success": False,
                    "error": f"æ–‡æ¡£è§£æå°šæœªå®Œæˆï¼ˆå½“å‰çŠ¶æ€: {task.status}ï¼‰ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æã€‚è¯·ç­‰å¾…æ–‡æ¡£è§£æå®Œæˆã€‚"
                }), 400
            
            # å¦‚æœçŠ¶æ€æ˜¯ parsedï¼Œæ£€æŸ¥è¿›åº¦æ˜¯å¦ä¸º100%
            if task.status == "parsed" and task.progress < 100:
                return jsonify({
                    "success": False,
                    "error": f"æ–‡æ¡£è§£æè¿›åº¦æœªè¾¾åˆ°100%ï¼ˆå½“å‰: {task.progress}%ï¼‰ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æã€‚"
                }), 400
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è§£æç»“æœ
            if not task.result:
                return jsonify({
                    "success": False,
                    "error": "æ–‡æ¡£è§£æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æã€‚"
                }), 400
            
            # è·å–è¯·æ±‚å‚æ•° - åŒ…å«æ–‡æ¡£è§£æçš„ç»“æœæ•°æ®
            data = request.get_json() or {}
            parsing_result = data.get("parsing_result") or task.result
            
            logger.info(f"âœ… å†…å®¹åˆ†æå‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹å†…å®¹åˆ†æ: {task_id}")
            logger.info(f"ğŸ“„ æ¥æ”¶åˆ°è§£æç»“æœæ•°æ®: å­—ç¬¦æ•°={parsing_result.get('char_count', 0)}, æ–‡ä»¶ç±»å‹={parsing_result.get('file_type', 'unknown')}")
            
            # å¼‚æ­¥å¼€å§‹å†…å®¹åˆ†æï¼Œä¼ å…¥è§£æç»“æœ
            executor.submit(process_content_analysis, task, parsing_result)
            
            return jsonify({
                "success": True,
                "message": "å†…å®¹åˆ†æå·²å¼€å§‹",
                "task_id": task_id,
                "input_data": {
                    "parsing_result_received": True,
                    "content_length": len(parsing_result.get("text_content", "")),
                    "file_type": parsing_result.get("file_type", "unknown")
                },
                "expected_output": {
                    "crud_operations": "å¢åˆ æ”¹æŸ¥åŠŸèƒ½åˆ†æ",
                    "business_requirements": "ä¸šåŠ¡éœ€æ±‚åˆ†æ",
                    "functional_changes": "åŠŸèƒ½å˜æ›´åˆ†æ"
                },
                "previous_status": "parsed",
                "current_status": "content_analyzing",
                "timestamp": datetime.now().isoformat()
            })
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"å†…å®¹åˆ†æå¯åŠ¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"å†…å®¹åˆ†æå¯åŠ¨å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/ai-analyze/<task_id>', methods=['POST'])
def ai_analyze(task_id):
    """æ™ºèƒ½è§£ææ¥å£ - æ¥æ”¶å¢åˆ æ”¹æŸ¥åŠŸèƒ½ï¼Œè¿”å›æ¥å£è®¾è®¡å’ŒMQé…ç½®"""
    try:
        task = get_task(task_id)
        if task:
            # ä¸¥æ ¼æ£€æŸ¥ï¼šåªæœ‰å½“å†…å®¹åˆ†æå®Œå…¨å®Œæˆæ—¶æ‰èƒ½å¼€å§‹AIåˆ†æ
            if task.status != "content_analyzed":
                return jsonify({
                    "success": False,
                    "error": f"å†…å®¹åˆ†æå°šæœªå®Œæˆï¼ˆå½“å‰çŠ¶æ€: {task.status}ï¼‰ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½è§£æã€‚è¯·ç­‰å¾…å†…å®¹åˆ†æå®Œæˆã€‚"
                }), 400
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹åˆ†æç»“æœ
            if not task.content_analysis:
                return jsonify({
                    "success": False,
                    "error": "å†…å®¹åˆ†æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½è§£æã€‚"
                }), 400
            
            # è·å–è¯·æ±‚å‚æ•° - åŒ…å«å¢åˆ æ”¹æŸ¥åŠŸèƒ½åˆ†æç»“æœ
            data = request.get_json() or {}
            content_analysis_result = data.get("content_analysis") or task.content_analysis
            crud_operations = data.get("crud_operations", {})
            analysis_type = data.get("analysis_type", "comprehensive")
            
            logger.info(f"âœ… AIåˆ†æå‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹AIåˆ†æ: {task_id}")
            logger.info(f"ğŸ” æ¥æ”¶åˆ°å†…å®¹åˆ†æç»“æœ: CRUDæ“ä½œ={len(crud_operations.get('operations', []))}")
            
            # å¼‚æ­¥å¼€å§‹AIåˆ†æï¼Œä¼ å…¥å†…å®¹åˆ†æç»“æœå’ŒCRUDæ“ä½œ
            executor.submit(process_ai_analysis, task, analysis_type, content_analysis_result, crud_operations)
            
            return jsonify({
                "success": True,
                "message": "AIåˆ†æå·²å¼€å§‹",
                "task_id": task_id,
                "analysis_type": analysis_type,
                "input_data": {
                    "content_analysis_received": True,
                    "crud_operations_count": len(crud_operations.get("operations", [])),
                    "business_requirements_count": len(crud_operations.get("requirements", [])),
                    "functional_changes_count": len(crud_operations.get("changes", []))
                },
                "expected_output": {
                    "api_interfaces": "å…·ä½“å¼€å‘æ¥å£è®¾è®¡",
                    "interface_parameters": "æ¥å£å…¥å‚è¿”å‚å­—æ®µ",
                    "mq_configuration": "MQ topic/client/serveré…ç½®",
                    "technical_specifications": "æŠ€æœ¯è§„æ ¼è¯´æ˜"
                },
                "previous_status": "content_analyzed",
                "current_status": "ai_analyzing",
                "timestamp": datetime.now().isoformat()
            })
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"AIåˆ†æå¯åŠ¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"AIåˆ†æå¯åŠ¨å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/result/<task_id>', methods=['GET'])
def get_analysis_result(task_id):
    """è·å–å®Œæ•´åˆ†æç»“æœ - ä»Redisè·å–æ•°æ®"""
    try:
        # ä¼˜å…ˆä»Redisè·å–åˆ†æç»“æœ
        parsing_result = redis_task_storage.get_parsing_result(task_id) or {}
        content_analysis = redis_task_storage.get_content_analysis(task_id) or {}
        ai_analysis = redis_task_storage.get_ai_analysis(task_id) or {}
        
        # ä»SQLiteè·å–ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
        task = get_task(task_id)
        if task:
            
            # è·å–ç”Ÿæˆçš„markdownå†…å®¹ - ä»Redisè·å–ï¼Œå› ä¸ºå†…å­˜ä¸­çš„ä»»åŠ¡å¯¹è±¡å¯èƒ½ä¸æ˜¯æœ€æ–°çš„
            markdown_content = redis_task_storage.get_markdown_content(task_id) or ""
            
            # æ„å»ºæœ€ç»ˆçš„æ•´åˆç»“æœ
            result = {
                "success": True,
                "task_id": task_id,
        
                  # åŸºæœ¬ä¿¡æ¯
                "basic_info": {
                    "filename": task.file_info.get("filename", "Unknown"),
                    "filesize": f"{task.file_info.get('size', 0) / 1024:.1f} KB",
                    "file_type": task.file_info.get("type", "Unknown")
                },
                #æ–‡æ¡£è§£æè¿”å›ç»“æœ
                "document_parsing": parsing_result,
                #å†…å®¹åˆ†æè¿”å›ç»“æœ
                "content_analysis": content_analysis,
                #AIåˆ†æè¿”å›ç»“æœ
                "ai_analysis": ai_analysis,
                # åç«¯ç”Ÿæˆçš„markdownå†…å®¹ - ä¿®å¤ï¼šä»Redisè·å–è€Œéä»taskå¯¹è±¡
                "markdown_content": markdown_content,
                "summary": {
                    "complexity_level": content_analysis.get("complexity_level", "ä¸­ç­‰"),
                    "crud_operations_count": len(content_analysis.get("crud_analysis", {}).get("operations", [])),
                    "api_interfaces_count": len(ai_analysis.get("api_interfaces", [])),
                    "mq_topics_count": len(ai_analysis.get("mq_configuration", {}).get("topics", [])),
                    "estimated_development_time": content_analysis.get("business_insights", {}).get("estimated_development_time", "æœªçŸ¥")
                },
                "error": task.error or "",
                "file_info": task.file_info or {},
                "timestamps": {
                    "created_at": task.created_at.isoformat() if task.created_at else "",
                    "updated_at": task.updated_at.isoformat() if task.updated_at else "",
                    "parsing_completed": task.created_at.isoformat() if task.created_at else "",
                    "content_analysis_completed":content_analysis.get("analysis_metadata", {}).get("analyzed_at", ""),
                    "ai_analysis_completed": ai_analysis.get("analyzed_at", "")
                }
            }
            
            return jsonify(result)
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"è·å–åˆ†æç»“æœå¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"è·å–åˆ†æç»“æœå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/list', methods=['GET'])
def list_files():
    """è·å–å·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨"""
    try:
        tasks = get_all_tasks()
        file_list = []
        for task in tasks:
            file_list.append({
                "task_id": task.id,
                "file_info": task.file_info,
                "status": task.status,
                "progress": task.progress,
                "created_at": task.created_at.isoformat(),
                "updated_at": task.updated_at.isoformat()
            })
        
        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        file_list.sort(key=lambda x: x["created_at"], reverse=True)
        
        return jsonify({
            "success": True,
            "files": file_list,
            "count": len(file_list),
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/delete/<task_id>', methods=['DELETE'])
def delete_file(task_id):
    """åˆ é™¤å·²ä¸Šä¼ æ–‡ä»¶"""
    try:
        # å…ˆè·å–ä»»åŠ¡ä¿¡æ¯
        task = get_task(task_id)
        if not task:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
        
        # åˆ é™¤ç‰©ç†æ–‡ä»¶
        if task.file_path and os.path.exists(task.file_path):
            try:
                os.remove(task.file_path)
                logger.info(f"åˆ é™¤æ–‡ä»¶: {task.file_path}")
            except Exception as e:
                logger.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
        
        # ä»æ•°æ®åº“ä¸­åˆ é™¤ä»»åŠ¡è®°å½•
        if delete_task(task_id):
            return jsonify({
                "success": True,
                "message": f"æ–‡ä»¶ {task.file_info.get('name', 'æœªçŸ¥æ–‡ä»¶')} å·²åˆ é™¤",
                "task_id": task_id,
                "timestamp": datetime.now().isoformat()
            })
        else:
            return jsonify({
                "success": False,
                "error": "åˆ é™¤ä»»åŠ¡è®°å½•å¤±è´¥"
            }), 500
            
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/v2/analysis/start', methods=['POST'])
def start_analysis_v2():
    """V2ç‰ˆæœ¬ï¼šå¯åŠ¨å®Œæ•´æµç¨‹ï¼ˆè‡ªåŠ¨æ‰§è¡Œ4é˜¶æ®µï¼‰"""
    try:
        # æ£€æŸ¥è¯·æ±‚ç±»å‹
        if request.content_type and 'application/json' in request.content_type:
            # JSONæ ¼å¼ä¸Šä¼ ï¼ˆå‰ç«¯ä½¿ç”¨ï¼‰
            data = request.get_json()
            if not data or 'file_info' not in data:
                return jsonify({
                    "success": False,
                    "error": "è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘file_info"
                }), 400
            
            file_info = data['file_info']
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not all(key in file_info for key in ['name', 'content']):
                return jsonify({
                    "success": False,
                    "error": "æ–‡ä»¶ä¿¡æ¯ä¸å®Œæ•´ï¼Œç¼ºå°‘nameæˆ–contentå­—æ®µ"
                }), 400
            
            # å®‰å…¨è®°å½•æ–‡ä»¶ä¿¡æ¯ï¼Œé¿å…base64å†…å®¹è¿‡é•¿
            logger.info(f"file_infoåŒ…å«contentå­—æ®µï¼Œé•¿åº¦: {len(file_info['content'])}")
            # è§£ç base64æ–‡ä»¶å†…å®¹
            try:
                import base64
                file_content = base64.b64decode(file_info['content'])

                logger.info(f"base64è§£ç æˆåŠŸï¼Œfile_contentç±»å‹: {type(file_content)}, å¤§å°: {len(file_content)} bytes")
            except Exception as e:
                return jsonify({
                    "success": False,
                    "error": f"æ–‡ä»¶å†…å®¹è§£ç å¤±è´¥: {str(e)}"
                }), 400
            
            # æ›´æ–°æ–‡ä»¶ä¿¡æ¯
            file_info['size'] = len(file_content)
            filename = file_info['name']
            
        else:
            # multipart/form-dataæ ¼å¼ä¸Šä¼ ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
            if 'file' not in request.files:
                return jsonify({
                    "success": False,
                    "error": "æ²¡æœ‰æ–‡ä»¶è¢«ä¸Šä¼ "
                }), 400
            
            file = request.files['file']
            if file.filename == '':
                return jsonify({
                    "success": False,
                    "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"
                }), 400
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            file_content = file.read()
            filename = file.filename
            
            # æ„å»ºæ–‡ä»¶ä¿¡æ¯
            file_info = {
                "name": filename,
                "type": file.content_type or "application/octet-stream",
                "size": len(file_content)
            }
        
        # ä½¿ç”¨åˆ†ææœåŠ¡è¿›è¡Œæ–‡ä»¶éªŒè¯
        file_validation = validate_file_upload(filename, len(file_content))
        if not all(file_validation.values()):
            validation_errors = [k for k, v in file_validation.items() if not v]
            return jsonify({
                "success": False,
                "error": f"æ–‡ä»¶éªŒè¯å¤±è´¥: {', '.join(validation_errors)}",
                "validation_details": file_validation
            }), 400
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # ä¿å­˜æ–‡ä»¶åˆ°uploads/tempç›®å½•
        uploads_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "uploads", "temp")
        os.makedirs(uploads_dir, exist_ok=True)
        file_path = os.path.join(uploads_dir, f"{task_id}_{filename}")
        
        with open(file_path, 'wb') as f:
            f.write(file_content)
        
       
        # åˆ›å»ºè§£æä»»åŠ¡
        task = FileParsingTask(
            task_id=task_id,
            file_info=file_info,
            file_path=file_path
        )
        
        # è®¾ç½®åˆå§‹è¿›åº¦çŠ¶æ€ä¸º"å¯åŠ¨ä¸­"
        task.update_progress(0, "åˆ†ææµç¨‹å¯åŠ¨ä¸­", "starting")
        
        # å¯åŠ¨å®Œæ•´çš„4é˜¶æ®µåˆ†ææµç¨‹
        executor.submit(run_full_analysis_pipeline, task)
        
        logger.info(f"V2 å®Œæ•´åˆ†æå¯åŠ¨æˆåŠŸ: {filename}, ä»»åŠ¡ID: {task_id}, å¤§å°: {len(file_content)} bytes")
        analysis_logger.info(f"ğŸš€ V2 å®Œæ•´åˆ†æå¯åŠ¨: {filename}, ä»»åŠ¡ID: {task_id}")
        
        return jsonify({
            "success": True,
            "task_id": task_id,
            "file_info": file_info,
            "message": "å®Œæ•´åˆ†ææµç¨‹å·²å¯åŠ¨",
            "stages": [
                {"name": "document_parsing", "title": "æ–‡æ¡£è§£æ", "status": "pending"},
                {"name": "content_analysis", "title": "å†…å®¹åˆ†æ", "status": "pending"},
                {"name": "ai_analysis", "title": "AIæ™ºèƒ½åˆ†æ", "status": "pending"},
                {"name": "document_generation", "title": "ç”Ÿæˆæ–‡æ¡£", "status": "pending"}
            ],
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"V2 åˆ†æå¯åŠ¨å¤±è´¥: {e}")
        analysis_logger.error(f"âŒ V2 åˆ†æå¯åŠ¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"åˆ†æå¯åŠ¨å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/v2/analysis/progress/<task_id>', methods=['GET'])
def get_analysis_progress_v2(task_id):
    """V2ç‰ˆæœ¬ï¼šè·å–å®æ—¶è¿›åº¦æ›´æ–°"""
    try:
        task = get_task(task_id)
        if not task:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
        
        # è®¡ç®—å„é˜¶æ®µè¿›åº¦
        stages = {
            "document_parsing": {
                "title": "æ–‡æ¡£è§£æ",
                "status": "pending",
                "progress": 0,
                "message": "ç­‰å¾…å¼€å§‹"
            },
            "content_analysis": {
                "title": "å†…å®¹åˆ†æ", 
                "status": "pending",
                "progress": 0,
                "message": "ç­‰å¾…å¼€å§‹"
            },
            "ai_analysis": {
                "title": "AIæ™ºèƒ½åˆ†æ",
                "status": "pending", 
                "progress": 0,
                "message": "ç­‰å¾…å¼€å§‹"
            },
            "document_generation": {
                "title": "ç”Ÿæˆæ–‡æ¡£",
                "status": "pending",
                "progress": 0,
                "message": "ç­‰å¾…å¼€å§‹"
            }
        }
        
        # è·å–æœ€æ–°çš„æ­¥éª¤æè¿°
        def get_latest_description(task):
            if task.steps and len(task.steps) > 0 and isinstance(task.steps[-1], dict):
                return task.steps[-1].get("description", "å¤„ç†ä¸­...")
            return "å¤„ç†ä¸­..."
        
        # æ ¹æ®ä»»åŠ¡çŠ¶æ€æ›´æ–°é˜¶æ®µä¿¡æ¯
        if task.status in ["parsing", "parsed"]:
            stages["document_parsing"]["status"] = "running" if task.status == "parsing" else "completed"
            stages["document_parsing"]["progress"] = task.progress if task.status == "parsing" else 100
            stages["document_parsing"]["message"] = get_latest_description(task) if task.status == "parsing" else "æ–‡æ¡£è§£æå®Œæˆ"
            
        if task.status in ["content_analyzing", "content_analyzed"]:
            stages["document_parsing"]["status"] = "completed"
            stages["document_parsing"]["progress"] = 100
            stages["document_parsing"]["message"] = "æ–‡æ¡£è§£æå®Œæˆ"
            
            stages["content_analysis"]["status"] = "running" if task.status == "content_analyzing" else "completed"
            stages["content_analysis"]["progress"] = task.progress if task.status == "content_analyzing" else 100
            stages["content_analysis"]["message"] = get_latest_description(task) if task.status == "content_analyzing" else "å†…å®¹åˆ†æå®Œæˆ"
            
        if task.status in ["ai_analyzing", "ai_analyzed", "document_generating", "document_generated", "fully_completed"]:
            stages["document_parsing"]["status"] = "completed"
            stages["document_parsing"]["progress"] = 100
            stages["document_parsing"]["message"] = "æ–‡æ¡£è§£æå®Œæˆ"
            
            stages["content_analysis"]["status"] = "completed"
            stages["content_analysis"]["progress"] = 100
            stages["content_analysis"]["message"] = "å†…å®¹åˆ†æå®Œæˆ"
            
            stages["ai_analysis"]["status"] = "running" if task.status == "ai_analyzing" else "completed"
            stages["ai_analysis"]["progress"] = task.progress if task.status == "ai_analyzing" else 100
            stages["ai_analysis"]["message"] = get_latest_description(task) if task.status == "ai_analyzing" else "AIåˆ†æå®Œæˆ"
            
        if task.status in ["document_generating", "document_generated", "fully_completed"]:
            stages["document_generation"]["status"] = "running" if task.status == "document_generating" else "completed"
            stages["document_generation"]["progress"] = task.progress if task.status == "document_generating" else 100
            stages["document_generation"]["message"] = get_latest_description(task) if task.status == "document_generating" else "æ–‡æ¡£ç”Ÿæˆå®Œæˆ"
        
        # è®¡ç®—æ•´ä½“è¿›åº¦
        total_progress = 0
        completed_stages = 0
        
        for stage in stages.values():
            if stage["status"] == "completed":
                completed_stages += 1
                total_progress += 100
            elif stage["status"] == "running":
                total_progress += stage["progress"]
        
        overall_progress = total_progress // 4
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        overall_status = "pending"
        if task.status == "fully_completed":
            overall_status = "completed"
        elif task.status in ["failed", "ai_failed", "content_failed"]:
            overall_status = "failed"
        elif completed_stages > 0 or any(stage["status"] == "running" for stage in stages.values()):
            overall_status = "running"
        
        return jsonify({
            "success": True,
            "task_id": task_id,
            "overall_status": overall_status,
            "overall_progress": overall_progress,
            "current_stage": task.status,
            "stages": stages,
            "file_info": task.file_info,
            "error": task.error,
            "started_at": task.created_at.isoformat() if task.created_at else None,
            "updated_at": task.updated_at.isoformat() if task.updated_at else None,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"è·å–V2è¿›åº¦å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"è·å–è¿›åº¦å¤±è´¥: {str(e)}"
        }), 500

def process_document_generation(task: FileParsingTask):
    """ç”ŸæˆMarkdownæ–‡æ¡£"""
    try:
        logger.info(f"å¼€å§‹ç”Ÿæˆæ–‡æ¡£: {task.id}")
        analysis_logger.info(f"ğŸ“„ å¼€å§‹æ–‡æ¡£ç”Ÿæˆä»»åŠ¡: {task.id}")
        
        # æ›´æ–°è¿›åº¦
        task.update_progress(10, "å‡†å¤‡ç”Ÿæˆæ–‡æ¡£", "document_generating")
        
        # è·å–æ‰€æœ‰åˆ†æç»“æœ
        task.update_progress(30, "è·å–åˆ†æç»“æœæ•°æ®", "document_generating")
        
        # ä»Redisè·å–åˆ†æç»“æœæ•°æ®
        parsing_result = redis_task_storage.get_parsing_result(task.id) or {}
        content_analysis = redis_task_storage.get_content_analysis(task.id) or {}
        ai_analysis = redis_task_storage.get_ai_analysis(task.id) or {}
        
        # æ„å»ºå®Œæ•´çš„ç»“æœæ•°æ®
        result_data = {
            "basic_info": {
                "filename": task.file_info.get("filename", "Unknown"),
                "filesize": f"{task.file_info.get('size', 0) / 1024:.1f} KB",
                "file_type": task.file_info.get("type", "Unknown"),
                "uploaded_at": task.created_at.isoformat() if task.created_at else None
            },
            "document_parsing": parsing_result,
            "content_analysis": content_analysis,
            "ai_analysis": ai_analysis
        }
        
        # è°ƒè¯•ä¿¡æ¯
        logger.info(f"ç”Ÿæˆæ–‡æ¡£æ•°æ®ç»“æ„: basic_info={bool(result_data.get('basic_info'))}, "
                   f"document_parsing={bool(result_data.get('document_parsing'))}, "
                   f"content_analysis={bool(result_data.get('content_analysis'))}, "
                   f"ai_analysis={bool(result_data.get('ai_analysis'))}")
        
        # ç”ŸæˆMarkdownå†…å®¹ï¼Œå°†ai_analysisè½¬æ¢ä¸ºMarkdownæ ¼å¼    
        task.update_progress(60, "è½¬æ¢ä¸ºMarkdownæ ¼å¼", "document_generating")
        markdown_content = generate_markdown_report(ai_analysis)
        
        # ä¿å­˜Markdownå†…å®¹åˆ°ä»»åŠ¡ä¸­å’ŒRedis
        task.update_progress(90, "ä¿å­˜æ–‡æ¡£å†…å®¹", "document_generating")
        task.markdown_content = markdown_content
        
        # ä¿å­˜åˆ°Redisç”¨äºæ¥å£è¿”å›
        redis_task_storage.save_markdown_content(task.id, markdown_content)
        
        # æ³¨é‡Šï¼šä¸éœ€è¦ä¿å­˜åˆ°SQLiteæ•°æ®åº“ï¼Œå› ä¸ºæ²¡æœ‰åœ°æ–¹ä¼šè¯»å–
        # task_storage.save_markdown_content(task.id, markdown_content)
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task.update_progress(100, "æ–‡æ¡£ç”Ÿæˆå®Œæˆ", "document_generated")
        logger.info(f"æ–‡æ¡£ç”Ÿæˆå®Œæˆ: {task.id}")
        analysis_logger.info(f"âœ… æ–‡æ¡£ç”Ÿæˆå®Œæˆ: {task.id}")
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£ç”Ÿæˆå¤±è´¥ {task.id}: {e}")
        task.update_progress(0, f"æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {str(e)}", "document_failed")
        analysis_logger.error(f"âŒ æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {task.id} - {e}")
        raise

def generate_markdown_report(result_data):
    """å°†JSONç»“æœè½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    logger.info(f"å¼€å§‹ç”ŸæˆMarkdownæŠ¥å‘Šï¼Œæ•°æ®é”®: {list(result_data.keys()) if result_data else 'None'}")
    
    if not result_data:
        logger.warning("ç»“æœæ•°æ®ä¸ºç©ºï¼Œç”ŸæˆåŸºç¡€æŠ¥å‘Š")
        return "# ğŸ“‹ å¼€å‘è®¾è®¡æ–¹æ¡ˆ\n\n**é”™è¯¯**: æ²¡æœ‰å¯ç”¨çš„åˆ†ææ•°æ®\n"
    
    markdown = "# ğŸ“‹ å¼€å‘è®¾è®¡æ–¹æ¡ˆ\n\n"
    
  
    # AIæ™ºèƒ½åˆ†æç»“æœ
    if result_data.get("ai_analysis"):
        ai_analysis = result_data["ai_analysis"]
        if isinstance(ai_analysis, str):
            markdown += f"{ai_analysis}\n\n"
        elif isinstance(ai_analysis, dict):
            for key, value in ai_analysis.items():
                markdown += f"### {key}\n\n{value}\n\n"
        markdown += "---\n\n"
    
    # æŠ€æœ¯æ‰§è¡Œè¦æ±‚
    markdown += "## ğŸ“æ‰§è¡Œè¦æ±‚\n\n"
    markdown += "1. ä¸¥æ ¼éµå¾ªè®¾è®¡æ–‡æ¡£ä¸­çš„æ¶æ„ã€å‘½åè§„èŒƒã€ä»£ç ç»“æ„\n"
    markdown += "2. åªå¯¹æŒ‡å®šéƒ¨åˆ†è¿›è¡Œä¿®æ”¹\n"
    markdown += "3. ä¿æŒå…¶ä»–éƒ¨åˆ†å®Œå…¨ä¸å˜\n"
    markdown += "4. å¦‚æœ‰ç–‘é—®ï¼Œå…ˆè¯¢é—®å†æ‰§è¡Œ\n"
    
    # æ—¶é—´æˆ³
    from datetime import datetime
    markdown += f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n"
    
    logger.info(f"MarkdownæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(markdown)} å­—ç¬¦")
    return markdown

def run_full_analysis_pipeline(task: FileParsingTask):
    """è¿è¡Œå®Œæ•´çš„é˜¶æ®µåˆ†ææµç¨‹"""
    try:
        logger.info(f"å¼€å§‹å®Œæ•´åˆ†ææµç¨‹: {task.id}")
        
        # é˜¶æ®µ1: æ–‡æ¡£è§£æ
        task.update_progress(10, "å¼€å§‹æ–‡æ¡£è§£æ", "parsing")
        process_file_parsing(task)
        
        # æ£€æŸ¥æ–‡æ¡£è§£ææ˜¯å¦æˆåŠŸ
        if task.status != "parsed":
            logger.error(f"æ–‡æ¡£è§£æå¤±è´¥ï¼Œåœæ­¢åç»­æµç¨‹: {task.id}")
            return
        
        # é˜¶æ®µ2: å†…å®¹åˆ†æ
        task.update_progress(40, "å¼€å§‹å†…å®¹åˆ†æ", "content_analyzing")
        process_content_analysis(task, task.result)
        
        # æ£€æŸ¥å†…å®¹åˆ†ææ˜¯å¦æˆåŠŸ
        if task.status != "content_analyzed":
            logger.error(f"å†…å®¹åˆ†æå¤±è´¥ï¼Œåœæ­¢åç»­æµç¨‹: {task.id}")
            return
        
        # é˜¶æ®µ3: AIæ™ºèƒ½åˆ†æ
        task.update_progress(70, "å¼€å§‹AIæ™ºèƒ½åˆ†æ", "ai_analyzing")
        process_ai_analysis(task, "comprehensive", task.content_analysis, {})
        
        # æ£€æŸ¥AIåˆ†ææ˜¯å¦æˆåŠŸ
        if task.status != "ai_analyzed":
            logger.error(f"AIåˆ†æå¤±è´¥: {task.id}")
            return
        
        # é˜¶æ®µ4: ç”Ÿæˆæ–‡æ¡£
        task.update_progress(90, "å¼€å§‹ç”Ÿæˆæ–‡æ¡£", "document_generating")
        process_document_generation(task)
        
        # æ£€æŸ¥æ–‡æ¡£ç”Ÿæˆæ˜¯å¦æˆåŠŸ
        if task.status != "document_generated":
            logger.error(f"æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {task.id}")
            return
        
        # å®Œæˆæ‰€æœ‰åˆ†æ
        task.update_progress(100, "å®Œæ•´åˆ†ææµç¨‹å®Œæˆ", "fully_completed")
        logger.info(f"å®Œæ•´åˆ†ææµç¨‹æˆåŠŸå®Œæˆ: {task.id}")
        analysis_logger.info(f"ğŸ‰ å®Œæ•´åˆ†ææµç¨‹å®Œæˆ: {task.id}")
        
    except Exception as e:
        logger.error(f"å®Œæ•´åˆ†ææµç¨‹å¤±è´¥: {task.id}, é”™è¯¯: {e}")
        task.error = f"åˆ†ææµç¨‹å¤±è´¥: {str(e)}"
        task.status = "failed"
        task.update_progress(task.progress, f"åˆ†æå¤±è´¥: {str(e)}", "failed")

def create_app():
    """åˆ›å»ºFlaskåº”ç”¨"""
    # åˆå§‹åŒ–ä»»åŠ¡å­˜å‚¨
    try:
        logger.info("åˆå§‹åŒ–ä»»åŠ¡å­˜å‚¨æ•°æ®åº“...")
        # TaskStorageåœ¨åˆå§‹åŒ–æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨_init_database
        logger.info("ä»»åŠ¡å­˜å‚¨æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"ä»»åŠ¡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    return app

def extract_text_from_file(file_path: str) -> str:
    """
    ä»æ–‡ä»¶è·¯å¾„è¯»å–æ–‡ä»¶å†…å®¹å¹¶è½¬æ¢ä¸ºMarkdownæˆ–æ–‡æœ¬å†…å®¹
    :param file_path: æ–‡ä»¶è·¯å¾„
    :return: extracted_text
    """
    file_content = None
    if file_path and os.path.exists(file_path):
        try:
            with open(file_path, 'rb') as f:
                file_content = f.read()
            logger.info(f"ä»æ–‡ä»¶è·¯å¾„è¯»å–: {file_path}")
        except Exception as e:
            logger.warning(f"ä»æ–‡ä»¶è·¯å¾„è¯»å–å¤±è´¥: {e}")
    else:
        raise ValueError("æ— æ³•è·å–æ–‡ä»¶å†…å®¹ï¼Œæ–‡ä»¶å¯èƒ½å·²è¢«åˆ é™¤")

    # å¤„ç†æ–‡ä»¶å†…å®¹
    try:
        # EnhancedAnalyzer å¿…é¡»åœ¨ä½œç”¨åŸŸå†…
        current_analyzer = analyzer if 'analyzer' in globals() and analyzer is not None else EnhancedAnalyzer()
        file_name = os.path.basename(file_path)
        file_type = file_name.split('.')[-1] if '.' in file_name else ''

        if file_name.lower().endswith(('.doc', '.docx')) or 'word' in file_type.lower():
            logger.info(f"æ£€æµ‹åˆ°Wordæ–‡æ¡£ï¼Œä½¿ç”¨Markdownè½¬æ¢: {file_name}")
            transform_result = current_analyzer.parse_word_document(file_content, file_name)
            extracted_text = transform_result.get("text_content", "Wordæ–‡æ¡£è§£æå¤±è´¥")
            if extracted_text and any(marker in extracted_text for marker in ['#', '|', '**', '*', '-']):
                logger.info(f"Wordæ–‡æ¡£å·²æˆåŠŸè½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œé•¿åº¦: {len(extracted_text)} å­—ç¬¦")
            else:
                logger.warning("Wordæ–‡æ¡£è½¬æ¢ç»“æœå¯èƒ½ä¸æ˜¯æ ‡å‡†Markdownæ ¼å¼")
        else:
            logger.info(f"ä½¿ç”¨é€šç”¨æ–‡ä»¶åˆ†ææ–¹æ³•: {file_name}")
            transform_result = current_analyzer.transform_file(file_content, file_name)
            extracted_text = transform_result.get("text_content", "æ–‡ä»¶è§£æå¤±è´¥")
    except Exception as e:
        logger.error(f"EnhancedAnalyzerä½¿ç”¨å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸºç¡€è§£æ")
        if file_name.lower().endswith(('.doc', '.docx')) or 'word' in file_type.lower():
            extracted_text = f"Wordæ–‡æ¡£è§£æå¤±è´¥: {str(e)}ï¼Œå»ºè®®æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ–å®‰è£…python-docxåº“"
        else:
            extracted_text = f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}"
    return extracted_text

if __name__ == '__main__':
    app = create_app()
    app.run(host='0.0.0.0', port=8082, debug=True) 