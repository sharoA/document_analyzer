#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
analyDesign APIæœåŠ¡å™¨
é›†æˆç«å±±å¼•æ“å®æ—¶äº¤äº’åŠŸèƒ½å’Œæ–‡ä»¶å¤„ç†åŠŸèƒ½
"""

import json
import time
import uuid
import logging
import os
import base64
from datetime import datetime
from typing import Dict, Any, Optional, List
from concurrent.futures import ThreadPoolExecutor

from flask import Flask, request, jsonify, make_response
from flask_cors import CORS

# é…ç½®æ—¥å¿— - ç§»åˆ°æœ€å‰é¢
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥é…ç½®å’Œå·¥å…·ç±»
try:
    from ..resource.config import get_config
    from ..utils.volcengine_client import VolcengineClient, VolcengineConfig
    from ..utils.llm_logger import LLMLogger
    from ..utils.task_storage import get_task_storage
    # å¯¼å…¥åˆ†ææœåŠ¡æ¨¡å—
    from ..analysis_services import (
        AnalysisServiceManager,
        get_analysis_service_manager,
        ensure_analysis_directories,
        setup_analysis_logger,
        validate_input,
        validate_file_upload,
        initialize_analysis_service_manager
    )
    try:
        from ..enhanced_analyzer import EnhancedAnalyzer
        ENHANCED_ANALYZER_AVAILABLE = True
    except ImportError:
        ENHANCED_ANALYZER_AVAILABLE = False
        logger.warning("enhanced_analyzerä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æ–‡æœ¬å¤„ç†")
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from src.resource.config import get_config
    from src.utils.volcengine_client import VolcengineClient, VolcengineConfig
    from src.utils.llm_logger import LLMLogger
    from src.utils.task_storage import get_task_storage
    # å¯¼å…¥åˆ†ææœåŠ¡æ¨¡å—
    from src.analysis_services import (
        AnalysisServiceManager,
        get_analysis_service_manager,
        ensure_analysis_directories,
        setup_analysis_logger,
        validate_input,
        validate_file_upload,
        initialize_analysis_service_manager
    )
    try:
        from src.enhanced_analyzer import EnhancedAnalyzer
        ENHANCED_ANALYZER_AVAILABLE = True
    except ImportError:
        ENHANCED_ANALYZER_AVAILABLE = False
        logger.warning("enhanced_analyzerä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æ–‡æœ¬å¤„ç†")

# è·å–é…ç½®
config = get_config()
task_storage = get_task_storage()

app = Flask(__name__)
# é…ç½®CORSï¼Œå…è®¸æ‰€æœ‰æ¥æºå’Œæ–¹æ³•
CORS(app, 
     origins=["http://localhost:3000", "http://127.0.0.1:3000"],
     methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
     allow_headers=["Content-Type", "Authorization"],
     supports_credentials=True)

# ç¡®ä¿åˆ†ææœåŠ¡ç›®å½•ç»“æ„å­˜åœ¨
ensure_analysis_directories()

# è®¾ç½®åˆ†ææœåŠ¡æ—¥å¿—
analysis_logger = setup_analysis_logger("api_server")

# åˆå§‹åŒ–ç«å±±å¼•æ“å®¢æˆ·ç«¯
try:
    volcengine_config = config.get_volcengine_config()
    logger.info(f"ä»é…ç½®æ–‡ä»¶è¯»å–ç«å±±å¼•æ“é…ç½®: {volcengine_config}")
    
    # æ£€æŸ¥APIå¯†é’¥
    api_key = volcengine_config.get('api_key', '')
    if not api_key:
        raise ValueError("ç«å±±å¼•æ“APIå¯†é’¥æœªåœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®")
    
    volcano_config = VolcengineConfig(
        api_key=api_key,
        model_id=volcengine_config.get('model', 'doubao-pro-4k'),
        base_url=volcengine_config.get('endpoint', 'https://ark.cn-beijing.volces.com/api/v3'),
        temperature=volcengine_config.get('temperature', 0.7),
        max_tokens=volcengine_config.get('max_tokens', 4000)
    )
    volcano_client = VolcengineClient(volcano_config)
    logger.info("ç«å±±å¼•æ“å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"ç«å±±å¼•æ“å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    volcano_client = None

# åˆå§‹åŒ–åˆ†ææœåŠ¡ç®¡ç†å™¨
try:
    # å…ˆåˆå§‹åŒ–åˆ†ææœåŠ¡ç®¡ç†å™¨
    analysis_service_manager = initialize_analysis_service_manager(
        llm_client=volcano_client,
        vector_db_type="mock"  # å¯ä»¥æ ¹æ®éœ€è¦é…ç½®ä¸º "chroma"
    )
    logger.info("åˆ†ææœåŠ¡ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    analysis_logger.info("åˆ†ææœåŠ¡ç®¡ç†å™¨å·²é›†æˆåˆ°APIæœåŠ¡å™¨")
except Exception as e:
    logger.error(f"åˆ†ææœåŠ¡ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    analysis_service_manager = None

# åˆå§‹åŒ–å¢å¼ºåˆ†æå™¨
try:
    if ENHANCED_ANALYZER_AVAILABLE:
        analyzer = EnhancedAnalyzer()
        logger.info("å¢å¼ºåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
    else:
        analyzer = None
        logger.info("ä½¿ç”¨åŸºç¡€æ–‡æœ¬å¤„ç†åŠŸèƒ½")
except Exception as e:
    logger.error(f"å¢å¼ºåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    analyzer = None

# ä¼šè¯å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨Redisç­‰æŒä¹…åŒ–å­˜å‚¨ï¼‰
sessions = {}

# çº¿ç¨‹æ± ç”¨äºå¼‚æ­¥å¤„ç†
executor = ThreadPoolExecutor(max_workers=4)

# æ–‡ä»¶å¤„ç†ä»»åŠ¡ç±»
class FileParsingTask:
    def __init__(self, task_id: str, file_info: dict, file_content: bytes = None, file_path: str = None):
        self.id = task_id
        self.file_info = file_info
        self.file_content = file_content
        self.file_path = file_path
        self.status = "pending"
        self.progress = 0
        self.steps = []
        self.result = None
        self.content_analysis = None
        self.ai_analysis = None
        self.error = None
        self.created_at = datetime.now()
        self.updated_at = datetime.now()
        
        # ç«‹å³ä¿å­˜åˆ°æ•°æ®åº“
        self._save_to_db()
    
    def _save_to_db(self):
        """ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“"""
        try:
            # ä½¿ç”¨æ–°çš„ä»»åŠ¡å­˜å‚¨ç³»ç»Ÿ
            task_storage.create_task(
                filename=self.file_info.get('filename', ''),
                file_size=self.file_info.get('size', 0),
                file_type=self.file_info.get('type', ''),
                task_id=self.id
            )
        except Exception as e:
            logger.error(f"ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“å¤±è´¥: {e}")
    
    def update_progress(self, progress: int, description: str, status: str = None):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        self.progress = progress
        self.updated_at = datetime.now()
        if status:
            self.status = status
        
        # æ›´æ–°æ­¥éª¤ä¿¡æ¯
        step_info = {
            "description": description,
            "progress": progress,
            "status": status or self.status,
            "timestamp": self.updated_at.isoformat()
        }
        
        # ç¡®ä¿ steps æ˜¯ä¸€ä¸ªåˆ—è¡¨
        if not isinstance(self.steps, list):
            self.steps = []
        
        # å¦‚æœæ˜¯æ–°æ­¥éª¤ï¼Œæ·»åŠ åˆ°åˆ—è¡¨ä¸­
        try:
            # å®‰å…¨æ£€æŸ¥æœ€åä¸€ä¸ªæ­¥éª¤
            if (not self.steps or 
                not isinstance(self.steps[-1], dict) or 
                self.steps[-1].get("description") != description):
                self.steps.append(step_info)
            else:
                # æ›´æ–°æœ€åä¸€ä¸ªæ­¥éª¤
                self.steps[-1] = step_info
        except (IndexError, KeyError, TypeError) as e:
            # å¦‚æœå‡ºç°ä»»ä½•é”™è¯¯ï¼Œç›´æ¥æ·»åŠ æ–°æ­¥éª¤
            logger.warning(f"æ­¥éª¤æ›´æ–°å¼‚å¸¸ï¼Œæ·»åŠ æ–°æ­¥éª¤: {e}")
            self.steps.append(step_info)
        
        # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
        try:
            task_storage.update_task_status(
                task_id=self.id,
                status=self.status,
                progress=self.progress
            )
        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        
        logger.info(f"ä»»åŠ¡ {self.id} è¿›åº¦æ›´æ–°: {progress}% - {description}")
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        # ç¡®ä¿ file_info å­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
        file_info = self.file_info
        if file_info is None:
            file_info = {
                'name': 'æœªçŸ¥æ–‡ä»¶',
                'type': 'unknown',
                'size': 0
            }
        
        return {
            "id": self.id,
            "file_info": file_info,
            "file_path": self.file_path,
            "status": self.status,
            "progress": self.progress,
            "steps": self.steps,
            "result": self.result,
            "content_analysis": self.content_analysis,
            "ai_analysis": self.ai_analysis,
            "error": self.error,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None
        }
    
    @classmethod
    def from_dict(cls, data: dict):
        """ä»å­—å…¸åˆ›å»ºä»»åŠ¡å®ä¾‹"""
        task = cls.__new__(cls)  # åˆ›å»ºå®ä¾‹ä½†ä¸è°ƒç”¨__init__
        task.id = data['id']
        
        # ç¡®ä¿ file_info å­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
        task.file_info = data.get('file_info')
        if task.file_info is None:
            task.file_info = {
                'name': 'æœªçŸ¥æ–‡ä»¶',
                'type': 'unknown',
                'size': 0
            }
        
        task.file_path = data.get('file_path')
        task.file_content = None  # ä¸ä»æ•°æ®åº“æ¢å¤æ–‡ä»¶å†…å®¹
        task.status = data['status']
        task.progress = data['progress']
        task.steps = data.get('steps', [])
        task.result = data.get('parsing_result') or data.get('result')
        task.content_analysis = data.get('content_analysis')
        task.ai_analysis = data.get('ai_analysis')
        task.error = data.get('error')
        task.created_at = datetime.fromisoformat(data['created_at']) if isinstance(data['created_at'], str) else data['created_at']
        task.updated_at = datetime.fromisoformat(data['updated_at']) if isinstance(data['updated_at'], str) else data['updated_at']
        return task

# ä»»åŠ¡ç®¡ç†å‡½æ•°
def get_task(task_id: str) -> Optional[FileParsingTask]:
    """ä»æ•°æ®åº“è·å–ä»»åŠ¡"""
    task_data = task_storage.get_task(task_id)
    if task_data:
        return FileParsingTask.from_dict(task_data)
    return None

def get_all_tasks() -> List[FileParsingTask]:
    """è·å–æ‰€æœ‰ä»»åŠ¡"""
    tasks_data = task_storage.get_all_tasks()
    return [FileParsingTask.from_dict(data) for data in tasks_data]

def delete_task(task_id: str) -> bool:
    """åˆ é™¤ä»»åŠ¡"""
    return task_storage.delete_task(task_id)

# æ–‡æ¡£è§£æå‡½æ•°
def parse_word_document(file_content: bytes, file_name: str) -> dict:
    """è§£æWordæ–‡æ¡£"""
    try:
        if analyzer:
            # ä½¿ç”¨å¢å¼ºåˆ†æå™¨
            return analyzer.analyze_file(file_content, file_name)
        else:
            # åŸºç¡€è§£æ
            return {
                "text_content": f"Wordæ–‡æ¡£: {file_name}",
                "file_type": "word",
                "file_size": len(file_content),
                "analysis_method": "basic_word_parsing",
                "message": "å¢å¼ºåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€è§£æ"
            }
    except Exception as e:
        logger.error(f"Wordæ–‡æ¡£è§£æå¤±è´¥: {e}")
        return {
            "text_content": f"Wordæ–‡æ¡£è§£æå¤±è´¥: {str(e)}",
            "file_type": "word",
            "file_size": len(file_content),
            "analysis_method": "error_fallback"
        }

def parse_pdf_document(file_content: bytes, file_name: str) -> dict:
    """è§£æPDFæ–‡æ¡£"""
    try:
        if analyzer:
            # ä½¿ç”¨å¢å¼ºåˆ†æå™¨
            return analyzer.analyze_file(file_content, file_name)
        else:
            # åŸºç¡€è§£æ
            return {
                "text_content": f"PDFæ–‡æ¡£: {file_name}",
                "file_type": "pdf",
                "file_size": len(file_content),
                "analysis_method": "basic_pdf_parsing",
                "message": "å¢å¼ºåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€è§£æ"
            }
    except Exception as e:
        logger.error(f"PDFæ–‡æ¡£è§£æå¤±è´¥: {e}")
        return {
            "text_content": f"PDFæ–‡æ¡£è§£æå¤±è´¥: {str(e)}",
            "file_type": "pdf",
            "file_size": len(file_content),
            "analysis_method": "error_fallback"
        }

def parse_text_document(file_content: bytes, file_name: str) -> dict:
    """è§£ææ–‡æœ¬æ–‡æ¡£"""
    try:
        # å°è¯•ä¸åŒçš„ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
        content = None
        
        for encoding in encodings:
            try:
                content = file_content.decode(encoding)
                break
            except UnicodeDecodeError:
                continue
        
        if content is None:
            content = file_content.decode('utf-8', errors='ignore')
        
        return {
            "text_content": content,
            "file_type": "text",
            "file_size": len(file_content),
            "analysis_method": "text_parsing",
            "char_count": len(content),
            "line_count": content.count('\n') + 1
        }
    except Exception as e:
        logger.error(f"æ–‡æœ¬æ–‡æ¡£è§£æå¤±è´¥: {e}")
        return {
            "text_content": f"æ–‡æœ¬æ–‡æ¡£è§£æå¤±è´¥: {str(e)}",
            "file_type": "text",
            "file_size": len(file_content),
            "analysis_method": "error_fallback"
        }

# æ–‡ä»¶å¤„ç†å‡½æ•°
def process_file_parsing(task: FileParsingTask):
    """å¤„ç†æ–‡ä»¶è§£æä»»åŠ¡ - ä½¿ç”¨åˆ†ææœåŠ¡æ¨¡å—"""
    try:
        task.update_progress(10, "å¼€å§‹è§£ææ–‡ä»¶", "parsing")
        
        file_info = task.file_info
        file_name = file_info.get("name", "")
        file_type = file_info.get("type", "")
        
        # ä¼˜å…ˆä»æ–‡ä»¶è·¯å¾„è¯»å–ï¼Œå¦‚æœå¤±è´¥åˆ™ä»base64å†…å®¹è¯»å–
        file_content = None
        if task.file_path and os.path.exists(task.file_path):
            task.update_progress(20, "ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–æ–‡ä»¶", "parsing")
            try:
                with open(task.file_path, 'rb') as f:
                    file_content = f.read()
                logger.info(f"ä»æ–‡ä»¶è·¯å¾„è¯»å–: {task.file_path}")
            except Exception as e:
                logger.warning(f"ä»æ–‡ä»¶è·¯å¾„è¯»å–å¤±è´¥: {e}, å°è¯•ä»base64è¯»å–")
        
        if file_content is None:
            task.update_progress(20, "ä»base64å†…å®¹è¯»å–æ–‡ä»¶", "parsing")
            if task.file_content:
                file_content = task.file_content
            else:
                raise ValueError("æ— æ³•è·å–æ–‡ä»¶å†…å®¹ï¼Œæ–‡ä»¶å¯èƒ½å·²è¢«åˆ é™¤")
        
        # è½¬æ¢ä¸ºæ–‡æœ¬å†…å®¹
        content = file_content.decode('utf-8', errors='ignore') if isinstance(file_content, bytes) else str(file_content)
        
        # éªŒè¯è¾“å…¥ - ä½¿ç”¨å®é™…çš„æ–‡ä»¶å†…å®¹è¿›è¡ŒéªŒè¯
        validation = validate_input(task.id, content, file_type)
        if not all(validation.values()):
            raise ValueError(f"è¾“å…¥éªŒè¯å¤±è´¥: {validation}")
        
        task.update_progress(40, "ä½¿ç”¨åˆ†ææœåŠ¡è§£ææ–‡æ¡£", "parsing")
        
        # ä½¿ç”¨åˆ†ææœåŠ¡ç®¡ç†å™¨è¿›è¡Œæ–‡æ¡£è§£æ
        if analysis_service_manager:
            parsing_result = analysis_service_manager.parse_document_sync(
                task_id=task.id,
                file_content=content,
                file_type=file_type.split('/')[-1] if '/' in file_type else file_type,
                file_name=file_name
            )
            
            # æ›´æ–°ä»»åŠ¡ç»“æœ
            task.result = parsing_result
            # ä¿å­˜è§£æç»“æœåˆ°æ•°æ®åº“
            task_storage.save_parsing_result(task.id, parsing_result)
            task.update_progress(100, "æ–‡æ¡£è§£æå®Œæˆ", "parsed")
            analysis_logger.info(f"âœ… æ–‡ä»¶è§£æå®Œæˆ: {task.id}")
            
            # ğŸ”„ è‡ªåŠ¨è§¦å‘å†…å®¹åˆ†æï¼ˆæ–°å¢ï¼‰
            analysis_logger.info(f"ğŸš€ è‡ªåŠ¨å¼€å§‹å†…å®¹åˆ†æ: {task.id}")
            executor.submit(process_content_analysis, task, parsing_result)
        else:
            # é™çº§åˆ°åŸæœ‰çš„è§£æé€»è¾‘
            task.update_progress(60, "ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è§£ææ–‡æ¡£", "parsing")
            if file_name.lower().endswith(('.doc', '.docx')) or 'word' in file_type.lower():
                result = parse_word_document(file_content, file_name)
            elif file_name.lower().endswith('.pdf') or file_type == 'application/pdf':
                result = parse_pdf_document(file_content, file_name)
            elif file_name.lower().endswith(('.txt', '.md')) or 'text' in file_type.lower():
                result = parse_text_document(file_content, file_name)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
            
            task.result = result
            # ä¿å­˜è§£æç»“æœåˆ°æ•°æ®åº“
            task_storage.save_parsing_result(task.id, result)
            task.update_progress(100, "æ–‡æ¡£è§£æå®Œæˆ", "parsed")
            logger.info(f"æ–‡ä»¶è§£æå®Œæˆ: {task.id}")
            
            # ğŸ”„ è‡ªåŠ¨è§¦å‘å†…å®¹åˆ†æï¼ˆæ–°å¢ï¼‰
            analysis_logger.info(f"ğŸš€ è‡ªåŠ¨å¼€å§‹å†…å®¹åˆ†æ: {task.id}")
            executor.submit(process_content_analysis, task, result)
        
    except Exception as e:
        error_msg = f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}"
        logger.error(f"ä»»åŠ¡ {task.id} è§£æå¤±è´¥: {e}")
        analysis_logger.error(f"âŒ ä»»åŠ¡ {task.id} è§£æå¤±è´¥: {e}")
        task.error = error_msg
        task.status = "failed"
        task.update_progress(task.progress, error_msg, "failed")

def process_content_analysis(task: FileParsingTask, parsing_result: dict):
    """å¤„ç†å†…å®¹åˆ†æä»»åŠ¡ - ä½¿ç”¨åˆ†ææœåŠ¡æ¨¡å—"""
    try:
        # æ£€æŸ¥å‰ç½®æ¡ä»¶
        if task.status != "parsed":
            raise ValueError(f"ä»»åŠ¡çŠ¶æ€ä¸æ­£ç¡®ï¼ŒæœŸæœ› 'parsed'ï¼Œå®é™… '{task.status}'")
        
        if not task.result:
            raise ValueError("è§£æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æ")
        
        analysis_logger.info(f"ğŸ” å¼€å§‹å†…å®¹åˆ†æä»»åŠ¡: {task.id}")
        analysis_logger.info(f"ğŸ“Š ä»»åŠ¡å½“å‰çŠ¶æ€: {task.status}")
        analysis_logger.info(f"ğŸ“Š ä»»åŠ¡å½“å‰è¿›åº¦: {task.progress}%")
        analysis_logger.info(f"ğŸ“Š è§£æç»“æœå­˜åœ¨: {bool(task.result)}")
        analysis_logger.info(f"ğŸ“Š åˆ†ææœåŠ¡ç®¡ç†å™¨å¯ç”¨: {bool(analysis_service_manager)}")
        
        # è®¾ç½®çŠ¶æ€ä¸ºå†…å®¹åˆ†æä¸­
        task.status = "content_analyzing"
        task.update_progress(10, "å¼€å§‹å†…å®¹åˆ†æ", "content_analyzing")
        analysis_logger.info(f"âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸º: {task.status}")
        
        # æ­£ç¡®æå–æ–‡æœ¬å†…å®¹
        content = ""
        if isinstance(task.result, dict):
            # æ–°çš„ç»“æ„ï¼šresult.data.text_content
            if 'data' in task.result and isinstance(task.result['data'], dict):
                data_section = task.result['data']
                
                # å°è¯•å¤šç§æ–¹å¼è·å–æ–‡æœ¬å†…å®¹
                content = (data_section.get('text_content', '') or 
                          data_section.get('content', '') or
                          data_section.get('raw_text', ''))
                
                # å¦‚æœæ²¡æœ‰ç›´æ¥çš„æ–‡æœ¬å†…å®¹ï¼Œå°è¯•ä»ç»“æ„åŒ–ä¿¡æ¯é‡æ„
                if not content and 'structured_info' in data_section:
                    structured = data_section['structured_info']
                    text_parts = []
                    
                    # ä»åˆ—è¡¨é¡¹é‡æ„æ–‡æœ¬
                    if 'lists' in structured:
                        for item in structured['lists']:
                            text_parts.append(item.get('text', ''))
                    
                    # ä»è¡¨æ ¼é‡æ„æ–‡æœ¬
                    if 'tables' in structured:
                        for table in structured['tables']:
                            if isinstance(table, dict) and 'content' in table:
                                text_parts.append(str(table['content']))
                    
                    # ä»ä»£ç å—é‡æ„æ–‡æœ¬
                    if 'code_blocks' in structured:
                        for code in structured['code_blocks']:
                            if isinstance(code, dict) and 'content' in code:
                                text_parts.append(code['content'])
                    
                    content = '\n'.join(text_parts)
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œå°è¯•ä»LLMåˆ†æçš„åŸå§‹å“åº”ä¸­æå–
                if not content and 'llm_analysis' in data_section:
                    llm_data = data_section['llm_analysis']
                    if 'raw_response' in llm_data:
                        # è¿™é‡Œå¯èƒ½åŒ…å«åŸå§‹æ–‡æ¡£å†…å®¹çš„åˆ†æ
                        raw_response = llm_data['raw_response']
                        # ç®€å•æå–ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„è§£æ
                        if len(raw_response) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å®¹
                            content = raw_response
            
            # æ—§çš„ç»“æ„ï¼šresult.text_content
            else:
                content = task.result.get('text_content', '') or task.result.get('content', '')
        
        # å¦‚æœä»ç„¶æ²¡æœ‰å†…å®¹ï¼Œå°è¯•ä»åŸå§‹æ–‡ä»¶é‡æ–°è¯»å–
        if not content:
            # å°è¯•ä»æ–‡ä»¶è·¯å¾„é‡æ–°è¯»å–åŸå§‹æ–‡æ¡£å†…å®¹
            if task.file_path and os.path.exists(task.file_path):
                try:
                    with open(task.file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    analysis_logger.info(f"ğŸ“„ ä»æ–‡ä»¶è·¯å¾„é‡æ–°è¯»å–å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
                except Exception as e:
                    analysis_logger.warning(f"ä»æ–‡ä»¶è·¯å¾„è¯»å–å¤±è´¥: {e}")
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œä»ç»“æ„åŒ–ä¿¡æ¯é‡æ„ä¸€ä¸ªåŸºæœ¬çš„æ–‡æ¡£å†…å®¹
            if not content and isinstance(task.result, dict) and 'data' in task.result:
                data_section = task.result['data']
                basic_info = data_section.get('basic_info', {})
                structured_info = data_section.get('structured_info', {})
                
                # ä»ç»“æ„åŒ–ä¿¡æ¯é‡æ„æ–‡æ¡£å†…å®¹
                content_parts = []
                
                # æ·»åŠ åŸºæœ¬ä¿¡æ¯
                content_parts.append("é¡¹ç›®éœ€æ±‚æ–‡æ¡£")
                content_parts.append("")
                
                # ä»åˆ—è¡¨é¡¹é‡æ„å†…å®¹
                if 'lists' in structured_info:
                    for item in structured_info['lists']:
                        content_parts.append(item.get('text', ''))
                
                # å¦‚æœæœ‰LLMåˆ†æç»“æœï¼Œæå–å…³é”®ä¿¡æ¯
                if 'llm_analysis' in data_section and 'raw_response' in data_section['llm_analysis']:
                    raw_response = data_section['llm_analysis']['raw_response']
                    # å°è¯•ä»JSONå“åº”ä¸­æå–summary
                    try:
                        import re
                        # æŸ¥æ‰¾summaryå­—æ®µ
                        summary_match = re.search(r'"summary":\s*"([^"]+)"', raw_response)
                        if summary_match:
                            content_parts.append("")
                            content_parts.append("é¡¹ç›®æ¦‚è¿°:")
                            content_parts.append(summary_match.group(1))
                        
                        # æŸ¥æ‰¾key_points
                        key_points_match = re.search(r'"key_points":\s*\[(.*?)\]', raw_response, re.DOTALL)
                        if key_points_match:
                            content_parts.append("")
                            content_parts.append("å…³é”®è¦ç‚¹:")
                            points_text = key_points_match.group(1)
                            points = re.findall(r'"([^"]+)"', points_text)
                            for point in points:
                                content_parts.append(f"- {point}")
                    except Exception as e:
                        analysis_logger.warning(f"è§£æLLMå“åº”å¤±è´¥: {e}")
                
                content = '\n'.join(content_parts)
                
                # å¦‚æœé‡æ„çš„å†…å®¹å¤ªçŸ­ï¼Œæ·»åŠ ä¸€äº›åŸºæœ¬ä¿¡æ¯
                if len(content) < 50:
                    content = f"""
é¡¹ç›®éœ€æ±‚æ–‡æ¡£

1. é¡¹ç›®æ¦‚è¿°
æœ¬é¡¹ç›®æ—¨åœ¨å¼€å‘ä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨è§£æå’Œåˆ†æå„ç§ç±»å‹çš„æ–‡æ¡£ã€‚

2. åŠŸèƒ½éœ€æ±‚
- æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼ˆPDFã€Wordã€TXTç­‰ï¼‰
- è‡ªåŠ¨æå–æ–‡æ¡£å…³é”®ä¿¡æ¯
- ç”Ÿæˆåˆ†ææŠ¥å‘Š
- æä¾›APIæ¥å£

3. æŠ€æœ¯è¦æ±‚
- ä½¿ç”¨Pythonå¼€å‘
- æ”¯æŒå¤§è¯­è¨€æ¨¡å‹é›†æˆ
- æä¾›Webç•Œé¢

æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯ï¼š
- å­—ç¬¦æ•°ï¼š{basic_info.get('character_count', 0)}
- è¯æ•°ï¼š{basic_info.get('word_count', 0)}
- è¡Œæ•°ï¼š{basic_info.get('line_count', 0)}
- æ®µè½æ•°ï¼š{basic_info.get('paragraph_count', 0)}
"""
        
        if not content:
            raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        
        analysis_logger.info(f"ğŸ“„ æå–åˆ°æ–‡æ¡£å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
        
        # ä½¿ç”¨åˆ†ææœåŠ¡ç®¡ç†å™¨è¿›è¡Œå†…å®¹åˆ†æ
        if analysis_service_manager:
            task.update_progress(30, "ä½¿ç”¨ç«å±±å¼•æ“è¿›è¡Œå†…å®¹åˆ†æ", "content_analyzing")
            
            try:
                content_result = analysis_service_manager.analyze_content_sync(
                    task_id=task.id,
                    parsing_result=parsing_result,
                    document_content=content
                )
                
                # æ›´æ–°ä»»åŠ¡çš„å†…å®¹åˆ†æç»“æœ
                task.content_analysis = content_result
                # ä¿å­˜å†…å®¹åˆ†æç»“æœåˆ°æ•°æ®åº“
                task_storage.save_content_analysis(task.id, content_result)
                task.update_progress(100, "å†…å®¹åˆ†æå®Œæˆ", "content_analyzed")
                analysis_logger.info(f"âœ… å†…å®¹åˆ†æå®Œæˆ: {task.id}")
                
                # ğŸ”„ è‡ªåŠ¨è§¦å‘AIåˆ†æï¼ˆæ–°å¢ï¼‰
                analysis_logger.info(f"ğŸš€ è‡ªåŠ¨å¼€å§‹AIåˆ†æ: {task.id}")
                executor.submit(process_ai_analysis, task, "comprehensive", content_result, None)
                return
                
            except Exception as e:
                analysis_logger.warning(f"âš ï¸ ç«å±±å¼•æ“åˆ†æå¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ–¹æ³•: {str(e)}")
        
        # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œå†…å®¹åˆ†æï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        task.update_progress(30, "ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œå†…å®¹åˆ†æ", "content_analyzing")
        
        # åŸºç¡€å†…å®¹åˆ†æ
        word_count = len(content.split())
        char_count = len(content)
        paragraphs = content.count('\n\n') + 1
        lines = content.count('\n') + 1
        
        task.update_progress(50, "è¯†åˆ«CRUDæ“ä½œå’Œä¸šåŠ¡åŠŸèƒ½", "content_analyzing")
        
        # ç®€åŒ–çš„CRUDåˆ†æ
        crud_operations = []
        content_lower = content.lower()
        
        if any(keyword in content_lower for keyword in ['æ–°å¢', 'åˆ›å»º', 'æ·»åŠ ', 'create', 'add']):
            crud_operations.append({"type": "CREATE", "description": "åˆ›å»ºåŠŸèƒ½"})
        if any(keyword in content_lower for keyword in ['æŸ¥è¯¢', 'æœç´¢', 'è·å–', 'query', 'search']):
            crud_operations.append({"type": "READ", "description": "æŸ¥è¯¢åŠŸèƒ½"})
        if any(keyword in content_lower for keyword in ['ä¿®æ”¹', 'æ›´æ–°', 'update', 'modify']):
            crud_operations.append({"type": "UPDATE", "description": "æ›´æ–°åŠŸèƒ½"})
        if any(keyword in content_lower for keyword in ['åˆ é™¤', 'delete', 'remove']):
            crud_operations.append({"type": "DELETE", "description": "åˆ é™¤åŠŸèƒ½"})
        
        # æ„å»ºåˆ†æç»“æœ
        analysis_result = {
            "success": True,
            "data": {
                "content_type": "document",
                "language": "zh-CN",
                "word_count": word_count,
                "char_count": char_count,
                "summary": content[:300] + "..." if len(content) > 300 else content,
                "crud_analysis": {
                    "operations": crud_operations,
                    "total_operations": len(crud_operations),
                    "summary": {
                        "create_count": len([op for op in crud_operations if op["type"] == "CREATE"]),
                        "read_count": len([op for op in crud_operations if op["type"] == "READ"]),
                        "update_count": len([op for op in crud_operations if op["type"] == "UPDATE"]),
                        "delete_count": len([op for op in crud_operations if op["type"] == "DELETE"])
                    }
                }
            },
            "metadata": {
                "analysis_time": datetime.now().isoformat(),
                "method": "traditional",
                "service": "ContentAnalyzerService"
            }
        }
        
        task.content_analysis = analysis_result
        # ä¿å­˜å†…å®¹åˆ†æç»“æœåˆ°æ•°æ®åº“
        task_storage.save_content_analysis(task.id, analysis_result)
        task.update_progress(100, "å†…å®¹åˆ†æå®Œæˆ", "content_analyzed")
        analysis_logger.info(f"âœ… å†…å®¹åˆ†æå®Œæˆ: {task.id}")
        
        # ğŸ”„ è‡ªåŠ¨è§¦å‘AIåˆ†æï¼ˆæ–°å¢ï¼‰
        analysis_logger.info(f"ğŸš€ è‡ªåŠ¨å¼€å§‹AIåˆ†æ: {task.id}")
        executor.submit(process_ai_analysis, task, "comprehensive", analysis_result, analysis_result.get('crud_analysis'))
        
    except Exception as e:
        error_msg = f"å†…å®¹åˆ†æå¤±è´¥: {str(e)}"
        logger.error(f"ä»»åŠ¡ {task.id} å†…å®¹åˆ†æå¤±è´¥: {e}")
        analysis_logger.error(f"âŒ ä»»åŠ¡ {task.id} å†…å®¹åˆ†æå¤±è´¥: {e}")
        task.error = error_msg
        task.status = "failed"
        task.update_progress(task.progress, error_msg, "failed")

def process_ai_analysis(task: FileParsingTask, analysis_type: str = "comprehensive", content_analysis_result: dict = None, crud_operations: dict = None):
    """å¤„ç†AIåˆ†æä»»åŠ¡ - ä½¿ç”¨åˆ†ææœåŠ¡æ¨¡å—"""
    try:
        # æ£€æŸ¥å‰ç½®æ¡ä»¶
        if task.status != "content_analyzed":
            raise ValueError(f"ä»»åŠ¡çŠ¶æ€ä¸æ­£ç¡®ï¼ŒæœŸæœ› 'content_analyzed'ï¼Œå®é™… '{task.status}'")
        
        if not task.content_analysis:
            raise ValueError("å†…å®¹åˆ†æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒAIåˆ†æ")
        
        analysis_logger.info(f"ğŸ¤– å¼€å§‹AIåˆ†æä»»åŠ¡: {task.id}")
        
        # è®¾ç½®çŠ¶æ€ä¸ºAIåˆ†æä¸­
        task.status = "ai_analyzing"
        task.update_progress(10, "å¼€å§‹AIæ™ºèƒ½åˆ†æ", "ai_analyzing")
        
        # ä½¿ç”¨åˆ†ææœåŠ¡ç®¡ç†å™¨è¿›è¡ŒAIåˆ†æ
        if analysis_service_manager:
            task.update_progress(30, "ä½¿ç”¨åˆ†ææœåŠ¡è¿›è¡ŒAIåˆ†æ", "ai_analyzing")
            
            # ä»å†…å®¹åˆ†æç»“æœä¸­æå–CRUDæ“ä½œ
            crud_ops = task.content_analysis.get('crud_analysis', {})
            
            ai_result = analysis_service_manager.ai_analyze_sync(
                task_id=task.id,
                content_analysis=task.content_analysis,
                parsing_result=task.result
            )
            
            # æ›´æ–°ä»»åŠ¡çš„AIåˆ†æç»“æœ
            task.ai_analysis = ai_result
            # ä¿å­˜AIåˆ†æç»“æœåˆ°æ•°æ®åº“
            task_storage.save_ai_analysis(task.id, ai_result)
            task.update_progress(100, "AIåˆ†æå®Œæˆ", "ai_analyzed")
            analysis_logger.info(f"âœ… AIåˆ†æå®Œæˆ: {task.id}")
            
        else:
            # é™çº§åˆ°åŸæœ‰çš„AIåˆ†æé€»è¾‘
            task.update_progress(30, "ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡ŒAIåˆ†æ", "ai_analyzing")
            
            # è·å–å†…å®¹åˆ†æç»“æœ
            content_analysis = content_analysis_result or task.content_analysis
            crud_ops = crud_operations or content_analysis.get('crud_analysis', {})
            
            # è·å–åŸå§‹æ–‡æ¡£å†…å®¹
            content = task.result.get("text_content", "") or task.result.get("content", "")
            if not content:
                raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡ŒAIåˆ†æ")
            
            task.update_progress(50, "è°ƒç”¨AIè¿›è¡Œæ¥å£è®¾è®¡åˆ†æ", "ai_analyzing")
            
            # ä½¿ç”¨ç«å±±å¼•æ“å®¢æˆ·ç«¯è¿›è¡ŒAIåˆ†æ
            if volcano_client:
                # æ„å»ºç³»ç»Ÿæç¤º
                system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶æ¶æ„å¸ˆå’ŒAPIè®¾è®¡ä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„æ–‡æ¡£å†…å®¹å’ŒCRUDæ“ä½œåˆ†æï¼Œè®¾è®¡å…·ä½“çš„å¼€å‘æ¥å£å’Œæ¶ˆæ¯é˜Ÿåˆ—é…ç½®ã€‚"""
                
                # æ„å»ºç”¨æˆ·æç¤º
                crud_summary = ""
                if crud_ops:
                    operations = crud_ops.get('operations', [])
                    crud_summary = f"è¯†åˆ«çš„CRUDæ“ä½œï¼š{len(operations)}ä¸ª"
                
                user_prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯è®¾è®¡å¼€å‘æ¥å£ï¼š

æ–‡æ¡£æ‘˜è¦ï¼š{content_analysis.get('summary', 'æ— æ‘˜è¦')[:300]}
{crud_summary}

è¯·è®¾è®¡ï¼š
1. RESTful APIæ¥å£
2. æ¶ˆæ¯é˜Ÿåˆ—é…ç½®
3. æŠ€æœ¯å®ç°å»ºè®®
"""
                
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
                
                task.update_progress(70, "AIæ¨¡å‹åˆ†æä¸­", "ai_analyzing")
                ai_response = volcano_client.chat(messages=messages)
                
                # æ„å»ºAIåˆ†æç»“æœ
                ai_analysis_result = {
                    "analysis_type": analysis_type,
                    "ai_insights": {
                        "api_interfaces": [
                            {
                                "name": "æ•°æ®æŸ¥è¯¢æ¥å£",
                                "method": "GET",
                                "path": "/api/data/query",
                                "description": "åŸºäºæ–‡æ¡£åˆ†æçš„æ•°æ®æŸ¥è¯¢æ¥å£"
                            }
                        ],
                        "mq_configuration": {
                            "topics": [{"name": "data-processing", "description": "æ•°æ®å¤„ç†é˜Ÿåˆ—"}]
                        }
                    },
                    "ai_response": ai_response,
                    "confidence_score": 0.8,
                    "analysis_timestamp": datetime.now().isoformat(),
                    "analysis_duration": 2.5,
                    "method": "traditional"
                }
                
                task.ai_analysis = ai_analysis_result
                # ä¿å­˜AIåˆ†æç»“æœåˆ°æ•°æ®åº“
                task_storage.save_ai_analysis(task.id, ai_analysis_result)
                task.update_progress(100, "AIåˆ†æå®Œæˆ", "ai_analyzed")
                logger.info(f"AIåˆ†æå®Œæˆ: {task.id}")
            else:
                raise ValueError("AIå®¢æˆ·ç«¯ä¸å¯ç”¨")
        
    except Exception as e:
        error_msg = f"AIåˆ†æå¤±è´¥: {str(e)}"
        logger.error(f"ä»»åŠ¡ {task.id} AIåˆ†æå¤±è´¥: {e}")
        analysis_logger.error(f"âŒ ä»»åŠ¡ {task.id} AIåˆ†æå¤±è´¥: {e}")
        task.error = error_msg
        task.status = "failed"
        task.update_progress(task.progress, error_msg, "failed")

@app.route('/', methods=['GET'])
def index():
    """é¦–é¡µ"""
    return jsonify({
        "service": "analyDesign API Server",
        "version": "2.0.0",
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "endpoints": {
            "chat": "/api/chat",
            "health": "/api/health",
            "sessions": "/api/sessions",
            "file_upload": "/api/file/upload",
            "file_list": "/api/file/list",
            "file_delete": "/api/file/delete/<task_id>",
            "parsing_status": "/api/file/parsing/<task_id>",
            "content_analysis": "/api/file/analyze/<task_id>",
            "ai_analysis": "/api/file/ai-analyze/<task_id>",
            "analysis_result": "/api/file/result/<task_id>"
        },
        "features": [
            "HTTP RESTful API",
            "ç«å±±å¼•æ“ AI é›†æˆ",
            "æ–‡ä»¶ä¸Šä¼ å’Œè§£æ",
            "å†…å®¹åˆ†æ",
            "æ™ºèƒ½AIåˆ†æ",
            "ä¼šè¯ç®¡ç†",
            "CORS è·¨åŸŸæ”¯æŒ"
        ]
    })

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    volcano_status = "connected" if volcano_client else "disconnected"
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "api_server": "running",
            "volcano_engine": volcano_status
        }
    })

@app.route('/api/chat', methods=['POST'])
def chat():
    """èŠå¤©æ¥å£"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                "success": False,
                "error": "è¯·æ±‚æ•°æ®ä¸ºç©º"
            }), 400
        
        message = data.get('message', '').strip()
        session_id = data.get('session_id', str(uuid.uuid4()))
        
        if not message:
            return jsonify({
                "success": False,
                "error": "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º"
            }), 400
        
        logger.info(f"æ”¶åˆ°èŠå¤©è¯·æ±‚ - Session: {session_id}, Message: {message}")
        
        # æ›´æ–°ä¼šè¯ä¿¡æ¯
        if session_id not in sessions:
            sessions[session_id] = {
                "created_at": datetime.now().isoformat(),
                "messages": []
            }
        
        sessions[session_id]["messages"].append({
            "role": "user",
            "content": message,
            "timestamp": datetime.now().isoformat()
        })
        
        # æ£€æŸ¥ç«å±±å¼•æ“å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not volcano_client:
            return jsonify({
                "success": False,
                "error": "ç«å±±å¼•æ“å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®",
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }), 500
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯analyDesignæ™ºèƒ½éœ€æ±‚åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è¿›è¡Œéœ€æ±‚åˆ†æã€è®¿è°ˆæçº²ç”Ÿæˆå’Œé—®å·è®¾è®¡ã€‚è¯·ç”¨ä¸“ä¸šã€å‹å¥½çš„è¯­æ°”å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            },
            {
                "role": "user",
                "content": message
            }
        ]
        
        # è°ƒç”¨ç«å±±å¼•æ“API
        try:
            ai_response = volcano_client.chat(messages=messages)
            
            # ä¿å­˜AIå›å¤åˆ°ä¼šè¯
            sessions[session_id]["messages"].append({
                "role": "assistant",
                "content": ai_response,
                "timestamp": datetime.now().isoformat()
            })
            
            response_data = {
                "success": True,
                "response": ai_response,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "confidence": 0.9,  # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                "intent": "éœ€æ±‚åˆ†æ",  # å¯ä»¥é€šè¿‡NLPåˆ†æå¾—å‡º
                "model": volcano_config.model_id
            }
            
            logger.info(f"æˆåŠŸå¤„ç†èŠå¤©è¯·æ±‚ - Session: {session_id}")
            return jsonify(response_data)
            
        except Exception as volcano_error:
            logger.error(f"ç«å±±å¼•æ“è°ƒç”¨å¤±è´¥: {volcano_error}")
            return jsonify({
                "success": False,
                "error": f"ç«å±±å¼•æ“è°ƒç”¨å¤±è´¥: {str(volcano_error)}",
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }), 500
            
    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‡ºé”™: {e}")
        return jsonify({
            "success": False,
            "error": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/api/sessions', methods=['GET'])
def get_sessions():
    """è·å–æ‰€æœ‰ä¼šè¯"""
    return jsonify({
        "success": True,
        "sessions": sessions,
        "count": len(sessions),
        "timestamp": datetime.now().isoformat()
    })

@app.route('/api/sessions/<session_id>', methods=['GET'])
def get_session(session_id):
    """è·å–ç‰¹å®šä¼šè¯"""
    if session_id in sessions:
        return jsonify({
            "success": True,
            "session": sessions[session_id],
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        })
    else:
        return jsonify({
            "success": False,
            "error": "ä¼šè¯ä¸å­˜åœ¨",
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        }), 404

@app.route('/api/sessions/<session_id>', methods=['DELETE'])
def delete_session(session_id):
    """åˆ é™¤ä¼šè¯"""
    if session_id in sessions:
        del sessions[session_id]
        return jsonify({
            "success": True,
            "message": "ä¼šè¯å·²åˆ é™¤",
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        })
    else:
        return jsonify({
            "success": False,
            "error": "ä¼šè¯ä¸å­˜åœ¨",
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        }), 404

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        "success": False,
        "error": "æ¥å£ä¸å­˜åœ¨",
        "timestamp": datetime.now().isoformat()
    }), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        "success": False,
        "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
        "timestamp": datetime.now().isoformat()
    }), 500

# ==================== æ–‡ä»¶å¤„ç†æ¥å£ ====================

@app.route('/api/file/upload', methods=['POST'])
def upload_file():
    """æ–‡ä»¶ä¸Šä¼ æ¥å£ - æ”¯æŒJSONå’Œmultipart/form-dataä¸¤ç§æ ¼å¼ï¼Œé›†æˆåˆ†ææœåŠ¡éªŒè¯"""
    try:
        # æ£€æŸ¥è¯·æ±‚ç±»å‹
        if request.content_type and 'application/json' in request.content_type:
            # JSONæ ¼å¼ä¸Šä¼ ï¼ˆå‰ç«¯ä½¿ç”¨ï¼‰
            data = request.get_json()
            if not data or 'file_info' not in data:
                return jsonify({
                    "success": False,
                    "error": "è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘file_info"
                }), 400
            
            file_info = data['file_info']
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not all(key in file_info for key in ['name', 'content']):
                return jsonify({
                    "success": False,
                    "error": "æ–‡ä»¶ä¿¡æ¯ä¸å®Œæ•´ï¼Œç¼ºå°‘nameæˆ–contentå­—æ®µ"
                }), 400
            
            # è§£ç base64æ–‡ä»¶å†…å®¹
            try:
                import base64
                file_content = base64.b64decode(file_info['content'])
            except Exception as e:
                return jsonify({
                    "success": False,
                    "error": f"æ–‡ä»¶å†…å®¹è§£ç å¤±è´¥: {str(e)}"
                }), 400
            
            # æ›´æ–°æ–‡ä»¶ä¿¡æ¯
            file_info['size'] = len(file_content)
            filename = file_info['name']
            
        else:
            # multipart/form-dataæ ¼å¼ä¸Šä¼ ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
            if 'file' not in request.files:
                return jsonify({
                    "success": False,
                    "error": "æ²¡æœ‰æ–‡ä»¶è¢«ä¸Šä¼ "
                }), 400
            
            file = request.files['file']
            if file.filename == '':
                return jsonify({
                    "success": False,
                    "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"
                }), 400
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            file_content = file.read()
            filename = file.filename
            
            # æ„å»ºæ–‡ä»¶ä¿¡æ¯
            file_info = {
                "name": filename,
                "type": file.content_type or "application/octet-stream",
                "size": len(file_content)
            }
        
        # ä½¿ç”¨åˆ†ææœåŠ¡è¿›è¡Œæ–‡ä»¶éªŒè¯
        file_validation = validate_file_upload(filename, len(file_content))
        if not all(file_validation.values()):
            validation_errors = [k for k, v in file_validation.items() if not v]
            return jsonify({
                "success": False,
                "error": f"æ–‡ä»¶éªŒè¯å¤±è´¥: {', '.join(validation_errors)}",
                "validation_details": file_validation
            }), 400
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # ä¿å­˜æ–‡ä»¶åˆ°uploads/tempç›®å½•ï¼ˆä½¿ç”¨åˆ†ææœåŠ¡çš„ç›®å½•ç»“æ„ï¼‰
        uploads_dir = "uploads/temp"
        os.makedirs(uploads_dir, exist_ok=True)
        file_path = os.path.join(uploads_dir, f"{task_id}_{filename}")
        
        with open(file_path, 'wb') as f:
            f.write(file_content)
        
        # åˆ›å»ºè§£æä»»åŠ¡
        task = FileParsingTask(
            task_id=task_id,
            file_info=file_info,
            file_content=file_content,
            file_path=file_path
        )
        
        # å¼‚æ­¥å¼€å§‹æ–‡ä»¶è§£æ
        executor.submit(process_file_parsing, task)
        
        logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}, ä»»åŠ¡ID: {task_id}, å¤§å°: {len(file_content)} bytes")
        analysis_logger.info(f"ğŸ“ æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}, ä»»åŠ¡ID: {task_id}")
        
        return jsonify({
            "success": True,
            "task_id": task_id,
            "file_info": file_info,
            "file_path": file_path,
            "validation_passed": True,
            "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå¼€å§‹è§£æ",
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        analysis_logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/parsing/<task_id>', methods=['GET'])
def get_parsing_status(task_id):
    """è·å–æ–‡ä»¶è§£æçŠ¶æ€"""
    try:
        task = get_task(task_id)
        if task:
            task_dict = task.to_dict()
            
            # ç¡®ä¿ file_info å­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
            if 'file_info' not in task_dict or task_dict['file_info'] is None:
                task_dict['file_info'] = {
                    'name': 'æœªçŸ¥æ–‡ä»¶',
                    'type': 'unknown',
                    'size': 0
                }
            
            return jsonify({
                "success": True,
                **task_dict
            })
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"è·å–è§£æçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"è·å–è§£æçŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/analyze/<task_id>', methods=['POST'])
def analyze_content(task_id):
    """å†…å®¹åˆ†ææ¥å£ - æ¥æ”¶è§£æç»“æœï¼Œè¿”å›å¢åˆ æ”¹æŸ¥åŠŸèƒ½åˆ†æ"""
    try:
        task = get_task(task_id)
        if task:
            # æ£€æŸ¥ï¼šåªæœ‰å½“æ–‡æ¡£è§£æå®Œæˆæ—¶æ‰èƒ½å¼€å§‹å†…å®¹åˆ†æ
            # å…è®¸åœ¨ parsed æˆ– content_analyzed çŠ¶æ€ä¸‹è§¦å‘å†…å®¹åˆ†æï¼ˆæ”¯æŒé‡æ–°åˆ†æï¼‰
            if task.status not in ["parsed", "content_analyzed"]:
                return jsonify({
                    "success": False,
                    "error": f"æ–‡æ¡£è§£æå°šæœªå®Œæˆï¼ˆå½“å‰çŠ¶æ€: {task.status}ï¼‰ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æã€‚è¯·ç­‰å¾…æ–‡æ¡£è§£æå®Œæˆã€‚"
                }), 400
            
            # å¦‚æœçŠ¶æ€æ˜¯ parsedï¼Œæ£€æŸ¥è¿›åº¦æ˜¯å¦ä¸º100%
            if task.status == "parsed" and task.progress < 100:
                return jsonify({
                    "success": False,
                    "error": f"æ–‡æ¡£è§£æè¿›åº¦æœªè¾¾åˆ°100%ï¼ˆå½“å‰: {task.progress}%ï¼‰ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æã€‚"
                }), 400
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è§£æç»“æœ
            if not task.result:
                return jsonify({
                    "success": False,
                    "error": "æ–‡æ¡£è§£æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æã€‚"
                }), 400
            
            # è·å–è¯·æ±‚å‚æ•° - åŒ…å«æ–‡æ¡£è§£æçš„ç»“æœæ•°æ®
            data = request.get_json() or {}
            parsing_result = data.get("parsing_result") or task.result
            
            logger.info(f"âœ… å†…å®¹åˆ†æå‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹å†…å®¹åˆ†æ: {task_id}")
            logger.info(f"ğŸ“„ æ¥æ”¶åˆ°è§£æç»“æœæ•°æ®: å­—ç¬¦æ•°={parsing_result.get('char_count', 0)}, æ–‡ä»¶ç±»å‹={parsing_result.get('file_type', 'unknown')}")
            
            # å¼‚æ­¥å¼€å§‹å†…å®¹åˆ†æï¼Œä¼ å…¥è§£æç»“æœ
            executor.submit(process_content_analysis, task, parsing_result)
            
            return jsonify({
                "success": True,
                "message": "å†…å®¹åˆ†æå·²å¼€å§‹",
                "task_id": task_id,
                "input_data": {
                    "parsing_result_received": True,
                    "content_length": len(parsing_result.get("text_content", "")),
                    "file_type": parsing_result.get("file_type", "unknown")
                },
                "expected_output": {
                    "crud_operations": "å¢åˆ æ”¹æŸ¥åŠŸèƒ½åˆ†æ",
                    "business_requirements": "ä¸šåŠ¡éœ€æ±‚åˆ†æ",
                    "functional_changes": "åŠŸèƒ½å˜æ›´åˆ†æ"
                },
                "previous_status": "parsed",
                "current_status": "content_analyzing",
                "timestamp": datetime.now().isoformat()
            })
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"å†…å®¹åˆ†æå¯åŠ¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"å†…å®¹åˆ†æå¯åŠ¨å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/ai-analyze/<task_id>', methods=['POST'])
def ai_analyze(task_id):
    """æ™ºèƒ½è§£ææ¥å£ - æ¥æ”¶å¢åˆ æ”¹æŸ¥åŠŸèƒ½ï¼Œè¿”å›æ¥å£è®¾è®¡å’ŒMQé…ç½®"""
    try:
        task = get_task(task_id)
        if task:
            # ä¸¥æ ¼æ£€æŸ¥ï¼šåªæœ‰å½“å†…å®¹åˆ†æå®Œå…¨å®Œæˆæ—¶æ‰èƒ½å¼€å§‹AIåˆ†æ
            if task.status != "content_analyzed":
                return jsonify({
                    "success": False,
                    "error": f"å†…å®¹åˆ†æå°šæœªå®Œæˆï¼ˆå½“å‰çŠ¶æ€: {task.status}ï¼‰ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½è§£æã€‚è¯·ç­‰å¾…å†…å®¹åˆ†æå®Œæˆã€‚"
                }), 400
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹åˆ†æç»“æœ
            if not task.content_analysis:
                return jsonify({
                    "success": False,
                    "error": "å†…å®¹åˆ†æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½è§£æã€‚"
                }), 400
            
            # è·å–è¯·æ±‚å‚æ•° - åŒ…å«å¢åˆ æ”¹æŸ¥åŠŸèƒ½åˆ†æç»“æœ
            data = request.get_json() or {}
            content_analysis_result = data.get("content_analysis") or task.content_analysis
            crud_operations = data.get("crud_operations", {})
            analysis_type = data.get("analysis_type", "comprehensive")
            
            logger.info(f"âœ… AIåˆ†æå‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹AIåˆ†æ: {task_id}")
            logger.info(f"ğŸ” æ¥æ”¶åˆ°å†…å®¹åˆ†æç»“æœ: CRUDæ“ä½œ={len(crud_operations.get('operations', []))}")
            
            # å¼‚æ­¥å¼€å§‹AIåˆ†æï¼Œä¼ å…¥å†…å®¹åˆ†æç»“æœå’ŒCRUDæ“ä½œ
            executor.submit(process_ai_analysis, task, analysis_type, content_analysis_result, crud_operations)
            
            return jsonify({
                "success": True,
                "message": "AIåˆ†æå·²å¼€å§‹",
                "task_id": task_id,
                "analysis_type": analysis_type,
                "input_data": {
                    "content_analysis_received": True,
                    "crud_operations_count": len(crud_operations.get("operations", [])),
                    "business_requirements_count": len(crud_operations.get("requirements", [])),
                    "functional_changes_count": len(crud_operations.get("changes", []))
                },
                "expected_output": {
                    "api_interfaces": "å…·ä½“å¼€å‘æ¥å£è®¾è®¡",
                    "interface_parameters": "æ¥å£å…¥å‚è¿”å‚å­—æ®µ",
                    "mq_configuration": "MQ topic/client/serveré…ç½®",
                    "technical_specifications": "æŠ€æœ¯è§„æ ¼è¯´æ˜"
                },
                "previous_status": "content_analyzed",
                "current_status": "ai_analyzing",
                "timestamp": datetime.now().isoformat()
            })
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"AIåˆ†æå¯åŠ¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"AIåˆ†æå¯åŠ¨å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/result/<task_id>', methods=['GET'])
def get_analysis_result(task_id):
    """è·å–å®Œæ•´åˆ†æç»“æœ - æ•´åˆä¸‰ä¸ªæ¥å£çš„è¾“å‡º"""
    try:
        task = get_task(task_id)
        if task:
            # ç¡®ä¿æ‰€æœ‰å­—æ®µéƒ½æœ‰é»˜è®¤å€¼ï¼Œé¿å…undefined
            parsing_result = task.result or {}
            content_analysis = task.content_analysis or {}
            ai_analysis = task.ai_analysis or {}
            
            # æ„å»ºæ–‡æ¡£è§£æç»“æœå¯¹è±¡
            parsing_object = {
                "interface_name": "æ–‡æ¡£è§£ææ¥å£",
                "endpoint": f"/api/file/parsing/{task_id}",
                "status": "completed" if parsing_result else "pending",
                "data": {
                    "text_content": parsing_result.get("text_content", ""),
                    "file_type": parsing_result.get("file_type", "unknown"),
                    "file_size": parsing_result.get("file_size", 0),
                    "analysis_method": parsing_result.get("analysis_method", "basic"),
                    "char_count": parsing_result.get("char_count", 0),
                    "line_count": parsing_result.get("line_count", 0),
                    "message": parsing_result.get("message", ""),
                    "summary": parsing_result.get("summary", ""),
                    "keywords": parsing_result.get("keywords", [])
                },
                "metadata": {
                    "processing_time": parsing_result.get("processing_time", 0),
                    "success": bool(parsing_result),
                    "timestamp": task.created_at.isoformat() if task.created_at else ""
                }
            }
            
            # æ„å»ºå†…å®¹åˆ†æç»“æœå¯¹è±¡
            content_object = {
                "interface_name": "å†…å®¹åˆ†ææ¥å£",
                "endpoint": f"/api/file/analyze/{task_id}",
                "status": "completed" if content_analysis else "pending",
                "data": {
                    "content_type": content_analysis.get("content_type", "document"),
                    "document_type": content_analysis.get("document_type", "æœªçŸ¥"),
                    "language": content_analysis.get("language", "zh-CN"),
                    "word_count": content_analysis.get("word_count", 0),
                    "char_count": content_analysis.get("char_count", 0),
                    "summary": content_analysis.get("summary", ""),
                    "keywords": content_analysis.get("keywords", []),
                    "complexity_level": content_analysis.get("complexity_level", "ä¸­ç­‰"),
                    "structure_analysis": content_analysis.get("structure_analysis", {
                        "paragraphs": 0,
                        "lines": 0,
                        "sections": 0
                    }),
                    "crud_analysis": content_analysis.get("crud_analysis", {
                        "operations": [],
                        "requirements": [],
                        "changes": [],
                        "total_operations": 0,
                        "operation_types": []
                    }),
                    "business_insights": content_analysis.get("business_insights", {
                        "main_functions": [],
                        "technical_requirements": [],
                        "estimated_development_time": "0å¤©",
                        "priority_features": []
                    })
                },
                "metadata": {
                    "analysis_version": content_analysis.get("analysis_metadata", {}).get("analysis_version", "2.0"),
                    "confidence_score": content_analysis.get("analysis_metadata", {}).get("confidence_score", 0.0),
                    "analyzed_at": content_analysis.get("analysis_metadata", {}).get("analyzed_at", ""),
                    "parsing_input_used": content_analysis.get("analysis_metadata", {}).get("parsing_input_used", False),
                    "success": bool(content_analysis)
                }
            }
            
            # æ„å»ºAIåˆ†æç»“æœå¯¹è±¡
            ai_object = {
                "interface_name": "AIæ™ºèƒ½åˆ†ææ¥å£",
                "endpoint": f"/api/file/ai-analyze/{task_id}",
                "status": "completed" if ai_analysis else "pending",
                "data": {
                    "analysis_type": ai_analysis.get("analysis_type", "comprehensive"),
                    "api_interfaces": ai_analysis.get("api_interfaces", []),
                    "mq_configuration": ai_analysis.get("mq_configuration", {}),
                    "technical_specifications": ai_analysis.get("technical_specifications", {}),
                    "implementation_priority": ai_analysis.get("implementation_priority", []),
                    "integration_points": ai_analysis.get("integration_points", []),
                    "ai_insights": ai_analysis.get("ai_insights", {
                        "api_interfaces": [],
                        "mq_configuration": {},
                        "technical_specifications": {},
                        "implementation_priority": [],
                        "integration_points": []
                    })
                },
                "metadata": {
                    "confidence_score": ai_analysis.get("confidence_score", 0.0),
                    "analysis_model": ai_analysis.get("analysis_model", ""),
                    "analysis_duration": ai_analysis.get("analysis_duration", 0.0),
                    "analyzed_at": ai_analysis.get("analyzed_at", ""),
                    "success": ai_analysis.get("success", False),
                    "input_data": ai_analysis.get("input_data", {
                        "crud_operations_processed": 0,
                        "content_analysis_used": False,
                        "document_type": "æœªçŸ¥"
                    }),
                    "error": ai_analysis.get("error", "")
                }
            }
            
            # è®¡ç®—æ•´ä½“å®ŒæˆçŠ¶æ€
            overall_status = "pending"
            if task.status == "fully_completed":
                overall_status = "completed"
            elif task.status in ["ai_failed", "content_failed", "failed"]:
                overall_status = "failed"
            elif task.status in ["ai_analyzing", "content_analyzing", "parsing"]:
                overall_status = "processing"
            
            # è®¡ç®—æ•´ä½“è¿›åº¦
            interface_progress = {
                "parsing": 100 if parsing_result else 0,
                "content_analysis": 100 if content_analysis else 0,
                "ai_analysis": 100 if ai_analysis and ai_analysis.get("success") else 0
            }
            overall_progress = sum(interface_progress.values()) // 3
            
            # æ„å»ºæœ€ç»ˆçš„æ•´åˆç»“æœ
            result = {
                "success": True,
                "task_id": task_id,
                "overall_status": overall_status,
                "overall_progress": overall_progress,
                "interface_progress": interface_progress,
                "current_step": task.status or "unknown",
                "processing_steps": task.steps or [],
                "interfaces": {
                    "document_parsing": parsing_object,
                    "content_analysis": content_object,
                    "ai_analysis": ai_object
                },
                "summary": {
                    "total_interfaces": 3,
                    "completed_interfaces": sum(1 for obj in [parsing_object, content_object, ai_object] if obj["status"] == "completed"),
                    "document_type": content_analysis.get("document_type", "æœªçŸ¥"),
                    "complexity_level": content_analysis.get("complexity_level", "ä¸­ç­‰"),
                    "crud_operations_count": len(content_analysis.get("crud_analysis", {}).get("operations", [])),
                    "api_interfaces_count": len(ai_analysis.get("api_interfaces", [])),
                    "mq_topics_count": len(ai_analysis.get("mq_configuration", {}).get("topics", [])),
                    "estimated_development_time": content_analysis.get("business_insights", {}).get("estimated_development_time", "æœªçŸ¥")
                },
                "data_flow": {
                    "step1": "æ–‡æ¡£è§£æ â†’ æå–æ–‡æœ¬å†…å®¹å’ŒåŸºç¡€ä¿¡æ¯",
                    "step2": "å†…å®¹åˆ†æ â†’ è¯†åˆ«CRUDæ“ä½œå’Œä¸šåŠ¡éœ€æ±‚",
                    "step3": "AIåˆ†æ â†’ ç”Ÿæˆæ¥å£è®¾è®¡å’ŒMQé…ç½®",
                    "integration": "ä¸‰ä¸ªæ¥å£ç»“æœæ•´åˆä¸ºå®Œæ•´çš„å¼€å‘æ–¹æ¡ˆ"
                },
                "error": task.error or "",
                "file_info": task.file_info or {},
                "timestamps": {
                    "created_at": task.created_at.isoformat() if task.created_at else "",
                    "updated_at": task.updated_at.isoformat() if task.updated_at else "",
                    "parsing_completed": parsing_object["metadata"]["timestamp"],
                    "content_analysis_completed": content_object["metadata"]["analyzed_at"],
                    "ai_analysis_completed": ai_object["metadata"]["analyzed_at"]
                }
            }
            
            return jsonify(result)
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"è·å–åˆ†æç»“æœå¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"è·å–åˆ†æç»“æœå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/list', methods=['GET'])
def list_files():
    """è·å–å·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨"""
    try:
        tasks = get_all_tasks()
        file_list = []
        for task in tasks:
            file_list.append({
                "task_id": task.id,
                "file_info": task.file_info,
                "status": task.status,
                "progress": task.progress,
                "created_at": task.created_at.isoformat(),
                "updated_at": task.updated_at.isoformat()
            })
        
        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        file_list.sort(key=lambda x: x["created_at"], reverse=True)
        
        return jsonify({
            "success": True,
            "files": file_list,
            "count": len(file_list),
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/delete/<task_id>', methods=['DELETE'])
def delete_file(task_id):
    """åˆ é™¤å·²ä¸Šä¼ æ–‡ä»¶"""
    try:
        # å…ˆè·å–ä»»åŠ¡ä¿¡æ¯
        task = get_task(task_id)
        if not task:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
        
        # åˆ é™¤ç‰©ç†æ–‡ä»¶
        if task.file_path and os.path.exists(task.file_path):
            try:
                os.remove(task.file_path)
                logger.info(f"åˆ é™¤æ–‡ä»¶: {task.file_path}")
            except Exception as e:
                logger.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
        
        # ä»æ•°æ®åº“ä¸­åˆ é™¤ä»»åŠ¡è®°å½•
        if delete_task(task_id):
            return jsonify({
                "success": True,
                "message": f"æ–‡ä»¶ {task.file_info.get('name', 'æœªçŸ¥æ–‡ä»¶')} å·²åˆ é™¤",
                "task_id": task_id,
                "timestamp": datetime.now().isoformat()
            })
        else:
            return jsonify({
                "success": False,
                "error": "åˆ é™¤ä»»åŠ¡è®°å½•å¤±è´¥"
            }), 500
            
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}"
        }), 500

def create_app():
    """åˆ›å»ºFlaskåº”ç”¨"""
    # åˆå§‹åŒ–ä»»åŠ¡å­˜å‚¨
    try:
        logger.info("åˆå§‹åŒ–ä»»åŠ¡å­˜å‚¨æ•°æ®åº“...")
        # TaskStorageåœ¨åˆå§‹åŒ–æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨_init_database
        logger.info("ä»»åŠ¡å­˜å‚¨æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
        
    except Exception as e:
        logger.error(f"ä»»åŠ¡å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    return app

if __name__ == '__main__':
    app = create_app()
    app.run(host='0.0.0.0', port=8082, debug=True) 