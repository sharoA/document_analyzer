#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
analyDesign APIæœåŠ¡å™¨
é›†æˆç«å±±å¼•æ“å®æ—¶äº¤äº’åŠŸèƒ½å’Œæ–‡ä»¶å¤„ç†åŠŸèƒ½
"""

import json
import time
import uuid
import logging
import os
import base64
from datetime import datetime
from typing import Dict, Any, Optional, List
from concurrent.futures import ThreadPoolExecutor

from flask import Flask, request, jsonify, make_response
from flask_cors import CORS

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—é…ç½®
try:
    from ..utils.logger_config import initialize_logging, get_logger, log_api_request, log_analysis_step
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨ç»å¯¹å¯¼å…¥
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from src.utils.logger_config import initialize_logging, get_logger, log_api_request, log_analysis_step

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
initialize_logging()

# è·å–æ—¥å¿—å™¨
logger = get_logger('api_server')

# å¯¼å…¥é…ç½®å’Œå·¥å…·ç±»
try:
    from ..resource.config import get_config
    from ..utils.volcengine_client import VolcengineClient, VolcengineConfig
    from ..utils.llm_logger import LLMLogger
    from ..utils.task_storage import get_task_storage
    from ..utils.redis_task_storage import get_redis_task_storage
    from ..utils.markdown_storage import get_markdown_storage
    # å¯¼å…¥åˆ†ææœåŠ¡æ¨¡å—
    from ..analysis_services import (
        AnalysisServiceManager,
        get_analysis_service_manager,
        ensure_analysis_directories,
        setup_analysis_logger,
        validate_input,
        validate_file_upload,
        initialize_analysis_service_manager
    )
    # å¯¼å…¥ç¼–ç æ™ºèƒ½ä½“API
    from ..apis.coder_agent_api import coder_agent_api
    try:
        from ..utils.enhanced_analyzer import EnhancedAnalyzer
        ENHANCED_ANALYZER_AVAILABLE = True
    except ImportError:
        ENHANCED_ANALYZER_AVAILABLE = False
        logger.warning("enhanced_analyzerä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æ–‡æœ¬å¤„ç†")
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from src.resource.config import get_config
    from src.utils.volcengine_client import VolcengineClient, VolcengineConfig
    from src.utils.llm_logger import LLMLogger
    from src.utils.task_storage import get_task_storage
    from src.utils.redis_task_storage import get_redis_task_storage
    from src.utils.markdown_storage import get_markdown_storage
    # å¯¼å…¥åˆ†ææœåŠ¡æ¨¡å—
    from src.analysis_services import (
        AnalysisServiceManager,
        get_analysis_service_manager,
        ensure_analysis_directories,
        setup_analysis_logger,
        validate_input,
        validate_file_upload,
        initialize_analysis_service_manager
    )
    # å¯¼å…¥ç¼–ç æ™ºèƒ½ä½“API
    from src.apis.coder_agent_api import coder_agent_api
    try:
        from src.utils.enhanced_analyzer import EnhancedAnalyzer
        ENHANCED_ANALYZER_AVAILABLE = True
    except ImportError:
        ENHANCED_ANALYZER_AVAILABLE = False
        logger.warning("enhanced_analyzerä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€æ–‡æœ¬å¤„ç†")

# è·å–é…ç½®
config = get_config()
task_storage = get_task_storage()
redis_task_storage = get_redis_task_storage()
markdown_storage = get_markdown_storage()

app = Flask(__name__)
# é…ç½®CORSï¼Œå…è®¸æ‰€æœ‰æ¥æºå’Œæ–¹æ³•
CORS(app, 
     origins=["http://localhost:3000", "http://127.0.0.1:3000"],
     methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
     allow_headers=["Content-Type", "Authorization"],
     supports_credentials=True)

# æ³¨å†Œç¼–ç æ™ºèƒ½ä½“APIè“å›¾
app.register_blueprint(coder_agent_api)

# æ·»åŠ è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶
@app.before_request
def log_request_info():
    """è®°å½•è¯·æ±‚ä¿¡æ¯"""
    import time
    request.start_time = time.time()

@app.after_request  
def log_request_result(response):
    """è®°å½•è¯·æ±‚ç»“æœ"""
    import time
    duration = time.time() - getattr(request, 'start_time', time.time())
    log_api_request(
        method=request.method,
        endpoint=request.endpoint or request.path,
        status_code=response.status_code,
        duration=duration
    )
    return response

# ç¡®ä¿åˆ†ææœåŠ¡ç›®å½•ç»“æ„å­˜åœ¨
ensure_analysis_directories()

# è®¾ç½®åˆ†ææœåŠ¡æ—¥å¿—
analysis_logger = setup_analysis_logger("api_server")

# åˆå§‹åŒ–ç«å±±å¼•æ“å®¢æˆ·ç«¯
try:
    volcengine_config = config.get_volcengine_config()
    logger.info(f"ä»é…ç½®æ–‡ä»¶è¯»å–ç«å±±å¼•æ“é…ç½®: {volcengine_config}")
    
    # æ£€æŸ¥APIå¯†é’¥
    api_key = volcengine_config.get('api_key', '')
    if not api_key:
        raise ValueError("ç«å±±å¼•æ“APIå¯†é’¥æœªåœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®")
    
    volcano_config = VolcengineConfig(
        api_key=api_key,
        model_id=volcengine_config.get('model', 'doubao-pro-4k'),
        base_url=volcengine_config.get('endpoint', 'https://ark.cn-beijing.volces.com/api/v3'),
        temperature=volcengine_config.get('temperature', 0.7),
        max_tokens=volcengine_config.get('max_tokens', 4000)
    )
    volcano_client = VolcengineClient(volcano_config)
    logger.info("ç«å±±å¼•æ“å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    logger.error(f"ç«å±±å¼•æ“å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    volcano_client = None

# åˆå§‹åŒ–åˆ†ææœåŠ¡ç®¡ç†å™¨
try:
    # å…ˆåˆå§‹åŒ–åˆ†ææœåŠ¡ç®¡ç†å™¨
    # ä»é…ç½®æ–‡ä»¶è¯»å–å‘é‡æ•°æ®åº“ç±»å‹å’Œé…ç½®
    vector_db_config = config.get_vector_database_config()
    vector_db_type = vector_db_config.get('type', 'mock')  # é»˜è®¤ä½¿ç”¨mock
    
    # å‡†å¤‡å‘é‡æ•°æ®åº“å‚æ•°
    vector_db_kwargs = {}
    if vector_db_type.lower() == 'weaviate':
        weaviate_config = vector_db_config.get('weaviate', {})
        vector_db_kwargs.update({
            'host': weaviate_config.get('host', 'localhost'),
            'port': weaviate_config.get('port', 8080),
            'grpc_port': weaviate_config.get('grpc_port', 50051),
            'scheme': weaviate_config.get('scheme', 'http'),
            'api_key': weaviate_config.get('api_key'),
            'collection_name': weaviate_config.get('default_collection', {}).get('name', 'AnalyDesignDocuments')
        })
        logger.info(f"Weaviateé…ç½®: {weaviate_config.get('scheme', 'http')}://{weaviate_config.get('host', 'localhost')}:{weaviate_config.get('port', 8080)}")
    
    analysis_service_manager = initialize_analysis_service_manager(
        llm_client=volcano_client,
        vector_db_type=vector_db_type,
        **vector_db_kwargs
    )
    logger.info("åˆ†ææœåŠ¡ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    analysis_logger.info("åˆ†ææœåŠ¡ç®¡ç†å™¨å·²é›†æˆåˆ°APIæœåŠ¡å™¨")
except Exception as e:
    logger.error(f"åˆ†ææœåŠ¡ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    analysis_service_manager = None

# åˆå§‹åŒ–å¢å¼ºåˆ†æå™¨
try:
    if ENHANCED_ANALYZER_AVAILABLE:
        analyzer = EnhancedAnalyzer()
        logger.info("å¢å¼ºåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
    else:
        analyzer = None
        logger.info("ä½¿ç”¨åŸºç¡€æ–‡æœ¬å¤„ç†åŠŸèƒ½")
except Exception as e:
    logger.error(f"å¢å¼ºåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    analyzer = None

# ä¼šè¯å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨Redisç­‰æŒä¹…åŒ–å­˜å‚¨ï¼‰
sessions = {}

# çº¿ç¨‹æ± ç”¨äºå¼‚æ­¥å¤„ç†
executor = ThreadPoolExecutor(max_workers=4)

# æ–‡ä»¶å¤„ç†ä»»åŠ¡ç±»
class FileParsingTask:
    def __init__(self, task_id: str, file_info: dict, file_path: str = None):
        self.id = task_id
        self.file_info = file_info
        self.file_content = None  # ä¸å†å­˜å‚¨æ–‡ä»¶å†…å®¹ï¼Œåªå­˜å‚¨è·¯å¾„
        self.file_path = file_path
        self.status = "pending"
        self.progress = 0
        self.steps = []
        self.result = None
        self.content_analysis = None
        self.ai_analysis = None
        self.markdown_content = None
        self.error = None
        self.created_at = datetime.now()
        self.updated_at = datetime.now()
        
        # ç«‹å³ä¿å­˜åˆ°æ•°æ®åº“
        self._save_to_db()
    
    def _save_to_db(self):
        """ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“"""
        try:
            # ä¿å­˜åˆ°SQLiteæ•°æ®åº“
            task_storage.create_task(
                filename=self.file_info.get('filename', ''),
                file_size=self.file_info.get('size', 0),
                file_type=self.file_info.get('type', ''),
                task_id=self.id
            )
            # åŒæ—¶ä¿å­˜åˆ°Redis
            redis_task_storage.create_task(
                filename=self.file_info.get('filename', ''),
                file_size=self.file_info.get('size', 0),
                file_type=self.file_info.get('type', ''),
                task_id=self.id
            )
        except Exception as e:
            logger.error(f"ä¿å­˜ä»»åŠ¡åˆ°æ•°æ®åº“å¤±è´¥: {e}")
    
    def update_progress(self, progress: int, description: str, status: str = None):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        self.progress = progress
        self.updated_at = datetime.now()
        if status:
            self.status = status
        
        # æ›´æ–°æ­¥éª¤ä¿¡æ¯
        step_info = {
            "description": description,
            "progress": progress,
            "status": status or self.status,
            "timestamp": self.updated_at.isoformat()
        }
        
        # ç¡®ä¿ steps æ˜¯ä¸€ä¸ªåˆ—è¡¨
        if not isinstance(self.steps, list):
            self.steps = []
        
        # å¦‚æœæ˜¯æ–°æ­¥éª¤ï¼Œæ·»åŠ åˆ°åˆ—è¡¨ä¸­
        try:
            # å®‰å…¨æ£€æŸ¥æœ€åä¸€ä¸ªæ­¥éª¤
            if (not self.steps or 
                not isinstance(self.steps[-1], dict) or 
                self.steps[-1].get("description") != description):
                self.steps.append(step_info)
            else:
                # æ›´æ–°æœ€åä¸€ä¸ªæ­¥éª¤
                self.steps[-1] = step_info
        except (IndexError, KeyError, TypeError) as e:
            # å¦‚æœå‡ºç°ä»»ä½•é”™è¯¯ï¼Œç›´æ¥æ·»åŠ æ–°æ­¥éª¤
            logger.warning(f"æ­¥éª¤æ›´æ–°å¼‚å¸¸ï¼Œæ·»åŠ æ–°æ­¥éª¤: {e}")
            self.steps.append(step_info)
        
        # æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
        try:
            task_storage.update_task_status(
                task_id=self.id,
                status=self.status,
                progress=self.progress
            )
        except Exception as e:
            logger.error(f"æ›´æ–°æ•°æ®åº“ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        
        logger.info(f"ä»»åŠ¡ {self.id} è¿›åº¦æ›´æ–°: {progress}% - {description}")
    
    def to_dict(self):
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        # ç¡®ä¿ file_info å­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
        file_info = self.file_info
        if file_info is None:
            file_info = {
                'name': 'æœªçŸ¥æ–‡ä»¶',
                'type': 'unknown',
                'size': 0
            }
        
        return {
            "id": self.id,
            "file_info": file_info,
            "file_path": self.file_path,
            "status": self.status,
            "progress": self.progress,
            "steps": self.steps,
            "result": self.result,
            "content_analysis": self.content_analysis,
            "ai_analysis": self.ai_analysis,
            "markdown_content": self.markdown_content,
            "error": self.error,
            "created_at": self.created_at.isoformat() if self.created_at else None,
            "updated_at": self.updated_at.isoformat() if self.updated_at else None
        }
    
    @classmethod
    def from_dict(cls, data: dict):
        """ä»å­—å…¸åˆ›å»ºä»»åŠ¡å®ä¾‹"""
        task = cls.__new__(cls)  # åˆ›å»ºå®ä¾‹ä½†ä¸è°ƒç”¨__init__
        task.id = data['id']
        
        # ç¡®ä¿ file_info å­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
        task.file_info = data.get('file_info')
        if task.file_info is None:
            task.file_info = {
                'name': 'æœªçŸ¥æ–‡ä»¶',
                'type': 'unknown',
                'size': 0
            }
        
        task.file_path = data.get('file_path')
        task.file_content = None  # ä¸ä»æ•°æ®åº“æ¢å¤æ–‡ä»¶å†…å®¹
        task.status = data['status']
        task.progress = data['progress']
        task.steps = data.get('steps', [])
        task.result = data.get('parsing_result') or data.get('result')
        task.content_analysis = data.get('content_analysis')
        task.ai_analysis = data.get('ai_analysis')
        task.markdown_content = data.get('markdown_content')
        task.error = data.get('error')
        task.created_at = datetime.fromisoformat(data['created_at']) if isinstance(data['created_at'], str) else data['created_at']
        task.updated_at = datetime.fromisoformat(data['updated_at']) if isinstance(data['updated_at'], str) else data['updated_at']
        return task

# ä»»åŠ¡ç®¡ç†å‡½æ•°
def get_task(task_id: str) -> Optional[FileParsingTask]:
    """ä»æ•°æ®åº“è·å–ä»»åŠ¡"""
    task_data = task_storage.get_task(task_id)
    if task_data:
        return FileParsingTask.from_dict(task_data)
    return None

def get_all_tasks() -> List[FileParsingTask]:
    """è·å–æ‰€æœ‰ä»»åŠ¡"""
    tasks_data = task_storage.get_all_tasks()
    return [FileParsingTask.from_dict(data) for data in tasks_data]

def delete_task(task_id: str) -> bool:
    """åˆ é™¤ä»»åŠ¡"""
    return task_storage.delete_task(task_id)

# æ–‡æ¡£è§£æå‡½æ•°
def parse_word_document(file_content: bytes, file_name: str) -> dict:
    """è§£æWordæ–‡æ¡£"""
    try:
        if analyzer:
            # ä½¿ç”¨å¢å¼ºåˆ†æå™¨
            return analyzer.transform_file(file_content, file_name)
        else:
            # åŸºç¡€è§£æ
            return {
                "text_content": f"Wordæ–‡æ¡£: {file_name}",
                "file_type": "word",
                "file_size": len(file_content),
                "analysis_method": "basic_word_parsing",
                "message": "å¢å¼ºåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€è§£æ"
            }
    except Exception as e:
        logger.error(f"Wordæ–‡æ¡£è§£æå¤±è´¥: {e}")
        return {
            "text_content": f"Wordæ–‡æ¡£è§£æå¤±è´¥: {str(e)}",
            "file_type": "word",
            "file_size": len(file_content),
            "analysis_method": "error_fallback"
        }

def parse_pdf_document(file_content: bytes, file_name: str) -> dict:
    """è§£æPDFæ–‡æ¡£"""
    try:
        if analyzer:
            # ä½¿ç”¨å¢å¼ºåˆ†æå™¨
            return analyzer.transform_file(file_content, file_name)
        else:
            # åŸºç¡€è§£æ
            return {
                "text_content": f"PDFæ–‡æ¡£: {file_name}",
                "file_type": "pdf",
                "file_size": len(file_content),
                "analysis_method": "basic_pdf_parsing",
                "message": "å¢å¼ºåˆ†æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€è§£æ"
            }
    except Exception as e:
        logger.error(f"PDFæ–‡æ¡£è§£æå¤±è´¥: {e}")
        return {
            "text_content": f"PDFæ–‡æ¡£è§£æå¤±è´¥: {str(e)}",
            "file_type": "pdf",
            "file_size": len(file_content),
            "analysis_method": "error_fallback"
        }

def parse_text_document(file_content: bytes, file_name: str) -> dict:
    """è§£ææ–‡æœ¬æ–‡æ¡£"""
    try:
        # å°è¯•ä¸åŒçš„ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
        content = None
        
        for encoding in encodings:
            try:
                content = file_content.decode(encoding)
                break
            except UnicodeDecodeError:
                continue
        
        if content is None:
            content = file_content.decode('utf-8', errors='ignore')
        
        return {
            "text_content": content,
            "file_type": "text",
            "file_size": len(file_content),
            "analysis_method": "text_parsing",
            "char_count": len(content),
            "line_count": content.count('\n') + 1
        }
    except Exception as e:
        logger.error(f"æ–‡æœ¬æ–‡æ¡£è§£æå¤±è´¥: {e}")
        return {
            "text_content": f"æ–‡æœ¬æ–‡æ¡£è§£æå¤±è´¥: {str(e)}",
            "file_type": "text",
            "file_size": len(file_content),
            "analysis_method": "error_fallback"
        }

def extract_text_from_file(file_path: str) -> str:
    """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–æ–‡æœ¬å†…å®¹"""
    try:
        logger.info(f"ä»æ–‡ä»¶è·¯å¾„è¯»å–: {file_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è·å–æ–‡ä»¶æ‰©å±•å
        file_name = os.path.basename(file_path)
        file_ext = file_name.lower().split('.')[-1] if '.' in file_name else ''
        
        logger.info(f"ä½¿ç”¨é€šç”¨æ–‡ä»¶åˆ†ææ–¹æ³•: {file_name}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, 'rb') as f:
            file_content = f.read()
        
        # ä½¿ç”¨å¢å¼ºåˆ†æå™¨å¤„ç†æ–‡ä»¶
        if ENHANCED_ANALYZER_AVAILABLE:
            try:
                current_analyzer = EnhancedAnalyzer()
                
                if file_name.lower().endswith(('.doc', '.docx')) or file_ext in ['doc', 'docx']:
                    logger.info(f"æ£€æµ‹åˆ°Wordæ–‡æ¡£ï¼Œä½¿ç”¨Markdownè½¬æ¢: {file_name}")
                    transform_result = current_analyzer.parse_word_document(file_content, file_name)
                    extracted_text = transform_result.get("text_content", "Wordæ–‡æ¡£è§£æå¤±è´¥")
                    if extracted_text and any(marker in extracted_text for marker in ['#', '|', '**', '*', '-']):
                        logger.info(f"Wordæ–‡æ¡£å·²æˆåŠŸè½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œé•¿åº¦: {len(extracted_text)} å­—ç¬¦")
                    else:
                        logger.warning("Wordæ–‡æ¡£è½¬æ¢ç»“æœå¯èƒ½ä¸æ˜¯æ ‡å‡†Markdownæ ¼å¼")
                else:
                    # å¯¹äºå…¶ä»–æ–‡ä»¶ç±»å‹ï¼Œå°è¯•ç›´æ¥è§£ç 
                    try:
                        extracted_text = file_content.decode('utf-8')
                    except UnicodeDecodeError:
                        try:
                            extracted_text = file_content.decode('gbk')
                        except UnicodeDecodeError:
                            extracted_text = file_content.decode('utf-8', errors='ignore')
                
                logger.info("æå–çš„æ–‡æœ¬å†…å®¹å·²å‡†å¤‡å®Œæ¯•")
                return extracted_text
                
            except Exception as e:
                logger.error(f"å¢å¼ºåˆ†æå™¨å¤„ç†å¤±è´¥: {e}")
                # é™çº§åˆ°åŸºç¡€æ–‡æœ¬å¤„ç†
                pass
        
        # åŸºç¡€æ–‡æœ¬å¤„ç†ï¼ˆfallbackï¼‰
        try:
            extracted_text = file_content.decode('utf-8')
        except UnicodeDecodeError:
            try:
                extracted_text = file_content.decode('gbk')
            except UnicodeDecodeError:
                extracted_text = file_content.decode('utf-8', errors='ignore')
        
        logger.info("ä½¿ç”¨åŸºç¡€æ–¹æ³•æå–æ–‡æœ¬å†…å®¹å®Œæˆ")
        return extracted_text
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶æ–‡æœ¬æå–å¤±è´¥: {e}")
        return f"æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}"

# æ–‡ä»¶å¤„ç†å‡½æ•°
def process_file_parsing(task: FileParsingTask):
    """å¤„ç†æ–‡ä»¶è§£æä»»åŠ¡ - ä½¿ç”¨åˆ†ææœåŠ¡æ¨¡å—"""
    try:
        log_analysis_step(task.id, "æ–‡ä»¶è§£æ", "å¼€å§‹", f"æ–‡ä»¶: {task.file_info.get('name', 'unknown')}")
        task.update_progress(10, "å¼€å§‹è§£ææ–‡ä»¶", "parsing")
        
        file_info = task.file_info
        file_name = file_info.get("name", "")
        file_type = file_info.get("type", "")
        

        # ç›´æ¥è°ƒç”¨æ–°æ–¹æ³•
        extracted_text = extract_text_from_file(task.file_path)
        logger.info("æå–çš„æ–‡æœ¬å†…å®¹å·²å‡†å¤‡å®Œæ¯•")
        extracted_preview = extracted_text.replace('{', '{{').replace('}', '}}') if extracted_text else "æ— å†…å®¹"
        logger.info(f"è½¬æ¢åå†…å®¹é¢„è§ˆ: {extracted_preview}")

        # å®‰å…¨çš„æ—¥å¿—è®°å½•ï¼Œé¿å…æ ¼å¼åŒ–é”™è¯¯
        logger.info("æå–çš„æ–‡æœ¬å†…å®¹å·²å‡†å¤‡å®Œæ¯•")
        extracted_preview = extracted_text.replace('{', '{{').replace('}', '}}') if extracted_text else "æ— å†…å®¹"
        logger.info(f"è½¬æ¢åå†…å®¹é¢„è§ˆ: {extracted_preview}")

        
        # éªŒè¯è¾“å…¥ - ä½¿ç”¨æå–çš„æ–‡æœ¬å†…å®¹è¿›è¡ŒéªŒè¯
        validation = validate_input(task.id, extracted_text, file_type)
        if not all(validation.values()):
            raise ValueError(f"è¾“å…¥éªŒè¯å¤±è´¥: {validation}")
        
        task.update_progress(40, "ä½¿ç”¨åˆ†ææœåŠ¡è§£ææ–‡æ¡£", "parsing")
        
        # ä½¿ç”¨åˆ†ææœåŠ¡ç®¡ç†å™¨è¿›è¡Œæ–‡æ¡£è§£æ
        if analysis_service_manager:
            parsing_result = analysis_service_manager.parse_document_sync(
                task_id=task.id,
                file_content=extracted_text,
                file_type=file_type.split('/')[-1] if '/' in file_type else file_type,
                file_name=file_name
            )
            
            # æ›´æ–°ä»»åŠ¡ç»“æœ
            task.result = parsing_result
            # ä¿å­˜è§£æç»“æœåˆ°Redis
            redis_task_storage.save_parsing_result(task.id, parsing_result)
            task.update_progress(100, "æ–‡æ¡£è§£æå®Œæˆ", "parsed")
            log_analysis_step(task.id, "æ–‡ä»¶è§£æ", "å®Œæˆ", f"è§£æç»“æœå·²ä¿å­˜")
            analysis_logger.info(f"âœ… æ–‡ä»¶è§£æå®Œæˆ: {task.id}")
        else:
            # é™çº§åˆ°åŸæœ‰çš„è§£æé€»è¾‘ - éœ€è¦è¯»å–æ–‡ä»¶å†…å®¹
            task.update_progress(60, "ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è§£ææ–‡æ¡£", "parsing")
            
            # è¯»å–æ–‡ä»¶å†…å®¹ç”¨äºä¼ ç»Ÿè§£ææ–¹æ³•
            with open(task.file_path, 'rb') as f:
                file_content = f.read()
            
            if file_name.lower().endswith(('.doc', '.docx')) or 'word' in file_type.lower():
                result = parse_word_document(file_content, file_name)
            elif file_name.lower().endswith('.pdf') or file_type == 'application/pdf':
                result = parse_pdf_document(file_content, file_name)
            elif file_name.lower().endswith(('.txt', '.md')) or 'text' in file_type.lower():
                result = parse_text_document(file_content, file_name)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_type}")
            
            task.result = result
            # ä¿å­˜è§£æç»“æœåˆ°Redis
            redis_task_storage.save_parsing_result(task.id, result)
            task.update_progress(100, "æ–‡æ¡£è§£æå®Œæˆ", "parsed")
            logger.info(f"æ–‡ä»¶è§£æå®Œæˆ: {task.id}")
        
    except Exception as e:
        error_msg = f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}"
        logger.error(f"ä»»åŠ¡ {task.id} è§£æå¤±è´¥: {e}")
        log_analysis_step(task.id, "æ–‡ä»¶è§£æ", "å¤±è´¥", str(e))
        analysis_logger.error(f"âŒ ä»»åŠ¡ {task.id} è§£æå¤±è´¥: {e}")
        task.error = error_msg
        task.status = "failed"
        task.update_progress(task.progress, error_msg, "failed")

def process_content_analysis(task: FileParsingTask, parsing_result: dict):
    """å¤„ç†å†…å®¹åˆ†æä»»åŠ¡ - ä½¿ç”¨åˆ†ææœåŠ¡æ¨¡å—"""
    try:
        # æ£€æŸ¥å‰ç½®æ¡ä»¶ - å…è®¸ content_analyzing çŠ¶æ€ï¼ˆV2æµç¨‹ä¸­å·²ç»é¢„å…ˆè®¾ç½®äº†çŠ¶æ€ï¼‰
        if task.status not in ["parsed", "content_analyzing"]:
            raise ValueError(f"ä»»åŠ¡çŠ¶æ€ä¸æ­£ç¡®ï¼ŒæœŸæœ› 'parsed' æˆ– 'content_analyzing'ï¼Œå®é™… '{task.status}'")
        
        if not task.result:
            raise ValueError("è§£æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æ")
        
        log_analysis_step(task.id, "å†…å®¹åˆ†æ", "å¼€å§‹", f"çŠ¶æ€: {task.status}, è¿›åº¦: {task.progress}%")
        analysis_logger.info(f"ğŸ” å¼€å§‹å†…å®¹åˆ†æä»»åŠ¡: {task.id}")
        analysis_logger.info(f"ğŸ“Š ä»»åŠ¡å½“å‰çŠ¶æ€: {task.status}")
        analysis_logger.info(f"ğŸ“Š ä»»åŠ¡å½“å‰è¿›åº¦: {task.progress}%")
        analysis_logger.info(f"ğŸ“Š è§£æç»“æœå­˜åœ¨: {bool(task.result)}")
        analysis_logger.info(f"ğŸ“Š åˆ†ææœåŠ¡ç®¡ç†å™¨å¯ç”¨: {bool(analysis_service_manager)}")
        
        # è®¾ç½®çŠ¶æ€ä¸ºå†…å®¹åˆ†æä¸­
        task.status = "content_analyzing"
        task.update_progress(10, "å¼€å§‹å†…å®¹åˆ†æ", "content_analyzing")
        analysis_logger.info(f"âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸º: {task.status}")
        
        # æ­£ç¡®æå–æ–‡æœ¬å†…å®¹
        content = extract_text_from_file(task.file_path)
        task.update_progress(30, "è·å–åˆ°ä¸Šä¼ æ–‡ä»¶", "content_analyzing")
        analysis_logger.info(f"ğŸ“„ æå–åˆ°æ–‡æ¡£å†…å®¹ï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
        
        # ä½¿ç”¨åˆ†ææœåŠ¡ç®¡ç†å™¨è¿›è¡Œå†…å®¹åˆ†æ
        if analysis_service_manager:
            task.update_progress(35, "å†…å®¹åˆ†æç®¡ç†å™¨å¼€å§‹åˆ†æ", "content_analyzing")
            try:
                content_result = analysis_service_manager.analyze_content_sync(
                    task_id=task.id,
                    parsing_result=parsing_result,
                    document_content=content
                )
                
                # ä¿å­˜å†…å®¹åˆ†æç»“æœåˆ°Redis
                redis_task_storage.save_content_analysis(task.id, content_result)
                task.update_progress(100, "å†…å®¹åˆ†æå®Œæˆ", "content_analyzed")
                analysis_logger.info(f"âœ… å†…å®¹åˆ†æå®Œæˆ: {task.id}")
                return
                
            except Exception as e:
                analysis_logger.warning(f"âš ï¸ ç«å±±å¼•æ“åˆ†æå¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿæ–¹æ³•: {str(e)}")
        
        # ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œå†…å®¹åˆ†æï¼ˆé™çº§æ–¹æ¡ˆï¼‰
        task.update_progress(30, "ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•è¿›è¡Œå†…å®¹åˆ†æ", "content_analyzing")
        
        # åŸºç¡€å†…å®¹åˆ†æ
        word_count = len(content.split())
        char_count = len(content)
        paragraphs = content.count('\n\n') + 1
        lines = content.count('\n') + 1
        
        task.update_progress(50, "è¯†åˆ«CRUDæ“ä½œå’Œä¸šåŠ¡åŠŸèƒ½", "content_analyzing")
        
        # ç®€åŒ–çš„CRUDåˆ†æ
        crud_operations = []
        content_lower = content.lower()
        
        if any(keyword in content_lower for keyword in ['æ–°å¢', 'åˆ›å»º', 'æ·»åŠ ', 'create', 'add']):
            crud_operations.append({"type": "CREATE", "description": "åˆ›å»ºåŠŸèƒ½"})
        if any(keyword in content_lower for keyword in ['æŸ¥è¯¢', 'æœç´¢', 'è·å–', 'query', 'search']):
            crud_operations.append({"type": "READ", "description": "æŸ¥è¯¢åŠŸèƒ½"})
        if any(keyword in content_lower for keyword in ['ä¿®æ”¹', 'æ›´æ–°', 'update', 'modify']):
            crud_operations.append({"type": "UPDATE", "description": "æ›´æ–°åŠŸèƒ½"})
        if any(keyword in content_lower for keyword in ['åˆ é™¤', 'delete', 'remove']):
            crud_operations.append({"type": "DELETE", "description": "åˆ é™¤åŠŸèƒ½"})
        
        # æ„å»ºåˆ†æç»“æœ
        analysis_result = {
            "success": True,
            "data": {
                "content_type": "document",
                "language": "zh-CN",
                "word_count": word_count,
                "char_count": char_count,
                "summary": content[:300] + "..." if len(content) > 300 else content,
                "crud_analysis": {
                    "operations": crud_operations,
                    "total_operations": len(crud_operations),
                    "summary": {
                        "create_count": len([op for op in crud_operations if op["type"] == "CREATE"]),
                        "read_count": len([op for op in crud_operations if op["type"] == "READ"]),
                        "update_count": len([op for op in crud_operations if op["type"] == "UPDATE"]),
                        "delete_count": len([op for op in crud_operations if op["type"] == "DELETE"])
                    }
                }
            },
            "metadata": {
                "analysis_time": datetime.now().isoformat(),
                "method": "traditional",
                "service": "ContentAnalyzerService"
            }
        }
        
        task.content_analysis = analysis_result
        # ä¿å­˜å†…å®¹åˆ†æç»“æœåˆ°Redis
        redis_task_storage.save_content_analysis(task.id, analysis_result)
        task.update_progress(100, "å†…å®¹åˆ†æå®Œæˆ", "content_analyzed")
        analysis_logger.info(f"âœ… å†…å®¹åˆ†æå®Œæˆ: {task.id}")
        
    except Exception as e:
        error_msg = f"å†…å®¹åˆ†æå¤±è´¥: {str(e)}"
        logger.error(f"ä»»åŠ¡ {task.id} å†…å®¹åˆ†æå¤±è´¥: {e}")
        analysis_logger.error(f"âŒ ä»»åŠ¡ {task.id} å†…å®¹åˆ†æå¤±è´¥: {e}")
        task.error = error_msg
        task.status = "failed"
        task.update_progress(task.progress, error_msg, "failed")

def process_ai_analysis(task: FileParsingTask, analysis_type: str = "comprehensive", content_analysis_result: dict = None, parsing_result: dict = None):
    """å¤„ç†AIåˆ†æä»»åŠ¡ - ä½¿ç”¨åˆ†ææœåŠ¡æ¨¡å—"""
    try:
        # æ£€æŸ¥å‰ç½®æ¡ä»¶ - å…è®¸ ai_analyzing çŠ¶æ€ï¼ˆV2æµç¨‹ä¸­å·²ç»é¢„å…ˆè®¾ç½®äº†çŠ¶æ€ï¼‰
        if task.status not in ["content_analyzed", "ai_analyzing"]:
            raise ValueError(f"ä»»åŠ¡çŠ¶æ€ä¸æ­£ç¡®ï¼ŒæœŸæœ› 'content_analyzed' æˆ– 'ai_analyzing'ï¼Œå®é™… '{task.status}'")
        
        if not content_analysis_result:
            raise ValueError("å†…å®¹åˆ†æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡ŒAIåˆ†æ")
        
        analysis_logger.info(f"ğŸ¤– å¼€å§‹AIåˆ†æä»»åŠ¡: {task.id}")
        
        # è®¾ç½®çŠ¶æ€ä¸ºAIåˆ†æä¸­
        task.status = "ai_analyzing"
        task.update_progress(10, "å¼€å§‹AIè®¾è®¡æ–¹æ¡ˆ", "ai_analyzing")
        
        # ä½¿ç”¨åˆ†ææœåŠ¡ç®¡ç†å™¨è¿›è¡ŒAIåˆ†æ
        if analysis_service_manager:
            task.update_progress(30, "ä½¿ç”¨åˆ†ææœåŠ¡è¿›è¡ŒAIåˆ†æ", "ai_analyzing")
            
            # åˆ›å»ºè¿›åº¦å›è°ƒå‡½æ•°
            def progress_callback(stage: int, message: str, status: str = "ai_analyzing"):
                # å°†AIåˆ†æçš„è¿›åº¦æ˜ å°„åˆ°30-100çš„èŒƒå›´
                mapped_progress = int(30 + (stage * 0.7))  # 30% + (stage% * 70%)
                task.update_progress(mapped_progress, message, status)
            
            # ä»å†…å®¹åˆ†æç»“æœä¸­æå–æ•°æ®
            content_data = content_analysis_result
            ai_result = analysis_service_manager.ai_analyze_sync(
                task_id=task.id,
                content_analysis=content_data,
                parsing_result=parsing_result,
                progress_callback=progress_callback
            )
            
            # æ£€æŸ¥AIåˆ†æç»“æœæ˜¯å¦æˆåŠŸ
            analysis_logger.info(f"ğŸ” AIåˆ†æç»“æœæ£€æŸ¥: {task.id}, success={ai_result.get('success')}, ç»“æœç±»å‹={type(ai_result)}")
            if not ai_result.get("success", False):
                analysis_logger.error(f"âŒ AIåˆ†ææœåŠ¡è¿”å›å¤±è´¥: {task.id}, é”™è¯¯={ai_result.get('error', 'æœªçŸ¥é”™è¯¯')}, å®Œæ•´ç»“æœ={str(ai_result)[:500]}")
                raise ValueError(f"AIåˆ†ææœåŠ¡è¿”å›å¤±è´¥: {ai_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            # å°è¯•ä¿å­˜AIåˆ†æç»“æœåˆ°Redisï¼ˆå…è®¸å¤±è´¥ï¼‰
            try:
                redis_task_storage.save_ai_analysis(task.id, ai_result)
                analysis_logger.info(f"âœ… AIåˆ†æç»“æœå·²ä¿å­˜åˆ°Redis: {task.id}")
            except Exception as redis_error:
                analysis_logger.warning(f"âš ï¸ AIåˆ†æç»“æœä¿å­˜åˆ°Rediså¤±è´¥ï¼ˆä¸å½±å“ä»»åŠ¡çŠ¶æ€ï¼‰: {redis_error}")
            
            # æ— è®ºRedisä¿å­˜æ˜¯å¦æˆåŠŸï¼Œéƒ½è¦æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            task.update_progress(100, "AIåˆ†æå®Œæˆ", "ai_analyzed")
            analysis_logger.info(f"âœ… AIåˆ†æå®Œæˆ: {task.id}")
            
        else:
            raise ValueError("AIå®¢æˆ·ç«¯ä¸å¯ç”¨")
        
    except Exception as e:
        error_msg = f"AIåˆ†æå¤±è´¥: {str(e)}"
        logger.error(f"ä»»åŠ¡ {task.id} AIåˆ†æå¤±è´¥: {e}")
        analysis_logger.error(f"âŒ ä»»åŠ¡ {task.id} AIåˆ†æå¤±è´¥: {e}")
        task.error = error_msg
        task.status = "failed"
        task.update_progress(task.progress, error_msg, "failed")

@app.route('/', methods=['GET'])
def index():
    """é¦–é¡µ"""
    return jsonify({
        "service": "analyDesign API Server",
        "version": "2.0.0",
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "endpoints": {
            "chat": "/api/chat",
            "health": "/api/health",
            "sessions": "/api/sessions",
            "file_upload": "/api/file/upload",
            "file_list": "/api/file/list",
            "file_delete": "/api/file/delete/<task_id>",
            "parsing_status": "/api/file/parsing/<task_id>",
            "content_analysis": "/api/file/analyze/<task_id>",
            "ai_analysis": "/api/file/ai-analyze/<task_id>",
            "analysis_result": "/api/file/result/<task_id>",
            "static_files": "/uploads/<path:filename>"
        },
        "features": [
            "HTTP RESTful API",
            "ç«å±±å¼•æ“ AI é›†æˆ",
            "æ–‡ä»¶ä¸Šä¼ å’Œè§£æ",
            "å†…å®¹åˆ†æ",
            "æ™ºèƒ½AIåˆ†æ",
            "ä¼šè¯ç®¡ç†",
            "CORS è·¨åŸŸæ”¯æŒ"
        ]
    })

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    volcano_status = "connected" if volcano_client else "disconnected"
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "api_server": "running",
            "volcano_engine": volcano_status
        }
    })

@app.route('/api/chat', methods=['POST'])
def chat():
    """èŠå¤©æ¥å£"""
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                "success": False,
                "error": "è¯·æ±‚æ•°æ®ä¸ºç©º"
            }), 400
        
        message = data.get('message', '').strip()
        session_id = data.get('session_id', str(uuid.uuid4()))
        
        if not message:
            return jsonify({
                "success": False,
                "error": "æ¶ˆæ¯å†…å®¹ä¸èƒ½ä¸ºç©º"
            }), 400
        
        logger.info(f"æ”¶åˆ°èŠå¤©è¯·æ±‚ - Session: {session_id}, Message: {message}")
        
        # æ›´æ–°ä¼šè¯ä¿¡æ¯
        if session_id not in sessions:
            sessions[session_id] = {
                "created_at": datetime.now().isoformat(),
                "messages": []
            }
        
        sessions[session_id]["messages"].append({
            "role": "user",
            "content": message,
            "timestamp": datetime.now().isoformat()
        })
        
        # æ£€æŸ¥ç«å±±å¼•æ“å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not volcano_client:
            return jsonify({
                "success": False,
                "error": "ç«å±±å¼•æ“å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥é…ç½®",
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }), 500
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯analyDesignæ™ºèƒ½éœ€æ±‚åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è¿›è¡Œéœ€æ±‚åˆ†æã€è®¿è°ˆæçº²ç”Ÿæˆå’Œé—®å·è®¾è®¡ã€‚è¯·ç”¨ä¸“ä¸šã€å‹å¥½çš„è¯­æ°”å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
            },
            {
                "role": "user",
                "content": message
            }
        ]
        
        # è°ƒç”¨ç«å±±å¼•æ“API
        try:
            ai_response = volcano_client.chat(messages=messages)
            
            # ä¿å­˜AIå›å¤åˆ°ä¼šè¯
            sessions[session_id]["messages"].append({
                "role": "assistant",
                "content": ai_response,
                "timestamp": datetime.now().isoformat()
            })
            
            response_data = {
                "success": True,
                "response": ai_response,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat(),
                "confidence": 0.9,  # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                "intent": "éœ€æ±‚åˆ†æ",  # å¯ä»¥é€šè¿‡NLPåˆ†æå¾—å‡º
                "model": volcano_config.model_id
            }
            
            logger.info(f"æˆåŠŸå¤„ç†èŠå¤©è¯·æ±‚ - Session: {session_id}")
            return jsonify(response_data)
            
        except Exception as volcano_error:
            logger.error(f"ç«å±±å¼•æ“è°ƒç”¨å¤±è´¥: {volcano_error}")
            return jsonify({
                "success": False,
                "error": f"ç«å±±å¼•æ“è°ƒç”¨å¤±è´¥: {str(volcano_error)}",
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }), 500
            
    except Exception as e:
        logger.error(f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‡ºé”™: {e}")
        return jsonify({
            "success": False,
            "error": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/api/sessions', methods=['GET'])
def get_sessions():
    """è·å–æ‰€æœ‰ä¼šè¯"""
    return jsonify({
        "success": True,
        "sessions": sessions,
        "count": len(sessions),
        "timestamp": datetime.now().isoformat()
    })

@app.route('/api/sessions/<session_id>', methods=['GET'])
def get_session(session_id):
    """è·å–ç‰¹å®šä¼šè¯"""
    if session_id in sessions:
        return jsonify({
            "success": True,
            "session": sessions[session_id],
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        })
    else:
        return jsonify({
            "success": False,
            "error": "ä¼šè¯ä¸å­˜åœ¨",
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        }), 404

@app.route('/api/sessions/<session_id>', methods=['DELETE'])
def delete_session(session_id):
    """åˆ é™¤ä¼šè¯"""
    if session_id in sessions:
        del sessions[session_id]
        return jsonify({
            "success": True,
            "message": "ä¼šè¯å·²åˆ é™¤",
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        })
    else:
        return jsonify({
            "success": False,
            "error": "ä¼šè¯ä¸å­˜åœ¨",
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        }), 404

@app.errorhandler(404)
def not_found(error):
    return jsonify({
        "success": False,
        "error": "æ¥å£ä¸å­˜åœ¨",
        "timestamp": datetime.now().isoformat()
    }), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({
        "success": False,
        "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
        "timestamp": datetime.now().isoformat()
    }), 500

# ==================== é™æ€æ–‡ä»¶æœåŠ¡æ¥å£ ====================

@app.route('/uploads/<path:filename>')
def uploaded_file(filename):
    """é™æ€æ–‡ä»¶æœåŠ¡ - æä¾›uploadsç›®å½•ä¸‹çš„æ–‡ä»¶è®¿é—®"""
    try:
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
        uploads_dir = os.path.join(project_root, 'uploads')
        
        # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        file_path = os.path.join(uploads_dir, filename)
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶è·¯å¾„åœ¨uploadsç›®å½•å†…ï¼ˆé˜²æ­¢è·¯å¾„éå†æ”»å‡»ï¼‰
        uploads_dir_abs = os.path.abspath(uploads_dir)
        file_path_abs = os.path.abspath(file_path)
        
        if not file_path_abs.startswith(uploads_dir_abs):
            logger.warning(f"éæ³•æ–‡ä»¶è®¿é—®å°è¯•: {filename}")
            return jsonify({"error": "éæ³•æ–‡ä»¶è·¯å¾„"}), 403
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return jsonify({"error": "æ–‡ä»¶ä¸å­˜åœ¨"}), 404
        
        # è·å–æ–‡ä»¶çš„ç›®å½•å’Œæ–‡ä»¶å
        directory = os.path.dirname(file_path)
        basename = os.path.basename(file_path)
        
        # ä½¿ç”¨Flaskçš„send_from_directoryå‘é€æ–‡ä»¶
        from flask import send_from_directory
        return send_from_directory(directory, basename, as_attachment=False)
        
    except Exception as e:
        logger.error(f"é™æ€æ–‡ä»¶æœåŠ¡é”™è¯¯: {e}")
        return jsonify({"error": f"æ–‡ä»¶è®¿é—®å¤±è´¥: {str(e)}"}), 500

# ==================== æ–‡ä»¶å¤„ç†æ¥å£ ====================

@app.route('/api/file/upload', methods=['POST'])
def upload_file():
    """æ–‡ä»¶ä¸Šä¼ æ¥å£ - æ”¯æŒJSONå’Œmultipart/form-dataä¸¤ç§æ ¼å¼ï¼Œé›†æˆåˆ†ææœåŠ¡éªŒè¯"""
    try:
        # æ£€æŸ¥è¯·æ±‚ç±»å‹
        if request.content_type and 'application/json' in request.content_type:
            # JSONæ ¼å¼ä¸Šä¼ ï¼ˆå‰ç«¯ä½¿ç”¨ï¼‰
            data = request.get_json()
            if not data or 'file_info' not in data:
                return jsonify({
                    "success": False,
                    "error": "è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘file_info"
                }), 400
            
            file_info = data['file_info']
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not all(key in file_info for key in ['name', 'content']):
                return jsonify({
                    "success": False,
                    "error": "æ–‡ä»¶ä¿¡æ¯ä¸å®Œæ•´ï¼Œç¼ºå°‘nameæˆ–contentå­—æ®µ"
                }), 400
            
            # è§£ç base64æ–‡ä»¶å†…å®¹
            try:
                import base64
                file_content = base64.b64decode(file_info['content'])
            except Exception as e:
                return jsonify({
                    "success": False,
                    "error": f"æ–‡ä»¶å†…å®¹è§£ç å¤±è´¥: {str(e)}"
                }), 400
            
            # æ›´æ–°æ–‡ä»¶ä¿¡æ¯
            file_info['size'] = len(file_content)
            filename = file_info['name']
            
        else:
            # multipart/form-dataæ ¼å¼ä¸Šä¼ ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
            if 'file' not in request.files:
                return jsonify({
                    "success": False,
                    "error": "æ²¡æœ‰æ–‡ä»¶è¢«ä¸Šä¼ "
                }), 400
            
            file = request.files['file']
            if file.filename == '':
                return jsonify({
                    "success": False,
                    "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"
                }), 400
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            file_content = file.read()
            filename = file.filename
            
            # æ„å»ºæ–‡ä»¶ä¿¡æ¯
            file_info = {
                "name": filename,
                "type": file.content_type or "application/octet-stream",
                "size": len(file_content)
            }
        
        # ä½¿ç”¨åˆ†ææœåŠ¡è¿›è¡Œæ–‡ä»¶éªŒè¯
        file_validation = validate_file_upload(filename, len(file_content))
        if not all(file_validation.values()):
            validation_errors = [k for k, v in file_validation.items() if not v]
            return jsonify({
                "success": False,
                "error": f"æ–‡ä»¶éªŒè¯å¤±è´¥: {', '.join(validation_errors)}",
                "validation_details": file_validation
            }), 400
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # ä¿å­˜æ–‡ä»¶åˆ°uploads/tempç›®å½•ï¼ˆä½¿ç”¨åˆ†ææœåŠ¡çš„ç›®å½•ç»“æ„ï¼‰
        uploads_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "uploads", "temp")
        os.makedirs(uploads_dir, exist_ok=True)
        file_path = os.path.join(uploads_dir, f"{task_id}_{filename}")
        
        with open(file_path, 'wb') as f:
            f.write(file_content)
        
        # åˆ›å»ºè§£æä»»åŠ¡
        task = FileParsingTask(
            task_id=task_id,
            file_info=file_info,
            file_content=file_content,
            file_path=file_path
        )
        
        # å¼‚æ­¥å¼€å§‹æ–‡ä»¶è§£æ
        executor.submit(process_file_parsing, task)
        
        logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}, ä»»åŠ¡ID: {task_id}, å¤§å°: {len(file_content)} bytes")
        analysis_logger.info(f"ğŸ“ æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {filename}, ä»»åŠ¡ID: {task_id}")
        
        return jsonify({
            "success": True,
            "task_id": task_id,
            "file_info": file_info,
            "file_path": file_path,
            "validation_passed": True,
            "message": "æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œå¼€å§‹è§£æ",
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        analysis_logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/parsing/<task_id>', methods=['GET'])
def get_parsing_status(task_id):
    """è·å–æ–‡ä»¶è§£æçŠ¶æ€"""
    try:
        task = get_task(task_id)
        if task:
            task_dict = task.to_dict()
            
            # ç¡®ä¿ file_info å­—æ®µå­˜åœ¨ä¸”æœ‰æ•ˆ
            if 'file_info' not in task_dict or task_dict['file_info'] is None:
                task_dict['file_info'] = {
                    'name': 'æœªçŸ¥æ–‡ä»¶',
                    'type': 'unknown',
                    'size': 0
                }
            
            return jsonify({
                "success": True,
                **task_dict
            })
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"è·å–è§£æçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"è·å–è§£æçŠ¶æ€å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/analyze/<task_id>', methods=['POST'])
def analyze_content(task_id):
    """å†…å®¹åˆ†ææ¥å£ - æ¥æ”¶è§£æç»“æœï¼Œè¿”å›å¢åˆ æ”¹æŸ¥åŠŸèƒ½åˆ†æ"""
    try:
        task = get_task(task_id)
        if task:
            # æ£€æŸ¥ï¼šåªæœ‰å½“æ–‡æ¡£è§£æå®Œæˆæ—¶æ‰èƒ½å¼€å§‹å†…å®¹åˆ†æ
            # å…è®¸åœ¨ parsed æˆ– content_analyzed çŠ¶æ€ä¸‹è§¦å‘å†…å®¹åˆ†æï¼ˆæ”¯æŒé‡æ–°åˆ†æï¼‰
            if task.status not in ["parsed", "content_analyzed"]:
                return jsonify({
                    "success": False,
                    "error": f"æ–‡æ¡£è§£æå°šæœªå®Œæˆï¼ˆå½“å‰çŠ¶æ€: {task.status}ï¼‰ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æã€‚è¯·ç­‰å¾…æ–‡æ¡£è§£æå®Œæˆã€‚"
                }), 400
            
            # å¦‚æœçŠ¶æ€æ˜¯ parsedï¼Œæ£€æŸ¥è¿›åº¦æ˜¯å¦ä¸º100%
            if task.status == "parsed" and task.progress < 100:
                return jsonify({
                    "success": False,
                    "error": f"æ–‡æ¡£è§£æè¿›åº¦æœªè¾¾åˆ°100%ï¼ˆå½“å‰: {task.progress}%ï¼‰ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æã€‚"
                }), 400
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è§£æç»“æœ
            if not task.result:
                return jsonify({
                    "success": False,
                    "error": "æ–‡æ¡£è§£æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œå†…å®¹åˆ†æã€‚"
                }), 400
            
            # è·å–è¯·æ±‚å‚æ•° - åŒ…å«æ–‡æ¡£è§£æçš„ç»“æœæ•°æ®
            data = request.get_json() or {}
            parsing_result = data.get("parsing_result") or task.result
            
            logger.info(f"âœ… å†…å®¹åˆ†æå‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹å†…å®¹åˆ†æ: {task_id}")
            logger.info(f"ğŸ“„ æ¥æ”¶åˆ°è§£æç»“æœæ•°æ®: å­—ç¬¦æ•°={parsing_result.get('char_count', 0)}, æ–‡ä»¶ç±»å‹={parsing_result.get('file_type', 'unknown')}")
            
            # å¼‚æ­¥å¼€å§‹å†…å®¹åˆ†æï¼Œä¼ å…¥è§£æç»“æœ
            executor.submit(process_content_analysis, task, parsing_result)
            
            return jsonify({
                "success": True,
                "message": "å†…å®¹åˆ†æå·²å¼€å§‹",
                "task_id": task_id,
                "input_data": {
                    "parsing_result_received": True,
                    "content_length": len(parsing_result.get("text_content", "")),
                    "file_type": parsing_result.get("file_type", "unknown")
                },
                "expected_output": {
                    "crud_operations": "å¢åˆ æ”¹æŸ¥åŠŸèƒ½åˆ†æ",
                    "business_requirements": "ä¸šåŠ¡éœ€æ±‚åˆ†æ",
                    "functional_changes": "åŠŸèƒ½å˜æ›´åˆ†æ"
                },
                "previous_status": "parsed",
                "current_status": "content_analyzing",
                "timestamp": datetime.now().isoformat()
            })
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"å†…å®¹åˆ†æå¯åŠ¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"å†…å®¹åˆ†æå¯åŠ¨å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/ai-analyze/<task_id>', methods=['POST'])
def ai_analyze(task_id):
    """æ™ºèƒ½è§£ææ¥å£ - æ¥æ”¶å¢åˆ æ”¹æŸ¥åŠŸèƒ½ï¼Œè¿”å›æ¥å£è®¾è®¡å’ŒMQé…ç½®"""
    try:
        task = get_task(task_id)
        if task:
            # ä¸¥æ ¼æ£€æŸ¥ï¼šåªæœ‰å½“å†…å®¹åˆ†æå®Œå…¨å®Œæˆæ—¶æ‰èƒ½å¼€å§‹AIåˆ†æ
            if task.status != "content_analyzed":
                return jsonify({
                    "success": False,
                    "error": f"å†…å®¹åˆ†æå°šæœªå®Œæˆï¼ˆå½“å‰çŠ¶æ€: {task.status}ï¼‰ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½è§£æã€‚è¯·ç­‰å¾…å†…å®¹åˆ†æå®Œæˆã€‚"
                }), 400
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹åˆ†æç»“æœ
            if not task.content_analysis:
                return jsonify({
                    "success": False,
                    "error": "å†…å®¹åˆ†æç»“æœä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½è§£æã€‚"
                }), 400
            
            # è·å–è¯·æ±‚å‚æ•° - åŒ…å«å¢åˆ æ”¹æŸ¥åŠŸèƒ½åˆ†æç»“æœ
            data = request.get_json() or {}
            content_analysis_result = data.get("content_analysis") or task.content_analysis
            crud_operations = data.get("crud_operations", {})
            analysis_type = data.get("analysis_type", "comprehensive")
            
            logger.info(f"âœ… AIåˆ†æå‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹AIåˆ†æ: {task_id}")
            logger.info(f"ğŸ” æ¥æ”¶åˆ°å†…å®¹åˆ†æç»“æœ: CRUDæ“ä½œ={len(crud_operations.get('operations', []))}")
            
            # å¼‚æ­¥å¼€å§‹AIåˆ†æï¼Œä¼ å…¥å†…å®¹åˆ†æç»“æœå’ŒCRUDæ“ä½œ
            executor.submit(process_ai_analysis, task, analysis_type, content_analysis_result, parsing_result)
            
            return jsonify({
                "success": True,
                "message": "AIåˆ†æå·²å¼€å§‹",
                "task_id": task_id,
                "analysis_type": analysis_type,
                "input_data": {
                    "content_analysis_received": True,
                    "crud_operations_count": len(crud_operations.get("operations", [])),
                    "business_requirements_count": len(crud_operations.get("requirements", [])),
                    "functional_changes_count": len(crud_operations.get("changes", []))
                },
                "expected_output": {
                    "api_interfaces": "å…·ä½“å¼€å‘æ¥å£è®¾è®¡",
                    "interface_parameters": "æ¥å£å…¥å‚è¿”å‚å­—æ®µ",
                    "mq_configuration": "MQ topic/client/serveré…ç½®",
                    "technical_specifications": "æŠ€æœ¯è§„æ ¼è¯´æ˜"
                },
                "previous_status": "content_analyzed",
                "current_status": "ai_analyzing",
                "timestamp": datetime.now().isoformat()
            })
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"AIåˆ†æå¯åŠ¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"AIåˆ†æå¯åŠ¨å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/result/<task_id>', methods=['GET'])
def get_analysis_result(task_id):
    """è·å–å®Œæ•´åˆ†æç»“æœ - ä»Redisè·å–æ•°æ®"""
    try:
        # ä¼˜å…ˆä»Redisè·å–åˆ†æç»“æœ
        parsing_result = redis_task_storage.get_parsing_result(task_id) or {}
        content_analysis = redis_task_storage.get_content_analysis(task_id) or {}
        ai_analysis = redis_task_storage.get_ai_analysis(task_id) or {}
        
        # æ·»åŠ è¯¦ç»†çš„ç±»å‹æ£€æŸ¥æ—¥å¿—
        logger.info(f"ğŸ” æ•°æ®ç±»å‹è¯Šæ–­ - parsing_result: {type(parsing_result)}")
        logger.info(f"ğŸ” æ•°æ®ç±»å‹è¯Šæ–­ - content_analysis: {type(content_analysis)}")
        logger.info(f"ğŸ” æ•°æ®ç±»å‹è¯Šæ–­ - ai_analysis: {type(ai_analysis)}")
        
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
        if isinstance(parsing_result, str):
            logger.warning(f"âš ï¸ parsing_resultæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•JSONè§£æ: {parsing_result[:200]}...")
            try:
                parsing_result = json.loads(parsing_result)
                logger.info("âœ… parsing_result JSONè§£ææˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ parsing_result JSONè§£æå¤±è´¥: {e}")
                parsing_result = {}
        
        if isinstance(content_analysis, str):
            logger.warning(f"âš ï¸ content_analysisæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•JSONè§£æ: {content_analysis[:200]}...")
            try:
                content_analysis = json.loads(content_analysis)
                logger.info("âœ… content_analysis JSONè§£ææˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ content_analysis JSONè§£æå¤±è´¥: {e}")
                content_analysis = {}
        
        if isinstance(ai_analysis, str):
            logger.warning(f"âš ï¸ ai_analysisæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•JSONè§£æ: {ai_analysis[:200]}...")
            try:
                ai_analysis = json.loads(ai_analysis)
                logger.info("âœ… ai_analysis JSONè§£ææˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ ai_analysis JSONè§£æå¤±è´¥: {e}")
                ai_analysis = {}
        
        # ä»SQLiteè·å–ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
        task = get_task(task_id)
        if task:
            
            # è·å–ç”Ÿæˆçš„markdownå†…å®¹ - ä»Redisè·å–ï¼Œå› ä¸ºå†…å­˜ä¸­çš„ä»»åŠ¡å¯¹è±¡å¯èƒ½ä¸æ˜¯æœ€æ–°çš„
            markdown_content = redis_task_storage.get_markdown_content(task_id) or ""
            
            # æ„å»ºæœ€ç»ˆçš„æ•´åˆç»“æœ
            result = {
                "success": True,
                "task_id": task_id,
        
                  # åŸºæœ¬ä¿¡æ¯
                "basic_info": {
                    "filename": task.file_info.get("filename", "Unknown"),
                    "filesize": f"{task.file_info.get('size', 0) / 1024:.1f} KB",
                    "file_type": task.file_info.get("type", "Unknown")
                },
                #æ–‡æ¡£è§£æè¿”å›ç»“æœ
                "document_parsing": parsing_result,
                #å†…å®¹åˆ†æè¿”å›ç»“æœ
                "content_analysis": content_analysis,
                #AIåˆ†æè¿”å›ç»“æœ
                "ai_analysis": ai_analysis,
                # åç«¯ç”Ÿæˆçš„markdownå†…å®¹ - ä¿®å¤ï¼šä»Redisè·å–è€Œéä»taskå¯¹è±¡
                "markdown_content": markdown_content,
                "summary": {
                    "complexity_level": content_analysis.get("complexity_level", "ä¸­ç­‰"),
                    "crud_operations_count": len(content_analysis.get("crud_analysis", {}).get("operations", [])),
                    "api_interfaces_count": len(ai_analysis.get("api_interfaces", [])),
                    "mq_topics_count": len(ai_analysis.get("mq_configuration", {}).get("topics", [])),
                    "estimated_development_time": content_analysis.get("business_insights", {}).get("estimated_development_time", "æœªçŸ¥")
                },
                "error": task.error or "",
                "file_info": task.file_info or {},
                "timestamps": {
                    "created_at": task.created_at.isoformat() if task.created_at else "",
                    "updated_at": task.updated_at.isoformat() if task.updated_at else "",
                    "parsing_completed": task.created_at.isoformat() if task.created_at else "",
                    "content_analysis_completed":content_analysis.get("analysis_metadata", {}).get("analyzed_at", ""),
                    "ai_analysis_completed": ai_analysis.get("analyzed_at", "")
                }
            }
            
            return jsonify(result)
        else:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
    except Exception as e:
        logger.error(f"è·å–åˆ†æç»“æœå¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"è·å–åˆ†æç»“æœå¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/list', methods=['GET'])
def list_files():
    """è·å–å·²ä¸Šä¼ æ–‡ä»¶åˆ—è¡¨"""
    try:
        tasks = get_all_tasks()
        file_list = []
        for task in tasks:
            file_list.append({
                "task_id": task.id,
                "file_info": task.file_info,
                "status": task.status,
                "progress": task.progress,
                "created_at": task.created_at.isoformat(),
                "updated_at": task.updated_at.isoformat()
            })
        
        # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—
        file_list.sort(key=lambda x: x["created_at"], reverse=True)
        
        return jsonify({
            "success": True,
            "files": file_list,
            "count": len(file_list),
            "timestamp": datetime.now().isoformat()
        })
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/delete/<task_id>', methods=['DELETE'])
def delete_file(task_id):
    """åˆ é™¤å·²ä¸Šä¼ æ–‡ä»¶"""
    try:
        # å…ˆè·å–ä»»åŠ¡ä¿¡æ¯
        task = get_task(task_id)
        if not task:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
        
        # åˆ é™¤ç‰©ç†æ–‡ä»¶
        if task.file_path and os.path.exists(task.file_path):
            try:
                os.remove(task.file_path)
                logger.info(f"åˆ é™¤æ–‡ä»¶: {task.file_path}")
            except Exception as e:
                logger.warning(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
        
        # ä»æ•°æ®åº“ä¸­åˆ é™¤ä»»åŠ¡è®°å½•
        if delete_task(task_id):
            return jsonify({
                "success": True,
                "message": f"æ–‡ä»¶ {task.file_info.get('name', 'æœªçŸ¥æ–‡ä»¶')} å·²åˆ é™¤",
                "task_id": task_id,
                "timestamp": datetime.now().isoformat()
            })
        else:
            return jsonify({
                "success": False,
                "error": "åˆ é™¤ä»»åŠ¡è®°å½•å¤±è´¥"
            }), 500
            
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"åˆ é™¤æ–‡ä»¶å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/v2/analysis/start', methods=['POST'])
def start_analysis_v2():
    """V2ç‰ˆæœ¬ï¼šå¯åŠ¨å®Œæ•´æµç¨‹ï¼ˆè‡ªåŠ¨æ‰§è¡Œ4é˜¶æ®µï¼‰"""
    try:
        # æ£€æŸ¥è¯·æ±‚ç±»å‹
        if request.content_type and 'application/json' in request.content_type:
            # JSONæ ¼å¼ä¸Šä¼ ï¼ˆå‰ç«¯ä½¿ç”¨ï¼‰
            data = request.get_json()
            if not data or 'file_info' not in data:
                return jsonify({
                    "success": False,
                    "error": "è¯·æ±‚æ•°æ®æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘file_info"
                }), 400
            
            file_info = data['file_info']
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not all(key in file_info for key in ['name', 'content']):
                return jsonify({
                    "success": False,
                    "error": "æ–‡ä»¶ä¿¡æ¯ä¸å®Œæ•´ï¼Œç¼ºå°‘nameæˆ–contentå­—æ®µ"
                }), 400
            
            # å®‰å…¨è®°å½•æ–‡ä»¶ä¿¡æ¯ï¼Œé¿å…base64å†…å®¹è¿‡é•¿
            logger.info(f"file_infoåŒ…å«contentå­—æ®µï¼Œé•¿åº¦: {len(file_info['content'])}")
            # è§£ç base64æ–‡ä»¶å†…å®¹
            try:
                import base64
                file_content = base64.b64decode(file_info['content'])

                logger.info(f"base64è§£ç æˆåŠŸï¼Œfile_contentç±»å‹: {type(file_content)}, å¤§å°: {len(file_content)} bytes")
            except Exception as e:
                return jsonify({
                    "success": False,
                    "error": f"æ–‡ä»¶å†…å®¹è§£ç å¤±è´¥: {str(e)}"
                }), 400
            
            # æ›´æ–°æ–‡ä»¶ä¿¡æ¯
            file_info['size'] = len(file_content)
            filename = file_info['name']
            
        else:
            # multipart/form-dataæ ¼å¼ä¸Šä¼ ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
            if 'file' not in request.files:
                return jsonify({
                    "success": False,
                    "error": "æ²¡æœ‰æ–‡ä»¶è¢«ä¸Šä¼ "
                }), 400
            
            file = request.files['file']
            if file.filename == '':
                return jsonify({
                    "success": False,
                    "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"
                }), 400
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            file_content = file.read()
            filename = file.filename
            
            # æ„å»ºæ–‡ä»¶ä¿¡æ¯
            file_info = {
                "name": filename,
                "type": file.content_type or "application/octet-stream",
                "size": len(file_content)
            }
        
        # ä½¿ç”¨åˆ†ææœåŠ¡è¿›è¡Œæ–‡ä»¶éªŒè¯
        file_validation = validate_file_upload(filename, len(file_content))
        if not all(file_validation.values()):
            validation_errors = [k for k, v in file_validation.items() if not v]
            return jsonify({
                "success": False,
                "error": f"æ–‡ä»¶éªŒè¯å¤±è´¥: {', '.join(validation_errors)}",
                "validation_details": file_validation
            }), 400
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # ä¿å­˜æ–‡ä»¶åˆ°uploads/tempç›®å½•
        uploads_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "uploads", "temp")
        os.makedirs(uploads_dir, exist_ok=True)
        file_path = os.path.join(uploads_dir, f"{task_id}_{filename}")
        
        with open(file_path, 'wb') as f:
            f.write(file_content)
        
       
        # åˆ›å»ºè§£æä»»åŠ¡
        task = FileParsingTask(
            task_id=task_id,
            file_info=file_info,
            file_path=file_path
        )
        
        # è®¾ç½®åˆå§‹è¿›åº¦çŠ¶æ€ä¸º"å¯åŠ¨ä¸­"
        task.update_progress(0, "åˆ†ææµç¨‹å¯åŠ¨ä¸­", "starting")
        
        # å¯åŠ¨å®Œæ•´çš„4é˜¶æ®µåˆ†ææµç¨‹
        executor.submit(run_full_analysis_pipeline, task)
        
        logger.info(f"V2 å®Œæ•´åˆ†æå¯åŠ¨æˆåŠŸ: {filename}, ä»»åŠ¡ID: {task_id}, å¤§å°: {len(file_content)} bytes")
        analysis_logger.info(f"ğŸš€ V2 å®Œæ•´åˆ†æå¯åŠ¨: {filename}, ä»»åŠ¡ID: {task_id}")
        
        return jsonify({
            "success": True,
            "task_id": task_id,
            "file_info": file_info,
            "message": "å®Œæ•´åˆ†ææµç¨‹å·²å¯åŠ¨",
            "stages": [
                {"name": "document_parsing", "title": "æ–‡æ¡£è§£æ", "status": "pending"},
                {"name": "content_analysis", "title": "å†…å®¹åˆ†æ", "status": "pending"},
                {"name": "ai_analysis", "title": "AIè®¾è®¡æ–¹æ¡ˆ", "status": "pending"},
                {"name": "document_generation", "title": "ç”Ÿæˆæ–‡æ¡£", "status": "pending"}
            ],
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"V2 åˆ†æå¯åŠ¨å¤±è´¥: {e}")
        analysis_logger.error(f"âŒ V2 åˆ†æå¯åŠ¨å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"åˆ†æå¯åŠ¨å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/v2/analysis/progress/<task_id>', methods=['GET'])
def get_analysis_progress_v2(task_id):
    """V2ç‰ˆæœ¬ï¼šè·å–å®æ—¶è¿›åº¦æ›´æ–°"""
    try:
        task = get_task(task_id)
        if not task:
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
        
        # è®¡ç®—å„é˜¶æ®µè¿›åº¦ï¼ˆåªä¿ç•™3ä¸ªé˜¶æ®µï¼‰
        stages = {
            "document_parsing": {
                "title": "æ–‡æ¡£è§£æ",
                "status": "pending",
                "progress": 0,
                "message": "ç­‰å¾…å¼€å§‹"
            },
            "content_analysis": {
                "title": "å†…å®¹åˆ†æ", 
                "status": "pending",
                "progress": 0,
                "message": "ç­‰å¾…å¼€å§‹"
            },
            "ai_analysis": {
                "title": "è®¾è®¡æ–¹æ¡ˆ",
                "status": "pending", 
                "progress": 0,
                "message": "ç­‰å¾…å¼€å§‹"
            }
        }
        
        # è·å–æœ€æ–°çš„æ­¥éª¤æè¿°
        def get_latest_description(task):
            if task.steps and len(task.steps) > 0 and isinstance(task.steps[-1], dict):
                return task.steps[-1].get("description", "å¤„ç†ä¸­...")
            return "å¤„ç†ä¸­..."
        
        # æ ¹æ®ä»»åŠ¡çŠ¶æ€æ›´æ–°é˜¶æ®µä¿¡æ¯
        if task.status in ["parsing", "parsed"]:
            stages["document_parsing"]["status"] = "running" if task.status == "parsing" else "completed"
            stages["document_parsing"]["progress"] = task.progress if task.status == "parsing" else 100
            stages["document_parsing"]["message"] = get_latest_description(task) if task.status == "parsing" else "æ–‡æ¡£è§£æå®Œæˆ"
            
        if task.status in ["content_analyzing", "content_analyzed"]:
            stages["document_parsing"]["status"] = "completed"
            stages["document_parsing"]["progress"] = 100
            stages["document_parsing"]["message"] = "æ–‡æ¡£è§£æå®Œæˆ"
            
            stages["content_analysis"]["status"] = "running" if task.status == "content_analyzing" else "completed"
            stages["content_analysis"]["progress"] = task.progress if task.status == "content_analyzing" else 100
            stages["content_analysis"]["message"] = get_latest_description(task) if task.status == "content_analyzing" else "å†…å®¹åˆ†æå®Œæˆ"
            
        if task.status in ["ai_analyzing", "ai_analyzed", "fully_completed"]:
            stages["document_parsing"]["status"] = "completed"
            stages["document_parsing"]["progress"] = 100
            stages["document_parsing"]["message"] = "æ–‡æ¡£è§£æå®Œæˆ"
            
            stages["content_analysis"]["status"] = "completed"
            stages["content_analysis"]["progress"] = 100
            stages["content_analysis"]["message"] = "å†…å®¹åˆ†æå®Œæˆ"
            
            stages["ai_analysis"]["status"] = "running" if task.status == "ai_analyzing" else "completed"
            stages["ai_analysis"]["progress"] = task.progress if task.status == "ai_analyzing" else 100
            stages["ai_analysis"]["message"] = get_latest_description(task) if task.status == "ai_analyzing" else "AIåˆ†æå®Œæˆ"
        
        # è®¡ç®—æ•´ä½“è¿›åº¦ï¼ˆåŸºäº3ä¸ªé˜¶æ®µï¼‰
        total_progress = 0
        completed_stages = 0
        
        for stage in stages.values():
            if stage["status"] == "completed":
                completed_stages += 1
                total_progress += 100
            elif stage["status"] == "running":
                total_progress += stage["progress"]
        
        overall_progress = total_progress // 3  # æ”¹ä¸º3ä¸ªé˜¶æ®µ
        
        # ç¡®å®šæ•´ä½“çŠ¶æ€
        overall_status = "pending"
        if task.status == "fully_completed":
            overall_status = "completed"
        elif task.status in ["failed", "ai_failed", "content_failed"]:
            overall_status = "failed"
        elif completed_stages > 0 or any(stage["status"] == "running" for stage in stages.values()):
            overall_status = "running"
        
        return jsonify({
            "success": True,
            "task_id": task_id,
            "overall_status": overall_status,
            "overall_progress": overall_progress,
            "current_stage": task.status,
            "stages": stages,
            "file_info": task.file_info,
            "error": task.error,
            "started_at": task.created_at.isoformat() if task.created_at else None,
            "updated_at": task.updated_at.isoformat() if task.updated_at else None,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"è·å–V2è¿›åº¦å¤±è´¥: {e}")
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {error_details}")
        return jsonify({
            "success": False,
            "error": f"è·å–è¿›åº¦å¤±è´¥: {str(e)}"
        }), 500

def run_full_analysis_pipeline(task: FileParsingTask):
    """
    è¿è¡Œå®Œæ•´çš„åˆ†ææµç¨‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    é˜¶æ®µ1: æ–‡æ¡£è§£æ (0-30%)
    é˜¶æ®µ2: å†…å®¹åˆ†æ (30-70%)  
    é˜¶æ®µ3: AIåˆ†æå¹¶ç”Ÿæˆform_data (70-100%)
    """
    try:
        logger.info(f"å¼€å§‹å®Œæ•´åˆ†ææµç¨‹: {task.id}")
        analysis_logger.info(f"ğŸš€ å®Œæ•´åˆ†ææµç¨‹å¯åŠ¨: {task.id}")
        
        # é˜¶æ®µ1: æ–‡æ¡£è§£æ
        task.update_progress(10, "å¼€å§‹æ–‡æ¡£è§£æ", "parsing")
        process_file_parsing(task)
        
        # æ£€æŸ¥æ–‡æ¡£è§£ææ˜¯å¦æˆåŠŸ
        if task.status != "parsed":
            logger.error(f"æ–‡æ¡£è§£æå¤±è´¥: {task.id}")
            return
        
        # è·å–è§£æç»“æœ
        parsing_result = redis_task_storage.get_parsing_result(task.id)
        if not parsing_result:
            logger.error(f"æ— æ³•è·å–è§£æç»“æœ: {task.id}")
            return
        
        # é˜¶æ®µ2: å†…å®¹åˆ†æ  
        task.update_progress(40, "å¼€å§‹å†…å®¹åˆ†æ", "content_analyzing")
        process_content_analysis(task, parsing_result)
        
        # æ£€æŸ¥å†…å®¹åˆ†ææ˜¯å¦æˆåŠŸ
        if task.status != "content_analyzed":
            logger.error(f"å†…å®¹åˆ†æå¤±è´¥: {task.id}")
            return
        
        # è·å–å†…å®¹åˆ†æç»“æœ
        content_analysis = redis_task_storage.get_content_analysis(task.id)
        
        # é˜¶æ®µ3: AIè®¾è®¡æ–¹æ¡ˆ
        task.update_progress(70, "å¼€å§‹AIè®¾è®¡æ–¹æ¡ˆ", "ai_analyzing")
        process_ai_analysis(task, "comprehensive", content_analysis, parsing_result)
        
        # æ£€æŸ¥AIåˆ†ææ˜¯å¦æˆåŠŸ
        if task.status != "ai_analyzed":
            logger.error(f"AIåˆ†æå¤±è´¥: {task.id}, å½“å‰çŠ¶æ€: {task.status}, é”™è¯¯ä¿¡æ¯: {task.error}")
            # å³ä½¿AIåˆ†æçŠ¶æ€ä¸æ˜¯ai_analyzedï¼Œå¦‚æœform_dataå·²ç»ç”Ÿæˆï¼Œä¹Ÿåº”è¯¥ç»§ç»­
            try:
                form_data_check = redis_task_storage.redis_manager.get(f"form_data:{task.id}")
                if form_data_check:
                    logger.info(f"æ£€æµ‹åˆ°form_dataå·²ç”Ÿæˆï¼Œç»§ç»­å®Œæˆæµç¨‹: {task.id}")
                    task.update_progress(100, "å®Œæ•´åˆ†ææµç¨‹å®Œæˆ", "fully_completed")
                    logger.info(f"å®Œæ•´åˆ†ææµç¨‹æˆåŠŸå®Œæˆ: {task.id}")
                    analysis_logger.info(f"ğŸ‰ å®Œæ•´åˆ†ææµç¨‹å®Œæˆ: {task.id}")
                    return
            except Exception as check_error:
                logger.error(f"æ£€æŸ¥form_dataæ—¶å‡ºé”™: {check_error}")
            return
        
        # AIåˆ†æå®Œæˆå³å®Œæˆæ‰€æœ‰æµç¨‹ï¼ˆform_dataå·²åœ¨AIåˆ†æé˜¶æ®µç”Ÿæˆå¹¶ä¿å­˜ï¼‰
        task.update_progress(100, "å®Œæ•´åˆ†ææµç¨‹å®Œæˆ", "fully_completed")
        logger.info(f"å®Œæ•´åˆ†ææµç¨‹æˆåŠŸå®Œæˆ: {task.id}")
        analysis_logger.info(f"ğŸ‰ å®Œæ•´åˆ†ææµç¨‹å®Œæˆ: {task.id}")
        
    except Exception as e:
        logger.error(f"å®Œæ•´åˆ†ææµç¨‹å¤±è´¥: {task.id}, é”™è¯¯: {e}")
        task.error = f"åˆ†ææµç¨‹å¤±è´¥: {str(e)}"
        task.status = "failed"
        task.update_progress(task.progress, f"åˆ†æå¤±è´¥: {str(e)}", "failed")


@app.route('/api/file/design-form/<task_id>', methods=['PUT'])
def update_design_form_data(task_id):
    """æ›´æ–°ä»»åŠ¡çš„è®¾è®¡æ–¹æ¡ˆè¡¨å•æ•°æ®"""
    try:
        # è®°å½•APIè°ƒç”¨
        logger.info(f"æ”¶åˆ°è®¾è®¡æ–¹æ¡ˆè¡¨å•æ•°æ®æ›´æ–°è¯·æ±‚: ä»»åŠ¡ID={task_id}")
        
        # è·å–è¯·æ±‚æ•°æ®
        data = request.get_json()
        if not data:
            logger.warning(f"è¡¨å•æ•°æ®æ›´æ–°è¯·æ±‚ç¼ºå°‘æ•°æ®: {data}")
            return jsonify({
                "success": False,
                "error": "ç¼ºå°‘è¯·æ±‚æ•°æ®"
            }), 400
        
        form_data = data.get('form_data', {})
        markdown_content = data.get('markdown_content', '')
        
        # éªŒè¯è¡¨å•æ•°æ®
        if not isinstance(form_data, dict):
            return jsonify({
                "success": False,
                "error": "form_dataå¿…é¡»æ˜¯å¯¹è±¡ç±»å‹"
            }), 400
        
        # éªŒè¯markdownå†…å®¹
        if not isinstance(markdown_content, str):
            return jsonify({
                "success": False,
                "error": "markdown_contentå¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹"
            }), 400
        
        # éªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        task = get_task(task_id)
        if not task:
            logger.warning(f"å°è¯•æ›´æ–°ä¸å­˜åœ¨çš„ä»»åŠ¡è¡¨å•æ•°æ®: {task_id}")
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
        
        # ä¿å­˜è¡¨å•æ•°æ®åˆ°Rediså­˜å‚¨ï¼ˆä½¿ç”¨redis_task_storageï¼‰
        try:
            # ä½¿ç”¨ç°æœ‰çš„redis_task_storageæ¥ä¿å­˜è¡¨å•æ•°æ®
            redis_task_storage.redis_manager.set(
                f"form_data:{task_id}", 
                form_data,  # RedisManagerçš„setæ–¹æ³•ä¼šè‡ªåŠ¨JSONåºåˆ—åŒ–
                ttl=86400 * 7  # 7å¤©è¿‡æœŸ
            )
        except Exception as e:
            logger.error(f"ä¿å­˜è¡¨å•æ•°æ®åˆ°Rediså¤±è´¥: {e}")
            return jsonify({
                "success": False,
                "error": "ä¿å­˜è¡¨å•æ•°æ®å¤±è´¥"
            }), 500
        
        # åŒæ—¶ä¿å­˜markdownå†…å®¹
        if markdown_content:
            success = markdown_storage.save_markdown(task_id, markdown_content)
            if not success:
                logger.warning(f"ä¿å­˜Markdownå†…å®¹å¤±è´¥ï¼Œä½†è¡¨å•æ•°æ®å·²ä¿å­˜: {task_id}")
        
        # æ›´æ–°ä»»åŠ¡å¯¹è±¡
        task.form_data = form_data
        task.markdown_content = markdown_content
        
        # è®°å½•æ“ä½œæ—¥å¿—
        logger.info(f"è®¾è®¡æ–¹æ¡ˆè¡¨å•æ•°æ®æ›´æ–°æˆåŠŸ: ä»»åŠ¡ID={task_id}, è¡¨å•å­—æ®µæ•°={len(form_data)}, Markdowné•¿åº¦={len(markdown_content)}")
        
        return jsonify({
            "success": True,
            "task_id": task_id,
            "form_fields_count": len(form_data),
            "markdown_length": len(markdown_content),
            "updated_at": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"æ›´æ–°è®¾è®¡æ–¹æ¡ˆè¡¨å•æ•°æ®å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"æ›´æ–°å¤±è´¥: {str(e)}"
        }), 500

@app.route('/api/file/design-form/<task_id>', methods=['GET'])
def get_design_form_data(task_id):
    """è·å–ä»»åŠ¡çš„è®¾è®¡æ–¹æ¡ˆè¡¨å•æ•°æ®"""
    try:
        # è®°å½•APIè°ƒç”¨
        logger.info(f"æ”¶åˆ°è®¾è®¡æ–¹æ¡ˆè¡¨å•æ•°æ®è·å–è¯·æ±‚: ä»»åŠ¡ID={task_id}")
        
        # éªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        task = get_task(task_id)
        if not task:
            logger.warning(f"å°è¯•è·å–ä¸å­˜åœ¨çš„ä»»åŠ¡è¡¨å•æ•°æ®: {task_id}")
            return jsonify({
                "success": False,
                "error": "ä»»åŠ¡ä¸å­˜åœ¨"
            }), 404
        
        # ä»Redisè·å–è¡¨å•æ•°æ®
        try:
            form_data = redis_task_storage.redis_manager.get(f"form_data:{task_id}")
            
            if form_data is None:
                logger.info(f"ä»»åŠ¡ {task_id} å°šæœªç”Ÿæˆè¡¨å•æ•°æ®")
                return jsonify({
                    "success": False,
                    "error": "è¡¨å•æ•°æ®ä¸å­˜åœ¨",
                    "message": "è¯¥ä»»åŠ¡å°šæœªç”Ÿæˆè®¾è®¡æ–¹æ¡ˆè¡¨å•æ•°æ®"
                }), 404
        except Exception as e:
            logger.error(f"ä»Redisè·å–è¡¨å•æ•°æ®å¤±è´¥: {e}")
            return jsonify({
                "success": False,
                "error": "è·å–è¡¨å•æ•°æ®å¤±è´¥"
            }), 500
        
        # åŒæ—¶è·å–markdownå†…å®¹
        markdown_content = markdown_storage.get_markdown_content_only(task_id) or ""
        
        logger.info(f"è®¾è®¡æ–¹æ¡ˆè¡¨å•æ•°æ®è·å–æˆåŠŸ: ä»»åŠ¡ID={task_id}, è¡¨å•å­—æ®µæ•°={len(form_data)}")
        
        return jsonify({
            "success": True,
            "task_id": task_id,
            "form_data": form_data,
            "markdown_content": markdown_content,
            "form_fields_count": len(form_data),
            "markdown_length": len(markdown_content),
            "storage_source": "Rediså­˜å‚¨",
            "retrieved_at": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"è·å–è®¾è®¡æ–¹æ¡ˆè¡¨å•æ•°æ®å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": f"è·å–å¤±è´¥: {str(e)}"
        }), 500

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8082, debug=True)